{"url": "https://aws.amazon.com/bedrock/", "text": " Search Amazon Bedrock Amazon Bedrock The platform for building generative AI applications and agents at production scale Drive real business impact Amazon Bedrock powers generative AI for more than 100,000 organizations worldwide—from startups to global enterprises across every industry. It provides the proven infrastructure and comprehensive capabilities to confidently build applications and agents that work in production with the flexibility, enterprise security, and proven scalability you need to innovate boldly and deliver AI that drives real business impact. Capabilities Organizations choose Amazon Bedrock to deliver personalized experiences, automate complex workflows, and uncover actionable insights—all in a comprehensive platform that lets teams innovate fast without compromising on security. Choose the best model for your use case Amazon Bedrock gives you access to hundreds of FMs from leading AI companies along with evaluation tools to pick the best model based on your unique performance and cost needs. Future-proof your AI strategy as your needs evolve and new models emerge. Securely customize with your data Move from generic AI to AI that knows your customer and your business by customizing models with your data. By combining multiple data customization tools—Knowledge Bases, Bedrock Data Automation, prompt engineering, and fine-tuning—you can optimize your AI applications to your business, while ensuring you’re always in control of sensitive information. Apply security, privacy, and responsible AI checks Amazon Bedrock provides industry-leadingsecurity, privacy, and compliancefor generative AI applications.Bedrock Guardrailscan help block up to 88% of harmful content and identify correct model responses with up to 99% accuracy to minimize hallucinations and data ambiguity using Automated Reasoning checks. Bedrock never stores or uses your data to train models, ensuring complete security and privacy, with encryption of data in transit and at rest, as well as identity-based policies for managing data access. Bedrock provides comprehensive monitoring and logging capabilities that can support your governance and audit requirements. Finally, Bedrock is in scope for common compliance standards including ISO, SOC, CSA STAR Level 2, GDPR, FedRAMP High, and is HIPAA eligible. Optimize for cost, latency, and accuracy Ensure your AI applications are optimized for the perfect balance of cost, speed, and accuracy. Features like Model Distillation, Prompt caching, and Intelligent Prompt Routing can reduce expenses while maintaining performance. For example, distilled models run up to 500% faster and cost up to 75% less, with minimal impact on accuracy. Intelligent Prompt Routing can cut costs by up to 30% while maintaining quality. With flexible options for both real-time and batch processing, Bedrock helps you build smart, efficient, and cost-effective AI systems. Accelerate agents to production with composable services With Amazon Bedrock AgentCore, developers can accelerate AI agents into production with the scale, reliability, and security critical to real-world deployment. AgentCore includes foundational tools required by agents to execute real-world workflows: Do more with Amazon Bedrock Get from idea to production fast Write new copy Jump-start your writing project with new pieces of original copy, such as blog posts, social media posts, and webpage copy. Create a virtual assistant Build assistive agents that understand user requests, automatically breakdown tasks, ingest and analyze multi-modal data, and then take actions and create new content to fulfill the request. Skip the reading Summarize long documents such as articles, reports, research papers, technical documentation, and even books to quickly and effectively extract important information. Be the designer Create stunningly realistic and visually appealing images for ad campaigns, websites, presentations, and more in an instant. Streamline workflows Automate routine tasks and streamline operations across functions like financial reporting and demand forecasting with agents. Get deep insights Use agents to automate complex data analysis and simulations—allowing researchers and creators to focus on strategic ideation rather than technical execution. Real results, real customers Robinhood Robinhood transformed into an AI-first financial innovator using Amazon Bedrock, scaling from 500 million to 5 billion tokens daily in just six months—while slashing AI costs by 80% and cutting development time in half. According to Dev Tagare, Robinhood's Head of AI, Amazon Bedrock's model diversity, security, and compliance features are purpose-built for regulated industries. Robinhood's success demonstrates how fintech leaders can use Amazon Bedrock to democratize finance with cutting-edge AI while maintaining enterprise-grade security and compliance. Next steps Amazon Bedrock customer success stories Amazon Bedrock documentation Explore common Amazon Bedrock use cases with a guided workshop View demos of Amazon Bedrock capabilities Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_innovation_5_1200.86a666ec42e4300da4044eff19e2fd5809a178ec.png", "local": "images\\3318d040c154b0bcbf4e118538f3f020294ff5f0.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_data_2_1200.64d58473e27cc36cd5efac49d7b300750a8e4e6b.png", "local": "images\\50e219659ef450c1c87c25de96fbc3d0317fd80e.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Prompt_Engineering_4_1200.714389e3df8383b1d11b84bbdece4654c89889ac.png", "local": "images\\ec951315d584eba5ea664ea711b8f7b57f763afe.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Large_Language_Model_1_1200_1.9b916d5df94e6e6380e6f6f4ff0fba09b2e1d1bc.png", "local": "images\\b7a33de2014722604c95f9a5363c18a18ea3b6c3.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/model-choice", "https://aws.amazon.com/bedrock/customize", "https://aws.amazon.com/bedrock/security-compliance/", "https://aws.amazon.com/bedrock/guardrails/", "https://aws.amazon.com/bedrock/security-privacy-responsible-ai/", "https://aws.amazon.com/bedrock/cost-optimization", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/customers/", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html", "https://aws.amazon.com/bedrock/"]}
{"url": "https://aws.amazon.com/bedrock/#aws-page-content-main", "text": " Search Amazon Bedrock Amazon Bedrock The platform for building generative AI applications and agents at production scale Drive real business impact Amazon Bedrock powers generative AI for more than 100,000 organizations worldwide—from startups to global enterprises across every industry. It provides the proven infrastructure and comprehensive capabilities to confidently build applications and agents that work in production with the flexibility, enterprise security, and proven scalability you need to innovate boldly and deliver AI that drives real business impact. Capabilities Organizations choose Amazon Bedrock to deliver personalized experiences, automate complex workflows, and uncover actionable insights—all in a comprehensive platform that lets teams innovate fast without compromising on security. Choose the best model for your use case Amazon Bedrock gives you access to hundreds of FMs from leading AI companies along with evaluation tools to pick the best model based on your unique performance and cost needs. Future-proof your AI strategy as your needs evolve and new models emerge. Securely customize with your data Move from generic AI to AI that knows your customer and your business by customizing models with your data. By combining multiple data customization tools—Knowledge Bases, Bedrock Data Automation, prompt engineering, and fine-tuning—you can optimize your AI applications to your business, while ensuring you’re always in control of sensitive information. Apply security, privacy, and responsible AI checks Amazon Bedrock provides industry-leadingsecurity, privacy, and compliancefor generative AI applications.Bedrock Guardrailscan help block up to 88% of harmful content and identify correct model responses with up to 99% accuracy to minimize hallucinations and data ambiguity using Automated Reasoning checks. Bedrock never stores or uses your data to train models, ensuring complete security and privacy, with encryption of data in transit and at rest, as well as identity-based policies for managing data access. Bedrock provides comprehensive monitoring and logging capabilities that can support your governance and audit requirements. Finally, Bedrock is in scope for common compliance standards including ISO, SOC, CSA STAR Level 2, GDPR, FedRAMP High, and is HIPAA eligible. Optimize for cost, latency, and accuracy Ensure your AI applications are optimized for the perfect balance of cost, speed, and accuracy. Features like Model Distillation, Prompt caching, and Intelligent Prompt Routing can reduce expenses while maintaining performance. For example, distilled models run up to 500% faster and cost up to 75% less, with minimal impact on accuracy. Intelligent Prompt Routing can cut costs by up to 30% while maintaining quality. With flexible options for both real-time and batch processing, Bedrock helps you build smart, efficient, and cost-effective AI systems. Accelerate agents to production with composable services With Amazon Bedrock AgentCore, developers can accelerate AI agents into production with the scale, reliability, and security critical to real-world deployment. AgentCore includes foundational tools required by agents to execute real-world workflows: Do more with Amazon Bedrock Get from idea to production fast Write new copy Jump-start your writing project with new pieces of original copy, such as blog posts, social media posts, and webpage copy. Create a virtual assistant Build assistive agents that understand user requests, automatically breakdown tasks, ingest and analyze multi-modal data, and then take actions and create new content to fulfill the request. Skip the reading Summarize long documents such as articles, reports, research papers, technical documentation, and even books to quickly and effectively extract important information. Be the designer Create stunningly realistic and visually appealing images for ad campaigns, websites, presentations, and more in an instant. Streamline workflows Automate routine tasks and streamline operations across functions like financial reporting and demand forecasting with agents. Get deep insights Use agents to automate complex data analysis and simulations—allowing researchers and creators to focus on strategic ideation rather than technical execution. Real results, real customers Robinhood Robinhood transformed into an AI-first financial innovator using Amazon Bedrock, scaling from 500 million to 5 billion tokens daily in just six months—while slashing AI costs by 80% and cutting development time in half. According to Dev Tagare, Robinhood's Head of AI, Amazon Bedrock's model diversity, security, and compliance features are purpose-built for regulated industries. Robinhood's success demonstrates how fintech leaders can use Amazon Bedrock to democratize finance with cutting-edge AI while maintaining enterprise-grade security and compliance. Next steps Amazon Bedrock customer success stories Amazon Bedrock documentation Explore common Amazon Bedrock use cases with a guided workshop View demos of Amazon Bedrock capabilities Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_innovation_5_1200.86a666ec42e4300da4044eff19e2fd5809a178ec.png", "local": "images\\3318d040c154b0bcbf4e118538f3f020294ff5f0.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_data_2_1200.64d58473e27cc36cd5efac49d7b300750a8e4e6b.png", "local": "images\\50e219659ef450c1c87c25de96fbc3d0317fd80e.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Prompt_Engineering_4_1200.714389e3df8383b1d11b84bbdece4654c89889ac.png", "local": "images\\ec951315d584eba5ea664ea711b8f7b57f763afe.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Large_Language_Model_1_1200_1.9b916d5df94e6e6380e6f6f4ff0fba09b2e1d1bc.png", "local": "images\\b7a33de2014722604c95f9a5363c18a18ea3b6c3.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/model-choice", "https://aws.amazon.com/bedrock/customize", "https://aws.amazon.com/bedrock/security-compliance/", "https://aws.amazon.com/bedrock/guardrails/", "https://aws.amazon.com/bedrock/security-privacy-responsible-ai/", "https://aws.amazon.com/bedrock/cost-optimization", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/customers/", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html", "https://aws.amazon.com/bedrock/"]}
{"url": "https://aws.amazon.com/bedrock/getting-started/", "text": " Search Amazon Bedrock Getting started with Amazon Bedrock Four pathways to start building Start building in the console 1 Sign into Amazon Bedrock in the console After setting up your Amazon Bedrock IAM role, you can sign into theAmazon Bedrock consoleand request access to foundation models 2 Explore the text playground Request access to Amazon Nova models or other foundation models offered through Amazon Bedrock by following the steps in theAmazon Bedrock user guide. Get started in the Amazon Bedrock API Set up your environment to make Amazon Bedrock requests through the AWS API 1 Generate API key For a step-by-step guide to generate an API key that you can use to quickly access the Amazon Bedrock API, see Get started withAmazon Bedrock API keys: Generate a 30-day keyand make your first API call. 2 Next steps After you've fulfilled all the prerequisites, select one of the following topics to test out making model invocation requests using Amazon Bedrock models: Get started in Amazon SageMaker Unified Studio 1 Access Amazon Bedrock in SageMaker Unified Studio To use Amazon Bedrock in SageMaker Unified Studio, you must be a member of an Amazon SageMaker Unified Studio domain. Your organization will provide you with login information; contact your administrator if you don't have your login details. 2 Start experimenting with Amazon Bedrock in SageMaker Unified Studio playgrounds Find serverless models with theAmazon Bedrock model catalog. Generate text responses from a model by sending text and image prompts in the chat playground or generate and edit images and videos by sending text and image prompts to a suitable model in theimage and video playground. 3 Build, evaluate, and share Create two types of generative AI apps. First, achat agent appto chat with an Amazon Bedrock model through a conversational interface. Second, aflow appto link together prompts, supported Amazon Bedrock models, and other units of work such as a knowledge base, to create generative AI workflows.Evaluatemodels for different task types and thenshare your appto collaborate with your team. Get started with Amazon Bedrock with an AWS SDK  Find the SDK for your use case AWS software development kits (SDKs) are available for many popular programming languages. Each SDK provides an API, code examples, and documentation that make it easier for you to build applications in your preferred language. Find SDK documentation and code examples here Explore essential resources Sharpen your skills with workshops, training, and tutorials Explore common Amazon Bedrock use cases with a guided workshop Get deep-drive trainings on Amazon Bedrock Get started with a step-by-step tutorial Learn Resources Developers Help", "images": [], "links": ["https://aws.amazon.com/bedrock/getting-started/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://console.aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://console.aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-model-access", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-keys.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-keys.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-cli.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-python.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-sm.html", "https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/bedrock.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/sdk-general-information-section.html", "https://aws.amazon.com/bedrock/getting-started/"]}
{"url": "https://aws.amazon.com/bedrock/agentcore/", "text": " Search Amazon Bedrock AgentCore Amazon Bedrock AgentCore Accelerate agents to production with composable services that work with any framework, any model What is Amazon Bedrock AgentCore? Amazon Bedrock AgentCore is an agentic platform to build, deploy, and operate highly capable agents securely at scale. With AgentCore you can easily enable agents to take actions across tools and data, run agents securely with low-latency and extended runtimes, and monitor agents in production - all without any infrastructure management. AgentCore is comprised of fully-managed services that can be used together or independently and work with any framework (including CrewAI, LangGraph, LlamaIndex, Google ADK, OpenAI Agents SDK, and Strands Agents), as well as any foundation model - eliminating the choice between open-source flexibility and enterprise-grade security and reliability. Bedrock AgentCore Capabilities Enhance agents with tools and memory Enable intelligent, personalized experiences with persistent memory that maintains agent knowledge across interactions. Easily connect agents to your existing tools and services with a gateway that transforms them into agent-ready capabilities with minimal code. Enhance your agents with powerful built-in tools including a secure browser runtime to help agents execute complex web-based workflows and a code interpreter to securely execute code for tasks such as generating visualizations. Deploy agents securely at scale Deploy and scale highly-capable agents securely using any framework, protocol, or model in or outside of Amazon Bedrock. Benefit from complete session isolation and industry leading support for long-running workloads up to 8 hours, enabling complex, multi-step agent tasks. Manage agent identity and access seamlessly across your services with native integration to your existing identity providers for automated authentication and permission delegation. Comprehensive observability to boost performance Gain deep operational insights with real-time visibility into your agents' performance through comprehensive monitoring dashboards powered by Amazon CloudWatch. Track key metrics such as token usage, latency, session duration, and error rates to support optimal operations. Trace, debug issues, audit agent decisions, and support compliance with visibility into the entire agent workflow. Integrates seamlessly with your existing observability systems via OpenTelemetry. Bedrock AgentCore Benefits  Faster time to value Accelerate from prototype to production with fully-managed services that eliminate infrastructure complexity, so you can bring groundbreaking agentic solutions to market faster. Flexibility and interoperability Build agents your way using any framework, model, or tool—while maintaining complete control over how your agents operate and integrate with existing systems. Security and trust Deploy with confidence using enterprise-grade security features, including complete session isolation, Amazon VPC connectivity, AWS PrivateLink support, and comprehensive controls—all designed to ensure your agents operate reliably and securely at scale. Bedrock AgentCore Services Deploy agents on secure, serverless infrastructure Prevent data leakage with complete session isolation. Handle any agent use case with support for low latency real-time iterations and 8-hour asynchronous workloads for long-running tasks. Easily connect agents to tools and data Convert APIs and Lambda functions into agent-compatible tools, connect to existing MCP servers, and enable intelligent tool discovery through semantic search. Maintain context across interactions without managing infrastructure Provides customers with control over what the AI agent remembers, delivering context-aware experiences with both short-term and long-term memory, all with zero infrastructure management. Secure, scalable agent identity and access management Allow your agents to securely access AWS resources and third-party tools and services on behalf of users or by themselves with pre-authorized user consent. Monitor agent behavior through intuitive dashboards Provides operational visibility into AI agents for monitoring, debugging, and supporting compliance. Integrates with preferred dashboards via OpenTelemetry compatibility. Execute code securely across multiple languages Enables AI agents to execute code securely in sandbox environments, enhancing their accuracy and expanding their ability to solve complex end-to-end tasks. Fast, secure, and serverless browser runtime for agents Enables agents to dynamically interact with web applications. Offers sub-second latency and compute-based session isolation and observability (Live View, Session Replay). Bedrock AgentCore in Action Transforming AI agents with AgentCore Join Swami Sivasubramanian, VP of Agentic AI at AWS, as he shares the vision behind Amazon Bedrock AgentCore and how it transforms the enterprise AI landscape. Learn how AgentCore's comprehensive suite of services addresses critical business challenges in deploying production-ready agents, from security and scalability to operational excellence. Discover how leading organizations are leveraging AgentCore to accelerate their AI initiatives while maintaining enterprise-grade security and reliability. Getting hands-on with AgentCore Watch AWS Developer Advocate Mike Chambers demonstrate AgentCore services in this hands-on technical walkthrough. Follow along step-by-step as Mike deploys an agent, implements secure tool integrations, and sets up monitoring. This practical demonstration covers AgentCore services including Runtime, Gateway, Memory, and how to leverage AgentCore Observability capabilities to monitor and debug agent behavior in production. Marketing campaign automation with AgentCore Watch Prashanth Athota, SVP of Product Engineering at Epsilon, as he shares how they revolutionized campaign automation using Amazon Bedrock AgentCore. Learn how Epsilon reduced campaign setup time by 30%, increased personalization by 20%, and saved teams 8 hours weekly. Discover how AgentCore enabled them to scale AI-powered marketing while maintaining enterprise security and reliability. Customers Ericsson \"At Ericsson, our 3G/4G/5G/6G systems span millions of lines of code across thousands of interconnected subsystems, representing decades of engineering innovation at the scale of nation-wide critical infrastructure. AgentCore powers our crucial fusion of data and information to deliver AI agents of unprecedented capability in real-world R&D, scaling to double-digit gains across a workforce in the tens of thousands. AgentCore also lets us use any agent framework, which is critical to help us scale across many teams and use cases.\" — Dag Lindbo, Head of AI and Emerging Technologies in Business Area Networks, Ericsson Thomson Reuters \"At Thomson Reuters, we're reimagining content workflows that we've built over decades. We see a significant opportunity with agentic systems - not just for automation, but for process simplification, improving content currency and quality, and modernizing our technology stack. We're exploring Amazon Bedrock AgentCore because of its potential to accelerate how we build and deploy agentic workflows, compressing timelines from months to weeks. What's compelling is how AgentCore reduces our engineers' cognitive load by abstracting away infrastructure complexity - agent runtimes, observability, lifecycle management - so they can focus on solving the business problems that matter.“ — Emre Caglar, Head of Product Engineering, Content and RFC Cox Automotive “At Cox Automotive, we’re transforming our customer experience with generative and agentic AI. AgentCore is one of the strategic platforms we’re using to deploy AI agents at scale, ranging from virtual assistants that improve our omnichannel dealer experience to an agentic marketplace that streamlines vehicle discovery and buying. AgentCore’s key services – Runtime for secured deployments, Observability for monitoring, and Identity for authentication – are enabling our teams to develop and test these agents efficiently as we scale AI across the enterprise\" — Marianne Johnson, EVP & Chief Product Officer, Cox Automotive Amazon Devices “Within Amazon Devices Operations & Supply Chain team, we’re leveraging AgentCore to develop agentic solutions for manufacturing processes to save on development costs and accelerate time-to-market. We implemented specialized agents — including a Task Planning agent for converting business requirements into station-level instructions and a Model Training agent for optimizing robotic vision. As a result, fine-tuning an object detection model, which used to take days of engineering time, can now be done in under an hour with high precision. With AgentCore's built-in monitoring and multi-agent communication capabilities, our teams can spend more time on core functionality rather than infrastructure management.” — John Powell, Leader of Advanced Manufacturing and Quality, Amazon Devices Pricing Tailor AgentCore to your needs—mix and match services, use them independently or together, and pay for what you use as your AI initiatives grow. Learn more on theAgentCore pricing page. Related Services Discover, buy and deploy pre-built agents and tools Explore hundreds of agent solutions from AWS Partners. Deploy pre-built agents from AWS Marketplace on AgentCore, connect to select API-based tools from AWS Marketplace with Gateway. Build agents using a fully-managed service Build agents without managing infrastructure or writing orchestration code. Connect with company systems, APIs, and data, and complete tasks while collaborating with other agents. I want tolearn more about AgentCore Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Prompt_Engineering_4_1200.117726305eec736d80248f6bdbcda62106ac132f.png", "local": "images\\99049a41dbe49950c83ebe503969f4fd92beb3ea.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library-ai-strands.bd74b89899a9e6bc5283c97a4540cd2a899a60e3.png", "local": "images\\a3cdae8ec5c4282ac537631faa1260ba93bbeb4e.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/ericsson-logo.63a48e6996f23f01078151ceb41db635310bb129.png", "local": "images\\7c8da9e797e2e2960a3e4cfdeee58a3782e66ca4.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/thomson-reuters_stacked_logo2.5727c2a8b909cae73857c4f8f40bc78e4ce7d97d.png", "local": "images\\6ef55beec7ed12d10234a01c834d8a4e78435833.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/cox-automotive_logo2.d42187b089b46ccffac1a85311239bd347310972.png", "local": "images\\1fc606a6c757ab13118217108fc0bc165d4574fd.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/amazon_logo2.19aceb2be668d3b9719398f4807ab3d55a79df94.png", "local": "images\\2ca24d571578d1948b5ff508fd2f1f379bfa9051.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/bedrock.2f8686a70625c753a79ed1dde629a123f792f38d.png", "local": "images\\de33d3ca03fb336e4ecfe3c85053ea67d396e29d.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/agentcore/#aws-page-content-main", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/agentcore/pricing/", "https://aws.amazon.com/bedrock/agentcore/resources/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agentcore-get-started-toolkit.html", "https://us-west-2.console.aws.amazon.com/bedrock-agentcore/home?region=us-west-2", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agents-tools-runtime.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/code-interpreter-tool.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/browser-sandbox-tool.html", "https://aws.amazon.com/bedrock/agentcore/pricing/", "https://aws.amazon.com/bedrock/agents/", "https://aws.amazon.com/bedrock/agentcore/"]}
{"url": "https://aws.amazon.com/bedrock/model-choice", "text": " Search Amazon Bedrock Model choice Turn model flexibility into a strategic business advantage—choose from industry-leading foundation models, deploy custom models, or explore specialized models Match your AI workloads to the perfect model every time With access to hundreds of top foundation models (FMs) to power your applications and the ability to swap them in and out without rewriting code, Amazon Bedrock gives you the flexibility to build and innovate as your needs evolve. The Amazon Bedrock built-in evaluation tools help you compare and balance performance, cost, and accuracy across models, turning model selection into a strategic advantage that drives real business outcomes. Broad selection of fully managed models from leading AI companies Unlock your freedom to innovate with flexible AI Access leading models on demand with no infrastructure hassle Take advantage of the latest generative AI innovations with easy access to a choice of high-performing models from leading AI companies like AI21 Labs, Anthropic, Cohere, DeepSeek, Luma AI, Meta, Mistral AI, OpenAI, Qwen, Stability AI, TwelveLabs, Writer, and Amazon. Start building today without provisioning servers or managing infrastructure. Discover emerging and specialized models for domain-specific tasks Amazon Bedrock Marketplace lets you discover, test, and use over 100 popular, emerging, and specialized FMs alongside other industry-leading models in Amazon Bedrock. Maximize existing AI investments by importing custom models Protect your competitive advantage by bringing proprietary models to Amazon Bedrock. Amazon Bedrock Custom Model Import lets you import and use your customized models alongside existing, out-of-the-box FMs through a single, serverless, unified API. You can access your imported custom models on demand and without the need to manage underlying infrastructure. Evaluate and optimize for what matters Build with confidence by comparing and validating responses from different models and Retrieval Augmented Generation (RAG) workflows for your real-world scenarios with Amazon Bedrock Evaluations. Evaluate model outputs for brand voice, friendliness, and relevance and assess workflows for context relevance and correctness. For deeper insights, use LLM-as-a-judge to evaluate outputs for harmfulness, completeness, and accuracy using your own prompts and criteria. Featured model providers Amazon Family of FMs delivering fast and cost effective multimodal intelligence-including text, image, document, and video understanding, image and video generation, interactive speech, and code generation. Anthropic Excels at complex reasoning, code generation, & instruction following. DeepSeek Advanced reasoning models that solve complex problems step-by-step. Mistral AI Specialized expert models for agentic reasoning and multimodal tasks. Meta Advanced image & language reasoning. OpenAI Open weight models that excel at coding, scientific analysis, and mathematical reasoning. Why model choice matters to customers Veolia Discover how Veolia uses almost all FMs available in Amazon Bedrock to automate translation services, image generation, and knowledge querying to solve critical environmental challenges. Vercel Learn how Vercel leverages Amazon Bedrock's breadth of model choice to enable experimentation and innovation while its reliability and scalability powers breakthrough developer toolkits for next-generation experiences. Get started Sign in to the Amazon Bedrock console Learn more about Amazon Bedrock Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Large_Language_Model_2_1200.e58db46b51106302fbbb6bb21819e35b66f6fc0f.png", "local": "images\\f7fe3ae0a0634cdfad2fc0a114687983bb8edb0d.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Large_Language_Model_1_1200_1.9b916d5df94e6e6380e6f6f4ff0fba09b2e1d1bc.png", "local": "images\\b7a33de2014722604c95f9a5363c18a18ea3b6c3.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Prompt_Engineering_1_1200_1.10c1252ea6c474527b08b89a5fc37d4b08648a45.png", "local": "images\\46084ae0eb0fcd90d1e65dff447cf4ed0323c264.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Fine_Tuning_1_1200.5142165611cec32fbea50154e8aa55602134b6c6.png", "local": "images\\eaaedbff7793c5ad2343061cce7dfd7d0c222331.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/model-choice#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://console.aws.amazon.com/bedrock/", "https://console.aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/marketplace/", "https://aws.amazon.com/bedrock/custom-model-import/", "https://aws.amazon.com/bedrock/evaluations/", "https://aws.amazon.com/bedrock/anthropic/", "https://aws.amazon.com/bedrock/deepseek", "https://aws.amazon.com/bedrock/mistral/", "https://aws.amazon.com/bedrock/meta/", "https://aws.amazon.com/bedrock/openai/", "https://console.aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock/latest/userguide/foundation-models-reference.html", "https://aws.amazon.com/bedrock/model-choice"]}
{"url": "https://aws.amazon.com/bedrock/customize", "text": " Search Amazon Bedrock Customize your applications Securely customize generative AI applications and agents with your data Build secure, tailored AI applications with enterprise data Organizations can leverage their unique enterprise data to build differentiated experiences for their business. Using techniques such as Retrieval Augmented Generation (RAG), model fine-tuning, model distillation, and multimodal data processing, you can build generative AI applications tailored to your specific use case. Maintain complete control over sensitive information—your data is never used to train base models or shared with any model providers, including Amazon. Create differentiation for your apps Combine multiple data customization tools to optimize models for domain-specific accuracy Amazon Bedrock Knowledge Bases Amazon Bedrock Knowledge Bases offers an end-to-end managed RAG workflow that lets you create highly accurate, low-latency, secure, and custom generative AI applications by incorporating contextual information from your own data sources. Model fine-tuning Train a foundation model to improve performance on specific tasks (known as fine-tuning) or pretrain a model by familiarizing it with certain types of inputs (known as continued pretraining). Adapt foundation models to your specific needs to improve performance for specialized tasks. Data automation Amazon Bedrock Data Automation is a fully managed API that can easily integrate into your applications. It streamlines the development of generative AI applications and automates workflows involving documents, images, audio, and videos. Model distillation With Amazon Bedrock Model Distillation, you can use smaller, faster, more cost-effective models that deliver use case–specific accuracy—comparable to the most advanced models in Amazon Bedrock. Distilled models in Amazon Bedrock are up to 500% faster and up to 75% less expensive than original models, with less than 2% accuracy loss for use cases like RAG. Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_data_2_1200.f685ef0f3e729061a17718569331a8d8cffaf3a5.png", "local": "images\\58daa57e6ec6771789e3f2dbba40c9888f6c68af.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_innovation_5_1200.1020a232da27f8b4a4de5713cbc5fabaea79f9a3.png", "local": "images\\0ee827cb1a510cbbc0949bcf376599b9fde9b3b8.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/bedrock-whatis.1294cb4a3257953cd8dda6f4528df34959e4f5d3.png", "local": "images\\d76e5ead276610df9660720b18321f9639381ba1.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Continued-PreTraining_1_1200_2.5c8d75b13b0670873787f14fc3260c1b29a8f5b9.png", "local": "images\\cf38e7411d4cc81d10ade3bf1dbbcd1a57a2c1e4.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Large_Language_Model_1_1200_1.df7147bcb113a61eac3f699d46d794e562b51693.png", "local": "images\\4345afdcbbcbf05a45667690ed6ef9ff401e4486.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/customize#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/knowledge-bases/", "https://aws.amazon.com/bedrock/bda/", "https://aws.amazon.com/bedrock/model-distillation/", "https://aws.amazon.com/bedrock/customize"]}
{"url": "https://aws.amazon.com/bedrock/security-compliance/", "text": " Search Amazon Bedrock Amazon Bedrock Security and Privacy Amazon Bedrock helps you keep your data and applications secure and private Secure your generative AI applications With Amazon Bedrock, you have full control over the data you use to customize the foundation models for your generative AI applications. Your data is encrypted in transit and at rest. Additionally, you can create, manage, and control encryption keys using the AWS Key Management Service (AWS KMS). Identity-based policies provide further control over your data, helping you manage what actions users and roles can perform, on which resources, and under what conditions. Build with comprehensive data protection and privacy Amazon Bedrock helps ensure that your data stays under your control. When you tune a foundation model, we base it on a private copy of that model. This means your data is not shared with model providers, and is not used to improve the base models. You can use AWS PrivateLink to establish private connectivity from your Amazon Virtual Private Cloud (VPC) to Amazon Bedrock, without having to expose your VPC to internet traffic. Finally, Bedrock is in scope for common compliance standards including ISO, SOC, CSA STAR Level 2, is HIPAA eligible, and customers can use Bedrock in compliance with the GDPR. Amazon Bedrock is a FedRAMP High authorized service in the AWS GovCloud (US-West) Region. Implement governance, and auditability Amazon Bedrock offers comprehensive monitoring and logging capabilities that can support your governance and audit requirements. You can use Amazon CloudWatch to track usage metrics and build customized dashboards with metrics that can be used for your audit purposes. You can also use AWS CloudTrail to monitor API activity and troubleshoot issues as you integrate other systems into your generative AI applications. You can also choose to store the metadata, requests, and responses in your Amazon Simple Storage Service (Amazon S3) bucket, as well as to Amazon CloudWatch Logs. Finally, to prevent potential misuse, Amazon Bedrock implementsautomated abuse detectionmechanisms. How to get started Get started with a step-by-step tutorial Explore common generative AI use cases with Amazon Bedrock Workshop Securely build generative AI apps & control data with Amazon Bedrock Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/secure-your-generative-ai-apps-1.c34e7bf1f8e4176641c17ce1ac19d99b54c7d87c.jpg", "local": "images\\200385b7e441c525800a0e06528c12af191729d1.jpg", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/build-with-data-protection-1.ad80970bb1115bb06d1329587d5dd513510d518a.jpg", "local": "images\\5f8d3fdb9d2d883e08c0d02bbe8ba1bcc526d2f1.jpg", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/implement-governance-and-auditability-1.2aedf6e96db2379b1e8819c2d71e01277f03abb0.jpg", "local": "images\\ba5b2132fdd2342a3ee499dd3720b011b26fd4c1.jpg", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/security-compliance/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock/latest/userguide/abuse-detection.html", "https://aws.amazon.com/bedrock/security-compliance/"]}
{"url": "https://aws.amazon.com/bedrock/guardrails/", "text": " Search Amazon Bedrock Amazon Bedrock Guardrails Implement safeguards customized to your application requirements and responsible AI policies Build configurable safeguards Amazon Bedrock Guardrails helps you safely build and deploy responsible generative AI applications with confidence. With industry-leading safety protections that block up to 88% of harmful content and deliver auditable, mathematically verifiable explanations for validation decisions with 99% accuracy, Guardrails provides configurable safeguards to help detect and filter harmful text and image content, redact sensitive information, detect model hallucinations, and more. Guardrails work consistently across any foundation model whether you're using models in Amazon Bedrock or self-hosted models including third-party models such as OpenAI and Google Gemini — giving you the same safety, privacy, and responsible AI controls across all your generative AI applications. Key capabilities Comprehensive safeguards for every generative AI application Configurable safeguards Amazon Bedrock Guardrails provides six safeguard policies that you can configure for your generative AI applications based on your use cases and responsible AI policies. These safeguards include content moderation (content and word filters), prompt attack detection, topic classification (denied topics), personally identifiable information (PII) redaction (sensitive information filters), and hallucination detection (contextual grounding and Automated Reasoning checks).  With expanded capabilities for code-related use cases, these safeguards now help protect against harmful content within code elements, detect malicious code injection attempts, and prevent PII exposure in code structures. Customize using any combination to help protect both user inputs and model responses. Deterministic explainability Automated Reasoning checks in Amazon Bedrock Guardrails is the first and only generative AI safeguard to useformal logicto help prevent factual errors from hallucinations. Using sound mathematical techniques to verify, correct, and explain AI-generated information, Automated Reasoning checks systematically validate correct model responses with up to 99% accuracy. You can build auditable generative AI applications with the mathematical certainty that AI responses comply with established policies and domain knowledge, especially important in regulated industries. Consistent level of safety The ApplyGuardrail API allows you to use the configurable safeguards offered by Bedrock Guardrails with any foundation model whether hosted on Amazon Bedrock or self-hosted models, including third-party models such as OpenAI and Google Gemini. You can also use Guardrails with an agent framework such as Strands Agents, including agents deployed using Amazon Bedrock AgentCore. With the ApplyGuardrail API, you can assess content using preconfigured guardrails without invoking foundation models, enabling real-time content moderation. Apply across AI workflows Amazon Bedrock Guardrails integrates seamlessly across your AI application stack, from individual model inference to complex multi-step workflows. Apply guardrails to foundation model interactions, associate them with agents using frameworks like Strands Agents for conversational AI, integrate with knowledge bases for retrieval-augmented generation, and embed within flows for sophisticated multi-node processes. This provides extensive safety protection across many use cases, from simple chatbots to complex enterprise workflows. Companies like Chime Financial, KONE, Panorama, Strava, Remitly, and PwC trust Bedrock Guardrails for their AI applications. Configurable safeguards Content filters Help block model prompts and responses for harmful text and image content, including hate speech, insults, sex, violence, and misconduct. Also, help protect against prompt attacks such as prompt injections and jailbreaks. Denied topics Define a set of topics that are undesirable in the context of your application and block them if detected in user queries or model responses. Word filters Configure filters to help block undesirable words, phrases, and profanity (exact match), including offensive terms, and competitor names. Sensitive information filters Configure filters to help block or mask sensitive information, such as personally identifiable information (PII), or custom regex in user inputs and model responses. Contextual grounding checks Help detect and filter hallucinations in model responses based on grounding in a source and relevance to the user query. Automated Reasoning checks Validate the accuracy of foundation model responses against a set of logical rules to detect hallucinations, suggest corrections, and highlight unstated assumptions in model responses. Resources User guide for Bedrock Guardrails Detailed technical guide to configuring and implementing content filters, PII detection, contextual grounding, and Automated Reasoning checks Expanded support for code-related use cases Help protect against malicious intent with harmful content in code with expanded capabilities Bedrock Guardrails for code-related use cases Safeguard your generative AI workloads from prompt injections An overview of prompt injection risks and strategies for mitigating these risks, including moderation with Bedrock Guardrails Build reliable AI systems with automated reasoning Learn about Automated Reasoning checks and how to establish mathematically rigorous guardrails for generative AI applications Automated Reasoning checks tutorials A series of videos to help you create, test, and refine Automated Reasoning policies in Bedrock Guardrails Build safe and responsible AI applications with guardrails Explore best practices and considerations for configuring safeguard with Bedrock Guardrails Get started Read the FAQs Explore the interactive demo Start building with Bedrock Guardrails Explore the github code samples Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_expertise_5_1200.9ef99866a280244dcb520cdebed9b9a01531a171.jpg", "local": "images\\a634084d98bef005d5f518de36fb9deeac65589f.jpg", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_security_8_1200.5ad67fc67bcbcbf8d5b8dd5a11341a48cf047dcc.jpg", "local": "images\\d76b0ec0127c70b7714780d02a7e4c04e8b5b30b.jpg", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_cloud_1_1200.edf82aacd5d9e202670338b81e79dd0d2f037dae.jpg", "local": "images\\318926c541c4c93e579441e8f8675d3e231e45be.jpg", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/guardrails/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/guardrails", "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-use-independent-api.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-content-filters-overview.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-denied-topics.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-word-filters.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-sensitive-filters.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-contextual-grounding-check.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html", "https://aws.amazon.com/bedrock/faqs/#guardrails", "https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/guardrails", "https://aws.amazon.com/bedrock/guardrails/"]}
{"url": "https://aws.amazon.com/bedrock/security-privacy-responsible-ai/", "text": " Search Amazon Bedrock Amazon Bedrock security, privacy, and responsible AI Deploy generative AI with confidence using enterprise-grade security, privacy, and responsible AI controls Develop generative AI on a trusted foundation Amazon Bedrock provides enterprise-grade security, privacy, and responsible AI safeguards that enable organizations to build generative AI applications with confidence. Your data remains completely secure and private—inputs and outputs are never shared with model providers or used to train base models. Built-in protections include configurable content safeguards, personally identifiable information (PII) detection, and industry-first Automated Reasoning checks that helps prevent hallucinations, providing comprehensive safeguards against harmful content and AI risks. Design AI responsibly with built-in safety and guardrails From data protection to hallucination mitigation—comprehensive enterprise security, privacy, and responsible AI controls for trusted AI deployment Maintain full control over your data Amazon Bedrock never shares your data with model providers or uses it to train foundation models. When you tune a foundation model, we create a private copy specifically for your use, keeping your data separate and secure. With full encryption in transit and at rest, AWS Key Management Service (KMS), and private VPC connectivity through AWS PrivateLink, your most sensitive information remains protected. Build trusted AI applications with configurable safeguards Amazon Bedrock Guardrails helps you build generative AI applications at scale with customizable safeguards that help filter harmful content, detect hallucinations, and redact sensitive information. With industry-leading multi-modal toxicity detection that blocks up to 88% of harmful content, you can filter hate speech, insults, sexual content, violence, misconduct, and prompt attacks using configurable thresholds that match your specific needs. Guardrails also helps you detect and redact personally identifiable information (PII) in user inputs and model responses, with options to select from predefined PII types or define custom sensitive-information types using regular expressions. Deploy these protections across any foundation model of your choice —whether using Amazon Bedrock models, third-party options (OpenAI, Google Gemini), or agent frameworks like Strands Agents—maintaining consistent safety, privacy, and truthfulness controls across all your generative AI applications. Improve AI reliability, accuracy, and explainability with automated reasoning Automated Reasoning checks in Amazon Bedrock Guardrails is the first and only generative AI safeguard that helps prevent factual errors from hallucinations. Using sound mathematical techniques to verify, correct, and logically explain generated information, it identifies correct model responses with up to 99% accuracy and provides provable explanations for why the information is accurate. Contextual grounding checks filter over 75% of hallucinated responses for Retrieval Augmented Generation (RAG) and summarization workloads by validating outputs against source information. Both capabilities help you build more trustworthy AI applications by improving output reliability, accuracy, and explainability. Deploy AI with enterprise governance Amazon Bedrock adheres to key industry regulations including GDPR, HIPAA, SOC, and FedRAMP High, helping organizations deploy AI in regulated environments with confidence. With fine-grained access controls through IAM policies, comprehensive audit trails via CloudTrail, and advanced observability through CloudWatch AppSignals, you can maintain governance and meet compliance requirements. Amazon Bedrock provides the security certifications and controls needed for regulated industries while enabling seamless integration with existing GRC (Governance, Risk, and Compliance) systems. Getting started Amazon Bedrock Guardrails Implement safeguards customized to your application requirements and responsible AI policies Amazon Bedrock security and compliance Build generative AI applications with support for data security and compliance standards Amazon Bedrock Guardrails demo A guided walkthrough showcasing the Bedrock Guardrails capabilities including content filtering, PII detection, and Automated Reasoning checks Security in Amazon Bedrock User guide to configuring data protection, identity management, compliance validation, and infrastructure security for Amazon Bedrock Automated Reasoning checks tutorials A series of videos to help you create, test, and refine Automated Reasoning policies in Bedrock Guardrails User guide for Bedrock Guardrails Detailed technical guide to configuring and implementing content filters, PII detection, contextual grounding, and Automated Reasoning checks Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_expertise_5_1200.9ef99866a280244dcb520cdebed9b9a01531a171.jpg", "local": "images\\a634084d98bef005d5f518de36fb9deeac65589f.jpg", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_security_10_1200.4bd5d9c26cc5174ec40e27d6e20f56cb7d58247c.jpg", "local": "images\\3c184d8ae73f78b19613a7fdf410b12c8bd247ca.jpg", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_cloud_1_1200.edf82aacd5d9e202670338b81e79dd0d2f037dae.jpg", "local": "images\\318926c541c4c93e579441e8f8675d3e231e45be.jpg", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/security-privacy-responsible-ai/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/security-compliance/", "https://aws.amazon.com/bedrock/guardrails/", "https://aws.amazon.com/bedrock/security-compliance/", "https://aws.amazon.com/bedrock/guardrails/", "https://aws.amazon.com/bedrock/security-compliance/", "https://docs.aws.amazon.com/bedrock/latest/userguide/security.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html", "https://aws.amazon.com/bedrock/security-privacy-responsible-ai/"]}
{"url": "https://aws.amazon.com/bedrock/cost-optimization", "text": " Search Amazon Bedrock Optimize for cost, latency, and accuracy Boost accuracy and performance while controlling cost with tools to evaluate and optimize AI Never compromise on performance Amazon Bedrock helps you strike the right balance between cost, latency, and accuracy—so your generative AI applications perform efficiently without overspending. With features like Model Distillation, Intelligent Prompt Routing, prompt caching, and flexible inference options including on-demand, batch, and provisioned throughput, Amazon Bedrock gives you the control to optimize across use cases and scale with confidence. Whether you're serving real-time or batch workloads, Amazon Bedrock lets you build smarter, leaner, and more cost-effective AI systems. Improve performance. Reduce costs. Use prompt caching to reduce costs up to 90% and latency up to 85% for supported models Many foundation model (FM) use cases will reuse certain portions of prompts (prefixes) across API calls. With prompt caching, supported models will let you cache these repeated prompt prefixes between requests. This cache lets the model skip recomputation of matching prefixes. Accelerate prompt engineering for generative AI applications Amazon Bedrock Prompt Management simplifies the creation, evaluation, versioning, and running of prompts to enable developers get the best responses from FMs for their use cases. Maximize performance at lower cost with Intelligent Prompt Routing Amazon Bedrock Intelligent Prompt Routing routes prompts to different FMs within a model family, helping you optimize for quality of responses and cost. Intelligent Prompt Routing can reduce costs by up to 30% without compromising on accuracy. Distilled models in Amazon Bedrock are up to 500% faster and up to 75% less expensive than original models, with less than 2% accuracy loss for use cases like RAG Use smaller, faster, more cost-effective models that deliver use case–specific accuracy—comparable to the most advanced models in Amazon Bedrock. Fine tune a ‘student’ model with a ‘teacher’ model that has the accuracy you want. Customers Yuewen Group Yuewen Group improved the accuracy of tasks and streamlined the prompt engineering process with Bedrock Prompt Optimization Smartsheet Smartsheet boosts developer productivity with Amazon Bedrock and Roo Code Resources Optimizing cost for using foundational models with Amazon Bedrock Track, allocate, and manage your generative AI cost and usage with Amazon Bedrock Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Prompt_Engineering_4_1200.c97e1cfa201609b334db28ab5c5837c4bb16f047.png", "local": "images\\5269109ce5a2441b3f98c75a12e5554eb36677f8.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Prompt_Engineering_1_1200.04666e5ffd8bb955e3cad0a26143ef59594cf4a7.png", "local": "images\\9f003bdbf529ad218dfbb0af8e81a05283887f5a.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Foundational_Models_1_1200_2.6847b4af94b19af69ba0621c4b9a15dbd0d072a6.png", "local": "images\\dab8afc8bf667f31e91d6fd094d53186888d9850.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library_illustration_innovation_5_1200.1020a232da27f8b4a4de5713cbc5fabaea79f9a3.png", "local": "images\\0ee827cb1a510cbbc0949bcf376599b9fde9b3b8.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/cost-optimization#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-routing.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/model-distillation.html#how-md-works", "https://aws.amazon.com/bedrock/cost-optimization"]}
{"url": "https://aws.amazon.com/bedrock/customers/", "text": " Search Amazon Bedrock Amazon Bedrock Customers See how leading organizations are building game-changing AI solutions powered by Amazon Bedrock Featured stories1/5Find out how Veolia supercharges environmental solutions across 50+ countries.Learn moreCase StudyAstraZeneca accelerates drug development decisions and insights with Amazon Bedrock AgentsRead the case studySee how NASDAQ revolutionizes ESG analysis with real-time market intelligenceLearn moreLearn how Adobe transformed developer support with 20% better search accuracy.Learn moreDiscover how Vercel empowers developers with 75M+ AI-powered code generations.Learn more Featured stories Find out how Veolia supercharges environmental solutions across 50+ countries. AstraZeneca accelerates drug development decisions and insights with Amazon Bedrock Agents See how NASDAQ revolutionizes ESG analysis with real-time market intelligence Learn how Adobe transformed developer support with 20% better search accuracy. Discover how Vercel empowers developers with 75M+ AI-powered code generations. Browse more Bedrock customer stories Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/Veolia_White.9471f65f21e75654cf58f7f543bd5afdf719f55c.png", "local": "images\\4e8e48677e94b3ffa1ef108abe26107b057aef32.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/AstraZeneca_AZ_RGB_H_TRANS_V3.30882012d9ea61211e44dbed1d35e1642161be4e.png", "local": "images\\bc5c6ecf7a6845c6ef0b0f4fdd55d4c7c7e64ae0.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/Nasdaq_White_Logo.6651024c0e0ec941f78513c55e203fb29de55c8d.png", "local": "images\\3a789b526904c95146e41d4f85a8f738db561e6a.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/Adobe_White_Logo.b207258a3b41df0609e78eabc55e8c64dc8d2a58.svg", "local": "images\\05a46f9cefafdf4cc17ceb848b5280e3ef3851ec.svg", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/Vercel_White_Logo.6c3f254c32720c6b7e226ded7b32cbf3df8f0d5d.png", "local": "images\\84841639ad20ae559e1244732193523d6dc8debf.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/customers/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/customers/"]}
{"url": "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html", "text": "Get started with Amazon Bedrock Before you can use Amazon Bedrock, you must carry out the following steps: Sign up for an AWS account (if you don't already have one). Create an AWS Identity and Access Management role with the necessary permissions for Amazon Bedrock. Beginning June 15, 2025 access to all Amazon Bedrock foundation models is enabled by default. For Anthropic models, first-time users may need to submit use case details before they can access the model. For more information, seeAccess Amazon Bedrock foundation models. If you're new to AWS and need to sign up for an AWS account, expandI'm new to AWS. Otherwise, skip that step and instead expandI already have an AWS account. If you do not have an AWS account, complete the following steps to create one. Openhttps://portal.aws.amazon.com/billing/signup. Follow the online instructions. Part of the sign-up procedure involves receiving a phone call or text message and entering \n  a verification code on the phone keypad. When you sign up for an AWS account, anAWS account root useris created. The root user has access to all AWS services\n  and resources in the account. As a security best practice, assign administrative access to a user, and use only the root user to performtasks that require root user access. AWS sends you a confirmation email after the sign-up process isn\n    complete. At any time, you can view your current account activity and manage your account by\n    going tohttps://aws.amazon.com/and choosingMy\n     Account. Sign in to theAWS Management Consoleas the account owner by choosingRoot userand entering your AWS account email address. On the next page, enter your password. For help signing in by using root user, seeSigning in as the root userin theAWS Sign-In User Guide. Turn on multi-factor authentication (MFA) for your root user. For instructions, seeEnable a virtual MFA device for your AWS account root user (console)in theIAM User Guide. Enable IAM Identity Center. For instructions, seeEnabling\n      AWS IAM Identity Centerin theAWS IAM Identity Center User Guide. In IAM Identity Center, grant administrative access to a user. For a tutorial about using the IAM Identity Center directory as your identity source, seeConfigure user access with the default IAM Identity Center directoryin theAWS IAM Identity Center User Guide. To sign in with your IAM Identity Center user, use the sign-in URL that was sent to your email address when you created the IAM Identity Center user. For help signing in using an IAM Identity Center user, seeSigning in to the AWS access portalin theAWS Sign-In User Guide. To learn more about IAM, seeIdentity and access management for Amazon Bedrockand theIAM User Guide. After you have created an administrative user, proceed toI already have an AWS accountto set up permissions for Amazon Bedrock. Use IAM to create a role for with the necessary permissions to use Amazon Bedrock. You can then add users to this role to grant the permissions. Create a role with a name of your choice by following the steps atCreating a role to delegate permissions to an IAM userin the IAM User Guide. When you reach the step to attach a policy to the role, attach theAmazonBedrockFullAccessAWS managed policy. Create a new policy to allow your role to manage access to Amazon Bedrock models. From the following list, select the link that corresponds to your method of choice and follow the steps. Use the following JSON object as the policy. Creating IAM policies (console) Creating IAM policies (AWS CLI) Creating IAM policies (AWS API) Attach the policy that you created in the last step to your Amazon Bedrock role by following the steps atAdding and removing IAM identity permissions. For users to access an IAM role, you must add them to the role. You can add both users in your account or from other accounts. To grant users permissions to switch to the Amazon Bedrock role that you created, follow the steps atGranting a user permissions to switch rolesand specify the Amazon Bedrock role as theResource. If you need to create more users in your account so that you can give them access to the Amazon Bedrock role, follow the steps inCreating an IAM user in your AWS account. After you've granted a user permissions to use the Amazon Bedrock role, provide the user with role name and ID or alias of the account to which the role belongs. Then, guide the user through how to switch to the role by following the instructions atProviding information to the user. Explore Amazon Bedrock features through the console or API After requesting access to the foundation models that you want to use, you'll be ready to explore the different capabilities offered by Amazon Bedrock. If you want to familiarize yourself more with Amazon Bedrock first, you can continue to the following pages: To learn how to run basic prompts and generate model responses using thePlaygroundsin the Amazon Bedrock console, continue toGet started in the Amazon Bedrock console. To learn how to set up access to Amazon Bedrock operations through the Amazon Bedrock API and test out some API calls, continue toGet started with the API. To learn about the software development kits (SDKs) supported by Amazon Bedrock, continue toUsing Amazon Bedrock with an AWS SDK. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf#getting-started", "https://docs.aws.amazon.com/bedrock/index.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-explore", "https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#new-to-aws", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-bedrock-role", "https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-bedrock-role", "https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam-awsmanpol.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-console.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/sdk-general-information-section.html"]}
{"url": "https://aws.amazon.com/bedrock/getting-started/#aws-page-content-main", "text": " Search Amazon Bedrock Getting started with Amazon Bedrock Four pathways to start building Start building in the console 1 Sign into Amazon Bedrock in the console After setting up your Amazon Bedrock IAM role, you can sign into theAmazon Bedrock consoleand request access to foundation models 2 Explore the text playground Request access to Amazon Nova models or other foundation models offered through Amazon Bedrock by following the steps in theAmazon Bedrock user guide. Get started in the Amazon Bedrock API Set up your environment to make Amazon Bedrock requests through the AWS API 1 Generate API key For a step-by-step guide to generate an API key that you can use to quickly access the Amazon Bedrock API, see Get started withAmazon Bedrock API keys: Generate a 30-day keyand make your first API call. 2 Next steps After you've fulfilled all the prerequisites, select one of the following topics to test out making model invocation requests using Amazon Bedrock models: Get started in Amazon SageMaker Unified Studio 1 Access Amazon Bedrock in SageMaker Unified Studio To use Amazon Bedrock in SageMaker Unified Studio, you must be a member of an Amazon SageMaker Unified Studio domain. Your organization will provide you with login information; contact your administrator if you don't have your login details. 2 Start experimenting with Amazon Bedrock in SageMaker Unified Studio playgrounds Find serverless models with theAmazon Bedrock model catalog. Generate text responses from a model by sending text and image prompts in the chat playground or generate and edit images and videos by sending text and image prompts to a suitable model in theimage and video playground. 3 Build, evaluate, and share Create two types of generative AI apps. First, achat agent appto chat with an Amazon Bedrock model through a conversational interface. Second, aflow appto link together prompts, supported Amazon Bedrock models, and other units of work such as a knowledge base, to create generative AI workflows.Evaluatemodels for different task types and thenshare your appto collaborate with your team. Get started with Amazon Bedrock with an AWS SDK  Find the SDK for your use case AWS software development kits (SDKs) are available for many popular programming languages. Each SDK provides an API, code examples, and documentation that make it easier for you to build applications in your preferred language. Find SDK documentation and code examples here Explore essential resources Sharpen your skills with workshops, training, and tutorials Explore common Amazon Bedrock use cases with a guided workshop Get deep-drive trainings on Amazon Bedrock Get started with a step-by-step tutorial Learn Resources Developers Help", "images": [], "links": ["https://aws.amazon.com/bedrock/getting-started/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://console.aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://console.aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-model-access", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-keys.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-keys.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-cli.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-python.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-sm.html", "https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/bedrock.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/sdk-general-information-section.html", "https://aws.amazon.com/bedrock/getting-started/"]}
{"url": "https://console.aws.amazon.com/bedrock/", "text": "", "images": [], "links": []}
{"url": "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "text": "What is Amazon Bedrock? Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from\n  leading AI companies and Amazon available for your use through a unified API. You can choose from a\n  wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also\n  offers a broad set of capabilities to build generative AI applications with security, privacy, and\n  responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top foundation models for\n  your use cases, privately customize them with your data using techniques such as fine-tuning and\n  Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise\n  systems and data sources. With Amazon Bedrock's serverless experience, you can get started quickly, privately customize\n  foundation models with your own data, and easily and securely integrate and deploy them into your\n  applications using AWS tools without having to manage any infrastructure. What can I do with Amazon Bedrock? How do I get started with Amazon Bedrock? Amazon Bedrock pricing Key terminology What can I do with Amazon Bedrock? You can use Amazon Bedrock to do the following: Experiment with prompts and configurationsâSubmit prompts and generate responses with model inferenceby sending prompts using different\n     configurations and foundation models to generate responses. You can use the API or the text,\n     image, and chat playgrounds in the console to experiment in a graphical interface. When you're\n     ready, set up your application to make requests to theInvokeModelAPIs. Augment response generation with information from your data\n      sourcesâCreate knowledge basesby\n     uploading data sources to be queried in order to augment a foundation model's generation of\n     responses. Create applications that reason through how to help a\n      customerâBuild agentsthat use foundation\n     models, make API calls, and (optionally) query knowledge bases in order to reason through and\n     carry out tasks for your customers. Adapt models to specific tasks and domains with training\n      dataâCustomize an Amazon Bedrock foundation\n      modelby providing training data for fine-tuning or continued-pretraining in order to\n     adjust a model's parameters and improve its performance on specific tasks or in certain\n     domains. Improve your FM-based application's efficiency and outputâPurchase Provisioned Throughputfor a foundation\n     model in order to run inference on models more efficiently and at discounted rates. Determine the best model for your use caseâEvaluate outputs of different modelswith built-in or custom\n     prompt datasets to determine the model that is best suited for your application. Prevent inappropriate or unwanted contentâUse guardrailsto implement safeguards for your generative AI\n     applications. Optimize your FM's latencyâGet faster response times and improved responsivenessfor AI applications with\n        Latency-optimized inference for foundation models. The Latency Optimized Inference feature is in preview release for Amazon Bedrock and is\n        subject to change. To learn about Regions that support Amazon Bedrock and the foundation models and features that Amazon Bedrock supports, seeSupported foundation models in Amazon BedrockandFeature support by AWS Region in Amazon Bedrock. How do I get started with Amazon Bedrock? We recommend that you start with Amazon Bedrock by  doing the following: Familiarize yourself\n     with theterms and conceptsthat Amazon Bedrock uses. Understand how AWSchargesyou for using Amazon Bedrock. Try theGet started with Amazon Bedrocktutorials. In the tutorials, you learn how to use the playgrounds inAmazon Bedrock console. You also learn and how to use theAWS SDKto call Amazon Bedrock API operations. Read the documentation for the features that you want to include\n     in your application. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf#what-is-bedrock", "https://docs.aws.amazon.com/bedrock/index.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#servicename-feature-overview", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#first-time-user", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#servicename-feature-overview", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#first-time-user", "https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-pricing.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/inference.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/features-regions.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-pricing.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api.html"]}
{"url": "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-model-access", "text": "Get started with Amazon Bedrock Before you can use Amazon Bedrock, you must carry out the following steps: Sign up for an AWS account (if you don't already have one). Create an AWS Identity and Access Management role with the necessary permissions for Amazon Bedrock. Beginning June 15, 2025 access to all Amazon Bedrock foundation models is enabled by default. For Anthropic models, first-time users may need to submit use case details before they can access the model. For more information, seeAccess Amazon Bedrock foundation models. If you're new to AWS and need to sign up for an AWS account, expandI'm new to AWS. Otherwise, skip that step and instead expandI already have an AWS account. If you do not have an AWS account, complete the following steps to create one. Openhttps://portal.aws.amazon.com/billing/signup. Follow the online instructions. Part of the sign-up procedure involves receiving a phone call or text message and entering \n  a verification code on the phone keypad. When you sign up for an AWS account, anAWS account root useris created. The root user has access to all AWS services\n  and resources in the account. As a security best practice, assign administrative access to a user, and use only the root user to performtasks that require root user access. AWS sends you a confirmation email after the sign-up process isn\n    complete. At any time, you can view your current account activity and manage your account by\n    going tohttps://aws.amazon.com/and choosingMy\n     Account. Sign in to theAWS Management Consoleas the account owner by choosingRoot userand entering your AWS account email address. On the next page, enter your password. For help signing in by using root user, seeSigning in as the root userin theAWS Sign-In User Guide. Turn on multi-factor authentication (MFA) for your root user. For instructions, seeEnable a virtual MFA device for your AWS account root user (console)in theIAM User Guide. Enable IAM Identity Center. For instructions, seeEnabling\n      AWS IAM Identity Centerin theAWS IAM Identity Center User Guide. In IAM Identity Center, grant administrative access to a user. For a tutorial about using the IAM Identity Center directory as your identity source, seeConfigure user access with the default IAM Identity Center directoryin theAWS IAM Identity Center User Guide. To sign in with your IAM Identity Center user, use the sign-in URL that was sent to your email address when you created the IAM Identity Center user. For help signing in using an IAM Identity Center user, seeSigning in to the AWS access portalin theAWS Sign-In User Guide. To learn more about IAM, seeIdentity and access management for Amazon Bedrockand theIAM User Guide. After you have created an administrative user, proceed toI already have an AWS accountto set up permissions for Amazon Bedrock. Use IAM to create a role for with the necessary permissions to use Amazon Bedrock. You can then add users to this role to grant the permissions. Create a role with a name of your choice by following the steps atCreating a role to delegate permissions to an IAM userin the IAM User Guide. When you reach the step to attach a policy to the role, attach theAmazonBedrockFullAccessAWS managed policy. Create a new policy to allow your role to manage access to Amazon Bedrock models. From the following list, select the link that corresponds to your method of choice and follow the steps. Use the following JSON object as the policy. Creating IAM policies (console) Creating IAM policies (AWS CLI) Creating IAM policies (AWS API) Attach the policy that you created in the last step to your Amazon Bedrock role by following the steps atAdding and removing IAM identity permissions. For users to access an IAM role, you must add them to the role. You can add both users in your account or from other accounts. To grant users permissions to switch to the Amazon Bedrock role that you created, follow the steps atGranting a user permissions to switch rolesand specify the Amazon Bedrock role as theResource. If you need to create more users in your account so that you can give them access to the Amazon Bedrock role, follow the steps inCreating an IAM user in your AWS account. After you've granted a user permissions to use the Amazon Bedrock role, provide the user with role name and ID or alias of the account to which the role belongs. Then, guide the user through how to switch to the role by following the instructions atProviding information to the user. Explore Amazon Bedrock features through the console or API After requesting access to the foundation models that you want to use, you'll be ready to explore the different capabilities offered by Amazon Bedrock. If you want to familiarize yourself more with Amazon Bedrock first, you can continue to the following pages: To learn how to run basic prompts and generate model responses using thePlaygroundsin the Amazon Bedrock console, continue toGet started in the Amazon Bedrock console. To learn how to set up access to Amazon Bedrock operations through the Amazon Bedrock API and test out some API calls, continue toGet started with the API. To learn about the software development kits (SDKs) supported by Amazon Bedrock, continue toUsing Amazon Bedrock with an AWS SDK. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf#getting-started", "https://docs.aws.amazon.com/bedrock/index.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-explore", "https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#new-to-aws", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-bedrock-role", "https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-bedrock-role", "https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam-awsmanpol.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-console.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/sdk-general-information-section.html"]}
{"url": "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-keys.html", "text": "Get started with Amazon Bedrock API keys: Generate a 30-day key and make your first API call This tutorial walks you through creating a long-term Amazon Bedrock API key that expires in 30 days and using it to make a simpleConverseAPI call using Python. This is the fastest way to start experimenting with Amazon Bedrock without setting up complex AWS credentials. Long-term API keys are recommended only for exploration and development of Amazon Bedrock. For production applications, usealternatives to long-term access keyssuch as IAM roles or temporary credentials. Follow these steps to create a long-term Amazon Bedrock API key that expires in 30 days: Sign in to the AWS Management Console with an IAM identity that has permissions to use the Amazon Bedrock console. Then, open the Amazon Bedrock console athttps://console.aws.amazon.com/bedrock. In the left navigation pane, selectAPI keys. In theLong-term API keystab, chooseGenerate long-term API keys. In theAPI key expirationsection, select30 days. ChooseGenerate. The key you generate provides permissions to carry out core Amazon Bedrock actions, as defined in the attachedAmazonBedrockLimitedAccesspolicy. Copy the generated API key and store it securely. You'll need this key for the next step. The API key is only displayed once. Make sure to copy and save it before closing the dialog. Remember that your API key will expire in 30 days. You can generate a new one by following the same steps, or consider transitioning to more secure authentication methods for ongoing use. Set the API key as an environment variable by replacing${api-key}with your generated API key value and use it to generate a response in your method of choice: Congratulations! You've successfully generated an Amazon Bedrock API key and made your first API call to the Amazon Bedrock service. After exploring some more Amazon Bedrock actions, you should transition to more secure methods of authentication such as short-term Amazon Bedrock API keys or AWS-wide temporary credentials. Refer to the following resources to learn more: Explore different modelsâ Learn about other foundation models available in Amazon Bedrock atAmazon Bedrock foundation model informationand change themodel_idin your code to try them out. Learn about model inferenceâ Learn about generating responses with model inference by reading about concepts and the options available in Amazon Bedrock atSubmit prompts and generate responses with model inference. Plan for production with more secure authentication methodsâ Read about Amazon Bedrock API keys in greater detail atGenerate Amazon Bedrock API keys to easily authenticate to the Amazon Bedrock APIand how to create more secure, short-term Amazon Bedrock API keys. When you're ready to build production applications, you should also reviewalternatives to long-term access keysfor more secure options that also allow access to other AWS services. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf#getting-started-api-keys", "https://docs.aws.amazon.com/bedrock/index.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html", "https://console.aws.amazon.com/bedrock", "https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonBedrockLimitedAccess", "https://docs.aws.amazon.com/bedrock/latest/userguide/foundation-models-reference.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/inference.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/api-keys.html"]}
{"url": "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-cli.html", "text": "Run example Amazon Bedrock API requests with the AWS Command Line Interface This section guides you through trying out some common operations in Amazon Bedrock using the AWS Command Line Interface to test that your permissions and authentication are set up properly. Before you run the following examples, you should check that you have fulfilled the following prerequisites: Prerequisites You have an AWS account and a user or role with authentication set up and the necessary permissions for Amazon Bedrock. Otherwise, follow the steps atGet started with the API. You've installed and set up authentication for the AWS CLI. To install the AWS CLI, follow the steps atInstall or update to the latest version of the AWS CLI. Verify that you've set up your credentials to use the CLI by following the steps atGet credentials to grant programmatic access. Test that your permissions are set up properly for Amazon Bedrock, using a user or role that you set up with the proper permissions. List the foundation models that Amazon Bedrock has to offer Submit a text prompt to a model and generate a text response with InvokeModel Submit a text prompt to a model and generate a text response with Converse List the foundation models that Amazon Bedrock has to offer The following example runs theListFoundationModelsoperation using the AWS CLI.ListFoundationModelslists the foundation models (FMs) that are available in Amazon Bedrock in your Region. In a terminal, run the following command: If the command is successful, the response returns a list of foundation models that are available in Amazon Bedrock. Submit a text prompt to a model and generate a text response with InvokeModel The following example runs theInvokeModeloperation using the AWS CLI.InvokeModellets you submit a prompt to generate a model response. In a terminal, run the following command: If the command is successful, the response generated by the model is written to theinvoke-model-output-text.txtfile. The text response is returned in theoutputTextfield, alongside accompanying information. Submit a text prompt to a model and generate a text response with Converse The following example runs theConverseoperation using the AWS CLI.Converselets you submit a prompt to generate a model response. We recommend usingConverseoperation overInvokeModelwhen supported, because it unifies the inference request across Amazon Bedrock models and simplifies the management of multi-turn conversations. In a terminal, run the following command: If the command is successful, the response generated by the model is returned in thetextfield, alongside accompanying information. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf#getting-started-api-ex-cli", "https://docs.aws.amazon.com/bedrock/index.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-cli.html#getting-started-api-ex-cli-listfm", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-cli.html#getting-started-api-ex-cli-invoke-text", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-cli.html#getting-started-api-ex-cli-converse", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api.html#gs-grant-program-access", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-cli.html#getting-started-api-ex-cli-listfm", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-cli.html#getting-started-api-ex-cli-invoke-text", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-cli.html#getting-started-api-ex-cli-converse", "https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html", "https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html", "https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html"]}
{"url": "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-python.html", "text": "Run example Amazon Bedrock API requests through the AWS SDK for Python (Boto3) This section guides you through trying out some common operations in Amazon Bedrock with the AWS\n        Python to test that your permissions and authentication are set up properly. Before you\n        run the following examples, you should check that you have fulfilled the following\n        prerequisites: Prerequisites You have an AWS account and a user or role with authentication set up and the necessary permissions for Amazon Bedrock. Otherwise, follow the steps atGet started with the API. You've installed and set up authentication for the AWS SDK for Python (Boto3). To install Boto3, follow the steps atQuickstartin the Boto3 documentation. Verify that you've set up your credentials to use Boto3 by following the steps atGet credentials to grant programmatic access. Test that your permissions are set up properly for Amazon Bedrock, using a user or role that you set up with the proper permissions. The Amazon Bedrock documentation also includes code examples for other programming languages. For more information, seeCode examples for Amazon Bedrock using AWS SDKs. List the foundation models that Amazon Bedrock has to offer Submit a text prompt to a model and generate a text response with InvokeModel Submit a text prompt to a model and generate a text response with Converse List the foundation models that Amazon Bedrock has to offer The following example runs theListFoundationModelsoperation using an Amazon Bedrock client.ListFoundationModelslists the foundation models (FMs) that are available in Amazon Bedrock in your Region. Run the following SDK for Python script to create an Amazon Bedrock client and test theListFoundationModelsoperation: If the script is successful, the response returns a list of foundation models that are available in Amazon Bedrock. Submit a text prompt to a model and generate a text response with InvokeModel The following example runs theInvokeModeloperation using an Amazon Bedrock client.InvokeModellets you submit a prompt to generate a model response. Run the following SDK for Python script to create an Amazon Bedrock runtime client and generate a text response with theoperation: If the command is successful, the response returns the text generated by the model in response to the prompt. Submit a text prompt to a model and generate a text response with Converse The following example runs theConverseoperation using an Amazon Bedrock client. We recommend usingConverseoperation overInvokeModelwhen supported, because it unifies the inference request across Amazon Bedrock models and simplifies the management of multi-turn conversations. Run the following SDK for Python script to create an Amazon Bedrock runtime client and generate a text response with theConverseoperation: If the command is successful, the response returns the text generated by the model in response to the prompt. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf#getting-started-api-ex-python", "https://docs.aws.amazon.com/bedrock/index.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-python.html#getting-started-api-ex-python-listfm", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-python.html#getting-started-api-ex-python-invoke-text", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-python.html#getting-started-api-ex-python-converse", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api.html#gs-grant-program-access", "https://docs.aws.amazon.com/bedrock/latest/userguide/service_code_examples.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-python.html#getting-started-api-ex-python-listfm", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-python.html#getting-started-api-ex-python-invoke-text", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-python.html#getting-started-api-ex-python-converse", "https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html", "https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html", "https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html", "https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html"]}
{"url": "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-sm.html", "text": "Run example Amazon Bedrock API requests using an Amazon SageMaker AI notebook This section guides you through trying out some common operations in Amazon Bedrock with an Amazon SageMaker AI notebook to test that your Amazon Bedrock role permissions are set up properly. Before you run the following examples, you should check that you have fulfilled the following prerequisites: Prerequisites You have an AWS account and have permissions to access a role with the necessary permissions for Amazon Bedrock. Otherwise, follow the steps atI already have an AWS account. Carry out the following steps to set up IAM permissions for SageMaker AI and create a notebook: Modify thetrust policyof the Amazon Bedrock role that you set up inI already have an AWS accountthrough theconsole,CLI, orAPI. Attach the following trust policy to the role to allow both the Amazon Bedrock and SageMaker AI services to assume the Amazon Bedrock role: Sign into the Amazon Bedrock role whose trust policy you just modified. Follow the steps atCreate an Amazon SageMaker AI Notebook Instance for the tutorialand specify the ARN of the Amazon Bedrock role that you created to create an SageMaker AI notebook instance. When theStatusof the notebook instance isInService, choose the instance and then chooseOpen JupyterLab. After you open up your SageMaker AI notebook, you can try out the following examples: List the foundation models that Amazon Bedrock has to offer Submit a text prompt to a model and generate a response List the foundation models that Amazon Bedrock has to offer The following example runs theListFoundationModelsoperation using an Amazon Bedrock client.ListFoundationModelslists the foundation models (FMs) that are available in Amazon Bedrock in your Region. Run the following SDK for Python script to create an Amazon Bedrock client and test theListFoundationModelsoperation: If the script is successful, the response returns a list of foundation models that are available in Amazon Bedrock. Submit a text prompt to a model and generate a response The following example runs theConverseoperation using an Amazon Bedrock client.Converselets you submit a prompt to generate a model response. Run the following SDK for Python script to create an Amazon Bedrock runtime client and test theConverseoperation: If the command is successful, the response returns the text generated by the model in response to the prompt. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf#getting-started-api-ex-sm", "https://docs.aws.amazon.com/bedrock/index.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-sm.html#getting-started-api-ex-sm-listfm", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-sm.html#getting-started-api-ex-sm-converse", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-bedrock-role", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-bedrock-role", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-sm.html#getting-started-api-ex-sm-listfm", "https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api-ex-sm.html#getting-started-api-ex-sm-converse", "https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html", "https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html", "https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html", "https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html"]}
{"url": "https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/bedrock.html", "text": "Amazon Bedrock in SageMaker Unified Studio With Amazon Bedrock in SageMaker Unified Studio you can build generative AI apps that use Amazon Bedrock models and features, such as knowledge bases and guardrails, without needing to write any code. To use Amazon Bedrock in SageMaker Unified Studio, you must be a member of an Amazon SageMaker Unified Studio domain. Your organization\n    will provide you with login details. Contact your administrator if you don't have your login details. Your organization's administrator determines which Amazon Bedrock models and features you\n    have access to. Contact your organization's administrator if you need access to a model or feature that you don't currently have access to. If you are administrator and need information about managing Amazon Bedrock in SageMaker Unified Studio, seeAmazon Bedrock in SageMaker Unified Studioin theAmazon SageMaker Unified Studio admin guide. Discover Amazon Bedrock in SageMaker Unified Studio playgrounds Amazon Bedrock in SageMaker Unified Studio provides various options for discovering and experimenting with\n      Amazon Bedrock models and apps. With the model catalog you can find information about the Amazon Bedrock models that are available to you and decide which model is\n      suitable for your use case. Different models have different capabilities and modalities. For more information, seeFind serverless models with the Amazon Bedrock model\n            catalog. Amazon Bedrock in SageMaker Unified Studio offers two playgrounds for you to experiment with Amazon Bedrock models in: thechatplayground and theimage and videoplayground. With the chat playground, \n      you can generate text responses from a model by sending text and image prompts. You can also interact with chat agent apps that have been shared with you. \n      With the image and video playground, you can generate and edit images and videos by sending text and image prompts to a suitable model. For more information, seeExperiment with the Amazon Bedrock playgrounds. Build generative AI apps Within an Amazon SageMaker Unified Studio project, you can create two types of generative AI apps: achat agent appand aflow app. You\n      can use a chat agent app to chat with an Amazon Bedrock model through a conversational interface, typically by sending prompts (text or image) and\n      receiving responses. You can use a flows ap to link prompts, supported Amazon Bedrock models, and other units of work, such as a knowledge base,\n      together and create generative AI workflows. Apps that you create with Amazon Bedrock in SageMaker Unified Studio can integrate the following Amazon Bedrock features. Data sourcesâ Enrich apps by including context that is received from querying a knowledge base\n          or a document. Guardrailsâ\n          Implement safeguards for your Amazon Bedrock in SageMaker Unified Studio app based on your use cases and responsible AI\n          policies. Functionsâ\n          Call a function with a model to access a specific capability when handling a prompt. Promptsâ\n          Access reusable prompts that you can use in a flow app. Within a project, you can use theasset galleryto organize the\n      prompts and components that you use for an app. A component is an Amazon Bedrock knowledge base,\n      guardrail, or function. A critical part of creating a generative AI app is deciding which model to use and which\n      model settings to use. To help you decide, you canevaluatea model for different task types. If you work on a team, you can collaborate bysharingan\n      app with other team members. You can alsoexportan app so\n      that you can use the app in your own environment. You can clone the repository that holds your Amazon SageMaker Unified Studio project files to your\n      computer. However, we don't recommend making changes to your project's files on your local desktop, as this may break your project's apps and components. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/bedrock.html#getting-started-explore", "https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/bedrock.html#getting-started-build", "https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/bedrock-explore-chat-playground.html", "https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/bedrock-playgrounds.html"]}
{"url": "https://docs.aws.amazon.com/bedrock/latest/userguide/sdk-general-information-section.html", "text": "Using Amazon Bedrock with an AWS SDK AWS software development kits (SDKs) are available for many popular programming languages. Each SDK provides an API, code examples, and documentation that \n        make it easier for developers to build applications in their preferred language. AWS SDK for C++ AWS SDK for C++ code examples AWS CLI AWS CLI code examples AWS SDK for Go AWS SDK for Go code examples AWS SDK for Java AWS SDK for Java code examples AWS SDK for JavaScript AWS SDK for JavaScript code examples AWS SDK for Kotlin AWS SDK for Kotlin code examples AWS SDK for .NET AWS SDK for .NET code examples AWS SDK for PHP AWS SDK for PHP code examples AWS Tools for PowerShell AWS Tools for PowerShell code examples AWS SDK for Python (Boto3) AWS SDK for Python (Boto3) code examples AWS SDK for Ruby AWS SDK for Ruby code examples AWS SDK for Rust AWS SDK for Rust code examples AWS SDK for SAP ABAP AWS SDK for SAP ABAP code examples AWS SDK for Swift AWS SDK for Swift code examples Can't find what you need? Request a code example by using theProvide feedbacklink at the bottom of this page. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf#sdk-general-information-section", "https://docs.aws.amazon.com/bedrock/index.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html"]}
{"url": "https://aws.amazon.com/bedrock/agentcore/#aws-page-content-main", "text": " Search Amazon Bedrock AgentCore Amazon Bedrock AgentCore Accelerate agents to production with composable services that work with any framework, any model What is Amazon Bedrock AgentCore? Amazon Bedrock AgentCore is an agentic platform to build, deploy, and operate highly capable agents securely at scale. With AgentCore you can easily enable agents to take actions across tools and data, run agents securely with low-latency and extended runtimes, and monitor agents in production - all without any infrastructure management. AgentCore is comprised of fully-managed services that can be used together or independently and work with any framework (including CrewAI, LangGraph, LlamaIndex, Google ADK, OpenAI Agents SDK, and Strands Agents), as well as any foundation model - eliminating the choice between open-source flexibility and enterprise-grade security and reliability. Bedrock AgentCore Capabilities Enhance agents with tools and memory Enable intelligent, personalized experiences with persistent memory that maintains agent knowledge across interactions. Easily connect agents to your existing tools and services with a gateway that transforms them into agent-ready capabilities with minimal code. Enhance your agents with powerful built-in tools including a secure browser runtime to help agents execute complex web-based workflows and a code interpreter to securely execute code for tasks such as generating visualizations. Deploy agents securely at scale Deploy and scale highly-capable agents securely using any framework, protocol, or model in or outside of Amazon Bedrock. Benefit from complete session isolation and industry leading support for long-running workloads up to 8 hours, enabling complex, multi-step agent tasks. Manage agent identity and access seamlessly across your services with native integration to your existing identity providers for automated authentication and permission delegation. Comprehensive observability to boost performance Gain deep operational insights with real-time visibility into your agents' performance through comprehensive monitoring dashboards powered by Amazon CloudWatch. Track key metrics such as token usage, latency, session duration, and error rates to support optimal operations. Trace, debug issues, audit agent decisions, and support compliance with visibility into the entire agent workflow. Integrates seamlessly with your existing observability systems via OpenTelemetry. Bedrock AgentCore Benefits  Faster time to value Accelerate from prototype to production with fully-managed services that eliminate infrastructure complexity, so you can bring groundbreaking agentic solutions to market faster. Flexibility and interoperability Build agents your way using any framework, model, or tool—while maintaining complete control over how your agents operate and integrate with existing systems. Security and trust Deploy with confidence using enterprise-grade security features, including complete session isolation, Amazon VPC connectivity, AWS PrivateLink support, and comprehensive controls—all designed to ensure your agents operate reliably and securely at scale. Bedrock AgentCore Services Deploy agents on secure, serverless infrastructure Prevent data leakage with complete session isolation. Handle any agent use case with support for low latency real-time iterations and 8-hour asynchronous workloads for long-running tasks. Easily connect agents to tools and data Convert APIs and Lambda functions into agent-compatible tools, connect to existing MCP servers, and enable intelligent tool discovery through semantic search. Maintain context across interactions without managing infrastructure Provides customers with control over what the AI agent remembers, delivering context-aware experiences with both short-term and long-term memory, all with zero infrastructure management. Secure, scalable agent identity and access management Allow your agents to securely access AWS resources and third-party tools and services on behalf of users or by themselves with pre-authorized user consent. Monitor agent behavior through intuitive dashboards Provides operational visibility into AI agents for monitoring, debugging, and supporting compliance. Integrates with preferred dashboards via OpenTelemetry compatibility. Execute code securely across multiple languages Enables AI agents to execute code securely in sandbox environments, enhancing their accuracy and expanding their ability to solve complex end-to-end tasks. Fast, secure, and serverless browser runtime for agents Enables agents to dynamically interact with web applications. Offers sub-second latency and compute-based session isolation and observability (Live View, Session Replay). Bedrock AgentCore in Action Transforming AI agents with AgentCore Join Swami Sivasubramanian, VP of Agentic AI at AWS, as he shares the vision behind Amazon Bedrock AgentCore and how it transforms the enterprise AI landscape. Learn how AgentCore's comprehensive suite of services addresses critical business challenges in deploying production-ready agents, from security and scalability to operational excellence. Discover how leading organizations are leveraging AgentCore to accelerate their AI initiatives while maintaining enterprise-grade security and reliability. Getting hands-on with AgentCore Watch AWS Developer Advocate Mike Chambers demonstrate AgentCore services in this hands-on technical walkthrough. Follow along step-by-step as Mike deploys an agent, implements secure tool integrations, and sets up monitoring. This practical demonstration covers AgentCore services including Runtime, Gateway, Memory, and how to leverage AgentCore Observability capabilities to monitor and debug agent behavior in production. Marketing campaign automation with AgentCore Watch Prashanth Athota, SVP of Product Engineering at Epsilon, as he shares how they revolutionized campaign automation using Amazon Bedrock AgentCore. Learn how Epsilon reduced campaign setup time by 30%, increased personalization by 20%, and saved teams 8 hours weekly. Discover how AgentCore enabled them to scale AI-powered marketing while maintaining enterprise security and reliability. Customers Ericsson \"At Ericsson, our 3G/4G/5G/6G systems span millions of lines of code across thousands of interconnected subsystems, representing decades of engineering innovation at the scale of nation-wide critical infrastructure. AgentCore powers our crucial fusion of data and information to deliver AI agents of unprecedented capability in real-world R&D, scaling to double-digit gains across a workforce in the tens of thousands. AgentCore also lets us use any agent framework, which is critical to help us scale across many teams and use cases.\" — Dag Lindbo, Head of AI and Emerging Technologies in Business Area Networks, Ericsson Thomson Reuters \"At Thomson Reuters, we're reimagining content workflows that we've built over decades. We see a significant opportunity with agentic systems - not just for automation, but for process simplification, improving content currency and quality, and modernizing our technology stack. We're exploring Amazon Bedrock AgentCore because of its potential to accelerate how we build and deploy agentic workflows, compressing timelines from months to weeks. What's compelling is how AgentCore reduces our engineers' cognitive load by abstracting away infrastructure complexity - agent runtimes, observability, lifecycle management - so they can focus on solving the business problems that matter.“ — Emre Caglar, Head of Product Engineering, Content and RFC Cox Automotive “At Cox Automotive, we’re transforming our customer experience with generative and agentic AI. AgentCore is one of the strategic platforms we’re using to deploy AI agents at scale, ranging from virtual assistants that improve our omnichannel dealer experience to an agentic marketplace that streamlines vehicle discovery and buying. AgentCore’s key services – Runtime for secured deployments, Observability for monitoring, and Identity for authentication – are enabling our teams to develop and test these agents efficiently as we scale AI across the enterprise\" — Marianne Johnson, EVP & Chief Product Officer, Cox Automotive Amazon Devices “Within Amazon Devices Operations & Supply Chain team, we’re leveraging AgentCore to develop agentic solutions for manufacturing processes to save on development costs and accelerate time-to-market. We implemented specialized agents — including a Task Planning agent for converting business requirements into station-level instructions and a Model Training agent for optimizing robotic vision. As a result, fine-tuning an object detection model, which used to take days of engineering time, can now be done in under an hour with high precision. With AgentCore's built-in monitoring and multi-agent communication capabilities, our teams can spend more time on core functionality rather than infrastructure management.” — John Powell, Leader of Advanced Manufacturing and Quality, Amazon Devices Pricing Tailor AgentCore to your needs—mix and match services, use them independently or together, and pay for what you use as your AI initiatives grow. Learn more on theAgentCore pricing page. Related Services Discover, buy and deploy pre-built agents and tools Explore hundreds of agent solutions from AWS Partners. Deploy pre-built agents from AWS Marketplace on AgentCore, connect to select API-based tools from AWS Marketplace with Gateway. Build agents using a fully-managed service Build agents without managing infrastructure or writing orchestration code. Connect with company systems, APIs, and data, and complete tasks while collaborating with other agents. I want tolearn more about AgentCore Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Prompt_Engineering_4_1200.117726305eec736d80248f6bdbcda62106ac132f.png", "local": "images\\99049a41dbe49950c83ebe503969f4fd92beb3ea.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/aws-library-ai-strands.bd74b89899a9e6bc5283c97a4540cd2a899a60e3.png", "local": "images\\a3cdae8ec5c4282ac537631faa1260ba93bbeb4e.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/ericsson-logo.63a48e6996f23f01078151ceb41db635310bb129.png", "local": "images\\7c8da9e797e2e2960a3e4cfdeee58a3782e66ca4.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/thomson-reuters_stacked_logo2.5727c2a8b909cae73857c4f8f40bc78e4ce7d97d.png", "local": "images\\6ef55beec7ed12d10234a01c834d8a4e78435833.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/cox-automotive_logo2.d42187b089b46ccffac1a85311239bd347310972.png", "local": "images\\1fc606a6c757ab13118217108fc0bc165d4574fd.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/amazon_logo2.19aceb2be668d3b9719398f4807ab3d55a79df94.png", "local": "images\\2ca24d571578d1948b5ff508fd2f1f379bfa9051.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/bedrock.2f8686a70625c753a79ed1dde629a123f792f38d.png", "local": "images\\de33d3ca03fb336e4ecfe3c85053ea67d396e29d.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/agentcore/#aws-page-content-main", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/agentcore/pricing/", "https://aws.amazon.com/bedrock/agentcore/resources/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agentcore-get-started-toolkit.html", "https://us-west-2.console.aws.amazon.com/bedrock-agentcore/home?region=us-west-2", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agents-tools-runtime.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/code-interpreter-tool.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/browser-sandbox-tool.html", "https://aws.amazon.com/bedrock/agentcore/pricing/", "https://aws.amazon.com/bedrock/agents/", "https://aws.amazon.com/bedrock/agentcore/"]}
{"url": "https://aws.amazon.com/bedrock/agentcore/pricing/", "text": " Search Amazon Bedrock AgentCore Amazon Bedrock AgentCore Pricing Tailor AgentCore to your needs—mix and match services, use them independently or together, and pay for what you use as your AI initiatives grow. Pay only for what you use Amazon Bedrock AgentCore offers flexible, consumption-based pricing with no upfront commitments or minimum fees. Each service—Runtime, Gateway, Identity, Memory, Observability, Browser, and Code Interpreter —can be used independently or together, and you pay only for what you use. This modular approach allows you to start small and scale as your agent applications grow. Get started with AgentCore at no cost - new AWS customers receive up to $200 in Free Tier credits. ExploreAWS Free Tierbenefits and start building today. Explore AgentCore Pricing using ourPricing Calculator. AgentCore Services  Runtime AgentCore Runtime is a secure, serverless runtime purpose-built for deploying and scaling agents and tools. Choose between direct code deployment for rapid iteration or container-based deployment for maximum control. You only pay for the active resources you consumeUnlike traditional compute services that charge for pre-allocated resources (i.e., fixed instance size and cost per second while hosting the agent), with AgentCore Runtime you only pay for active resource consumption. This delivers substantial cost savings for agentic workloads, which typically spend 30-70% of time in I/O wait (waiting for LLM responses, tool / API calls, or database queries). With pre-allocated pricing, you would pay for idle CPU during these wait periods. With the active resource consumption-based pricing in AgentCore Runtime, I/O wait and idle time is free, if no other background process is running. Billing is based on CPU and memory consumption across your session lifetime, calculated at per-second increments. For CPU resources, you are charged based on actual consumption - if your agent consumes no CPU during I/O wait, there are no CPU charges. For memory resources, you're charged for the peak memory consumed up to that second. Key details: Browser AgentCore Browser tool provides a fast, secure, cloud-based browser runtime to enable agents to interact with websites at scale. You only pay for the active resources you consumeUnlike traditional compute services that charge for pre-allocated resources (i.e., fixed instance size and cost per second while hosting the agent), with AgentCore Browser you only pay for active resource consumption. This delivers substantial cost savings for agentic workloads, which typically spend 30-70% of time in I/O wait (waiting for LLM responses, tool / API calls, or database queries). With pre-allocated pricing, you would pay for idle CPU during these wait periods. With the active resource consumption-based pricing in AgentCore Browser, I/O wait and idle time is free, if no other background process is running. Billing is based on CPU and memory consumption across your session lifetime, calculated at per-second increments. For CPU resources, you are charged based on actual consumption - if your agent consumes no CPU during I/O wait, there are no CPU charges. For memory resources, you're charged for the peak memory consumed up to that second. Key details: Code Interpreter AgentCore Code Interpreter tool enables agents to execute code securely in sandbox environments, enhancing their accuracy and expanding their ability to solve complex end-to-end tasks. You only pay for the active resources you consumeUnlike traditional compute services that charge for pre-allocated resources (i.e., fixed instance size and cost per second while hosting the agent), with AgentCore Code Interpreter you only pay for active resource consumption. This delivers substantial cost savings for agentic workloads, which typically spend 30-70% of time in I/O wait (waiting for LLM responses, tool / API calls, or database queries). With pre-allocated pricing, you would pay for idle CPU during these wait periods. With the active resource consumption-based pricing in AgentCore Code Interpreter, I/O wait and idle time is free, if no other background process is running. Billing is based on CPU and memory consumption across your session lifetime, calculated at per-second increments. For CPU resources, you are charged based on actual consumption - if your agent consumes no CPU during I/O wait, there are no CPU charges. For memory resources, you're charged for the peak memory consumed up to that second. Key details: Gateway Amazon Bedrock AgentCore Gateway enables agents to securely access tools by transforming APIs and Lambda functions into agent-compatible tools and connecting to existing MCP servers. Consumption-based pricingYou pay only for the API calls your agents make through Gateway. You're charged based on the number of MCP operations (such as ListTools, CallTool, and Ping), search queries, and tools indexed for semantic search functionality. Key details:  Identity AgentCore Identity simplifies agent identity and access management and allows your agents to securely access AWS resources and third-party tools and services on behalf of users or by themselves with pre-authorized user consent. Consumption-based pricingCustomers who use AgentCore Identity through either AgentCore Runtime or AgentCore Gateway, do not incur any additional charges for their use of AgentCore Identity. For all other scenarios, you pay for only what you use and are charged based on the number of requests from the agent to AgentCore Identity for an OAuth token or an API key. Key details: Memory AgentCore Memory makes it easy for developers to build context-aware agents by eliminating complex memory infrastructure management while providing full control over what the agent remembers. Consumption-based pricingYou only pay for what you use. Our pricing is simple and usage-based, aligning directly with how your agents create value: Key details: Observability AgentCore Observability gives developers complete visibility into agent workflows to trace, debug, and monitor agents' performance in production environments. Consumption-based pricingYou pay as you go for telemetry generated, stored, and queried for your agents. The telemetry data is ingested and stored in your Amazon CloudWatch account. You are charged for data ingestion and storage, queries to retrieve and analyze information, and masking of sensitive/Personally Identifiable Information (PII) data in logs. To review pricing details visit Amazon CloudWatchpricing page. Pricing Table Consumption based Consumption based CPU Memory $0.0895 per vCPU-hour $0.00945 per GB-hour Consumption Based Consumption Based CPU Memory $0.0895 per vCPU-hour $0.00945 per GB-hour Consumption Based Consumption Based CPU Memory $0.0895 per vCPU-hour $0.00945 per GB-hour Consumption Based Consumption Based Consumption Based API Invocations (ListTools, InvokeTool, Ping) Search API Tool Indexing $0.005 per 1,000 invocations $0.025 per 1,000 invocations $0.02 per 100 tools indexed per month Consumption Based Token or API key requests for non-AWS resources $0.010 per 1,000 token or API keys requested by the agent (Note: AgentCore Identity is available at no additional charge to customers when they use it through either AgentCore Runtime or AgentCore Gateway) Consumption Based Consumption Based Consumption Based Short-Term Memory Long-Term Memory Storage Long-Term Memory Retrieval $0.25 per 1,000 new events Using built-in memory strategies: $0.75 per 1000 memory records stored per month Using built-in with override or self-managed memory strategies: $0.25 per 1000 memory records stored per month* $0.50 per 1000 memory record retrievals Consumption Based Spans, logs, and metrics Charged as perAmazon CloudWatch pricing *For built-in with override and self-managed strategies, you may incur additional charges for the model usage in your account Pricing Examples  Runtime Example: Customer Support Agent Deployment You plan to deploy a customer support agent that resolves user queries across chat and email. The agent handles order issues, account verification, and policy clarifications. It uses retrieval-augmented generation (RAG) to fetch product policies, and Model Context Protocol (MCP)-compatible tools to query order status and update support tickets. Each agent session involves sophisticated multi-step reasoning with 1 RAG call to a vector store, 2 MCP tool calls (e.g., OrderAPI, TicketAPI), and 2 LLM reasoning steps. You deployed your agent on AgentCore Runtime because you require complete session isolation and the flexibility to scale to thousands of sessions in seconds. Processing 10M user requests monthly, each session runs for 60 seconds with 70% I/O wait time (waiting for LLM responses and API calls), and no other background process is running during I/O. Each agent session utilizes 1vCPU during active processing. Memory usage starts at 1GB during initialization, increases to 2GB during RAG processing, then peaks at 2.5GB during complex tool calls for the remainder of the session. Your monthly costs break down as follows: CPU cost per session: 18 seconds (only active processing time) × 1vCPU × ($0.0895/3600) = $0.0004475Memory cost per session: 10 seconds × 1GB × ($0.00945/3600) + 20 seconds × 2GB × ($0.00945/3600) + 30 seconds × 2.5GB × ($0.00945/3600) = $0.000026 + $0.000053 + $0.000197 = $0.000276Total cost per session: $0.0007235 Monthly total: 10M sessions × $0.0007235 = $7,235 Storage costs: With container-based deployment, you manage ECR storage separately based on published ECR rates. If you used direct code deployment instead, S3 Standard pricing (starting February 27, 2026) would apply for your code artifacts - for a 100MB agent, this adds up to $0.0023/month in storage costs. Comparison to pre-allocated pricing: If this workload ran on a service charging for pre-allocated resources, you would need to provision for peak usage (2.5GB memory, 1vCPU) for the entire 60-second duration, resulting in up to 3.3x higher CPU costs and up to 1.4x higher memory costs. In the example above, the CPU cost savings are significant, given that CPU consumption accounted for 62% of the total workload cost. AgentCore Runtime's consumption-based pricing automatically captures the \"I/O wait is free\" benefit and charges only for actual memory usage as it scales up, delivering substantial cost efficiency compared to traditional compute options. Browser Example: Automated Travel Booking System You plan to create a travel booking agent that automates full trip planning and booking through web interactions. Your implementation requires AgentCore Browser's secure, serverless runtime to dynamically manage headless browsers for searching flights, hotels, simulating clicks, extracting prices, and submitting booking forms. AgentCore Browser tool provides enterprise-grade capabilities including session-isolated sandbox compute and comprehensive observability through Live View and Session Replay. The agent processes 100K monthly requests. Each browser session runs for 10 minutes with 80% I/O wait time. During active processing it utilizes 2vCPU and 4GB memory continuously, and during I/O it is utilizing 0.4vCPU and 5GB memory. Your monthly costs break down as follows: CPU cost per session: 120 seconds (adjusting for 80% I/O wait) × 2 vCPU × ($0.0895/3600) = $0.005967Memory cost per session: 600 seconds × 4GB × ($0.00945/3600) = $0.0063Total cost per session: $0.012267Monthly total: 100K sessions × $0.012267 = $1,226.67 Code Interpreter Example: Natural Language Data Analysis Automation You plan to deploy a data analyst agent that supports business and product teams with dataset queries, visualizations, and statistical analysis—all through natural language. Your agent dynamically generates and executes Python code for complex requests like correlation analysis between site traffic and conversion rates. You leverage AgentCore Code Interpreter because it provides isolated sandbox environments compliant with enterprise security policies, pre-built execution runtimes for multiple languages (JavaScript, TypeScript, Python), and large file size support. The agent processes 10K monthly requests with 3 code executions per request. Each execution runs for 2 minutes with 60% I/O wait time, utilizing 2vCPU during active processing and 4GB memory continuously. Your monthly costs break down as follows: CPU cost per session: 48 seconds (adjusting for 60% I/O wait) × 2 vCPU × ($0.0895/3600) = $0.002387Memory cost per session: 120 seconds × 4GB × ($0.00945/3600) = $0.00126Total cost per session: $0.003647Monthly total: 30K executions × $0.003647 = $109.40 Gateway Example: Connecting HR Assistant agent to internal tools You plan to build an HR assistant agent for a mid-sized enterprise, handling internal policy questions, leave balances, benefits enrollment, and payroll inquiries. To serve the user requests, the agent needs to access multiple internal systems (Onboarding, Benefits, Payroll, and Leave Management APIs) as tools. You used AgentCore Gateway to create MCP servers for 200 internal tools that your agent can interact with from anywhere, all without writing any code. To improve tool use accuracy, you leveraged the search capability to index tool metadata and enable dynamic matching of tools during agent invocation based on interaction context. Each agent interaction requires 1 Search API and 4 InvokeTool API invocations. 50M monthly interactions result in 50M Search and 200M InvokeTool calls. Your monthly costs break down as follows: SearchToolIndex charges: 200 tools × $0.02 per 100 tools = $0.04Search API charges: 50M × $25/million = $1,250InvokeTool API charges: 200M × $5/million = $1,000Monthly total: $2,250.04 Identity Example: Secure Customer Support Access Management You plan to operate a customer support agent that assists technical teams by accessing multiple tools—Slack for support conversations, Zoom to fetch call logs, and GitHub for issue tracking and commit logs. Your implementation uses AgentCore Identity for secure, delegated access for users or support engineers. The system is compatible with existing identity providers ( e.g. Amazon Cognito, Okta, Microsoft Entra ID) and manages all authentication methods from OAuth tokens to API keys, eliminating the need for custom security infrastructure. Lets assume the agent is being used by 10K monthly active users averaging 5 interactions each, requiring 3 tool accesses per session for each user per month, your monthly costs break down as follows: Total tokens requested: 10K users × 5 sessions × 3 tools = 150K tokensMonthly total: 150K requests × $0.010/1,000 = $1.50 Note: AgentCore Identity is included at no additional cost when using AgentCore Runtime or Gateway. Memory Example: Personalized Coding Assistant Agent Implementation You plan to develop a coding assistant agent that helps software engineers write, debug, and refactor code across IDEs and terminals. To provide a personalized experience, the agent needs to maintain context during a session and remember user preferences over multiple sessions. Your implementation uses AgentCore Memory for equipping the agent with both short-term memory (immediate conversations and events) and long-term memory (persistent knowledge across sessions).Each time a user interacts with the agent (e.g., by sending a code snippet or asking a coding question), you send an event to AgentCore Memory for storing it as short-term memory. For long-term memory, you configured built-in extraction strategies to automatically extract and store summarization of debugging sessions and user preferences across sessions. The agent can then retrieve these long-term memories to provide a personalized experience for developers. With 100,000 monthly short-term memory events, 10,000 stored long-term memory records, and 20,000 monthly memory record retrieval calls, your costs break down as follows: Short-term memory: 100,000 events × $0.25/1,000 = $25Long-term memory storage: 10,000 memories × $0.75/1,000 = $7.50Long-term memory retrieval: 20,000 retrievals × $0.50/1,000 = $10Monthly total: $42.50 Note: With built-in with override extraction strategies, long-term storage cost would be lower at $0.25 per 1000 memories stored. However, you may incur additional charges for model usage in your account. Observability Example: Multi-Agent Financial Advisory Platform You plan to deploy a comprehensive financial advisory platform with multiple specialized agents handling investment research, portfolio analysis, and regulatory compliance checks. Each agent performs complex multi-step reasoning with database and websearch queries, API calls to financial data providers, and document analysis. The platform processes millions of transactions and generates extensive telemetry data including traces, metrics, and logs across all agent interactions. You use AgentCore Observability to monitor performance, debug issues, and ensure compliance with financial regulations through comprehensive audit trails. Your platform generates 10 TB of observability data monthly from agent interactions, API calls, and system events. Assuming 30% of the spans are event logs (model invocation, tool call), then approximately 6TB (assuming 2KB of log data per span event) are written to CloudWatch standard logs. Your monthly costs break down as follows: Monthly Span Ingestion charges: 10 TB × 1,000 GB/TB × $0.35/GB = $3,500Monthly Event Logging charges: 6 TB × 1,000 GB/TB × $0.50/GB = $3,000Monthly total: $3,500 + $3,00 = $6,500 *Standard CloudWatch rates will apply for any metrics and non-telemetry (standard) logs data sent to CloudWatch. I want tolearn more about AgentCore Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [], "links": ["https://aws.amazon.com/bedrock/agentcore/pricing/#aws-page-content-main", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/agentcore/pricing/", "https://aws.amazon.com/bedrock/agentcore/resources/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/agentcore/pricing/"]}
{"url": "https://aws.amazon.com/bedrock/agentcore/resources/", "text": " Search Amazon Bedrock AgentCore Amazon Bedrock AgentCore Resources Find essential resources to master AgentCore and accelerate your agentic AI deployment Navigate AgentCore Resources Find everything you need to master AgentCore Discover the resources you need to deploy and operate production-ready agents with AgentCore. Whether you're getting started or optimizing existing implementations, we provide materials for each AgentCore service - Runtime, Gateway, Memory, Browser tool, Code Interpreter, Identity, and Observability. From technical deep-dives to implementation guides, find the resources that match your deployment stage. Watch the AgentCore deep-dive video to get started Follow along as AWS Developer Advocate Mike Chambers demonstrates AgentCore services in this hands-on technical walkthrough. This practical demonstration covers how to deploy, enhance and monitor agents in production. AgentCore Overview Go from prototype to production with AgentCore Building your first production-ready agent with AgentCore AgentCore Runtime Securely launch and scale agents with AgentCore Runtime In-depth video walkthrough of Runtime AgentCore Gateway Transform enterprise agent tools with AgentCore Gateway In-depth video walkthrough of Gateway AgentCore Memory Build context-aware agents with AgentCore Memory In-depth video walkthrough of Memory AgentCore Browser Tool Enable web automation with AgentCore Browser Tool In-depth video walkthrough of Browser AgentCore Code Interpreter Execute code securely with AgentCore Code Interpreter In-depth video walkthrough of Code Interpreter AgentCore Identity Secure agents at scale with AgentCore Identity In-depth video walkthrough of Identity AgentCore Observability Build trustworthy agents with AgentCore Observability In-depth video walkthrough of Observability I want tolearn more about AgentCore Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [], "links": ["https://aws.amazon.com/bedrock/agentcore/resources/#aws-page-content-main", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/agentcore/pricing/", "https://aws.amazon.com/bedrock/agentcore/resources/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/agentcore/", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agentcore-get-started-toolkit.html", "https://aws.amazon.com/bedrock/agentcore/resources/"]}
{"url": "https://aws.amazon.com/bedrock/agentcore/faqs/", "text": " Search Amazon Bedrock AgentCore Amazon Bedrock AgentCore FAQs Page topics General What is Amazon Bedrock AgentCore? Amazon Bedrock AgentCore is an agentic platform to build, deploy and operate highly capable agents securely at scale. AgentCore lets you build agents faster, enable agents to take actions across tools and data, run agents securely with low-latency and extended runtimes, and monitor agents in production - all without any infrastructure management. AgentCore helps developers to accelerate agents into production with the scale, reliability, and security critical to real-world deployment. Its services are composable and work with any open-source framework and any model, so you don’t have to choose between open-source flexibility and enterprise-grade security and reliability. Who is AgentCore designed for? AgentCore is designed for organizations who want to move agents from proofs of concept built using open-source or custom agent frameworks to production. It serves developers and enterprises who need robust infrastructure to support dynamic execution paths at runtime, controls to monitor behavior, powerful tools to enhance agents, and the flexibility to adapt as the landscape evolves. What key services and tools does AgentCore provide? AgentCore consists of seven modular services:Runtime: Provides a secure, serverless environment purpose-built for deploying and scaling dynamic agents and tools.Memory: Enables developers to build context-aware agents by eliminating complex memory infrastructure management while providing full control over agent memory.Gateway: Offers an easy and secure way for agents to access tools by transforming APIs and Lambda functions into agent-compatible tools and connecting to existing MCP servers.Browser tool: Provides a fast, secure, cloud-based browser runtime to enable agents to interact with websites.Code Interpreter: Enables agents to write and execute code securely in sandbox environments, enhancing their accuracy and expanding their ability to solve complex end-to-end tasks.Identity: Allows agents to securely access and operate across AWS resources or third-party tools and services, on behalf of users or by themselves.Observability: Gives developers complete visibility into agent workflows to trace, debug, and monitor agents' performance. Which agent frameworks does AgentCore support? AgentCore works with custom frameworks and any open-source framework, including CrewAI, LangGraph, LlamaIndex, Google ADK, OpenAI Agents SDK, and Strands Agents. What is AgentCore's stance on supporting opensource protocols? AgentCore supports Model Context Protocol (MCP) and Agent to Agent Protocol (A2A). A2A support is currently available in AgentCore Runtime, with broader A2A support across other AgentCore services coming soon. By supporting these emerging standards, AgentCore aims to be the preferred choice for hosting agents regardless of the protocols used. Which foundation models can I use with AgentCore? AgentCore is designed to be model-agnostic, working with any foundation model in or outside of Amazon Bedrock including OpenAI, Google's Gemini, Anthropic's Claude, Amazon Nova, Meta Llama, and Mistral models. How does Strands Agents integrate with AgentCore? Strands Agents enables developers to leverage modern models' capabilities for planning, reasoning, and tool use, while seamlessly integrating with AgentCore services through a simple SDK. Developers can connect to AgentCore Gateway, configure memory stores, and deploy agents with just a few lines of code. Which regions is AgentCore available in? AgentCore is available in nine AWS Regions: Asia Pacific (Mumbai), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe (Dublin), Europe (Frankfurt), US East (N. Virginia), US East (Ohio), and US West (Oregon). This expanded regional coverage allows customers to deploy closer to their users and data, improving performance and helping to meet data residency requirements across geographical locations. How does AgentCore help accelerate development? AgentCore accelerates development by eliminating months of infrastructure work. With just a few lines of code, it integrates with any framework - including LangChain, Strands Agents, and CrewAI - and model, while providing essential services (Runtime, Memory, Gateway, Browser tool, Code Interpreter, Identity, and Observability) that help build, deploy, and operate your agents in production. Automatic infrastructure provisioning lets developers focus on innovation while ensuring protocol compatibility, reducing development time from months to hours. I am using Amazon Bedrock Agents today. Should I switch to AgentCore? If you are using Amazon Bedrock Agents today, you can continue to do so. With that said, AgentCore is an agentic platform that provides enhanced capabilities including support for any open-source framework including Strands Agents, LangChain, LangGraph, LlamaIndex, and CrewAI and the flexibility to use any foundation model of your choice, whether in or outside of Amazon Bedrock. AgentCore delivers enterprise-grade capabilities with Model Context Protocol (MCP) support for standardized tool access, Virtual Private Cloud (VPC) connectivity for secure network access, and Agent-to-Agent (A2A) support for inter-agent communication.AgentCore's agentic infrastructure services include Runtime for secure, serverless deployment, Memory for customizable context retention, Gateway for seamless tool integration, Browser tool and Code Interpreter for enhanced agent capabilities, Identity for fine-grained access control, and Observability for comprehensive monitoring. These services work together to help you move agents from prototype to production with the scale, reliability, and security critical for real-world deployment. Does AgentCore offer VPC connectivity? Yes, AgentCore offers VPC connectivity across all its services - Runtime, Memory, Gateway, Browser tool, Code Interpreter, Identity, and Observability. This allows secure access to resources within your private network environment. You can configure each AgentCore service to integrate with your VPC to ensure secure communication between agents and your internal resources. Runtime What is AgentCore Runtime? AgentCore Runtime is a secure, serverless runtime purpose-built for deploying and scaling dynamic agents and tools using any open-source framework (including CrewAI, LangGraph, LlamaIndex, Google ADK, Openagents SDK, and Strands Agents), any protocol (MCP and A2A), and any model (i.e. Bedrock, OpenAI, Gemini, etc.). Developers can securely and reliably run any type of agent including multi-modal, real-time, or long-running agents in a VPC-enabled environment with PrivateLink support. The service is highly reliable with checkpointing and recovery capabilities to ensure graceful recovery in case of unexpected interruptions and failures, and can scale up to thousands of agent sessions in seconds so developers don't have to worry about managing infrastructure and only pay for actual usage. It provides complete session isolation with dedicated execution environments for each user interaction. AgentCore Runtime also seamlessly integrates with leading identity providers such as Amazon Cognito, Microsoft Entra ID, and Okta, as well as popular OAuth providers such as Google and GitHub. It supports all authentication methods, from OAuth tokens and API keys to IAM roles, so developers don't have to build custom security infrastructure. What are the key benefits of AgentCore Runtime? 1/Accelerate time to market:Deploy and host any agent or tool using your preferred open-source frameworks, models, and tools without needing to manage any infrastructure. With native support for MCP and A2A protocols, developers can build sophisticated agent-to-agent and agent-to-tool interactions while focusing on innovative capabilities, speeding up deployment and freeing developers to focus on building innovative agents.2/Seamlessly scale from real-time to multi-hour agentic workloads:AgentCore Runtime supports both interactive experiences with low latency, and complex asynchronous workloads running up to 8 hours along across any modality. AgentCore Runtime automatically handles scaling from zero to thousands of concurrent sessions, eliminating capacity planning and infrastructure maintenance.3/Deploy with enterprise-grade security and compliance:Protect sensitive data with true session isolation that provides dedicated compute environments for each user interaction, VPC connectivity and PrivateLink support. AgentCore Runtime seamlessly integrates with your existing identity providers (Amazon Cognito, Microsoft Entra ID, and Okta) to limit who can authenticate in your agent, and manages credentials for downstream services like Salesforce, Github, and Stripe—delivering security without development overhead. Gateway What is AgentCore Gateway? Amazon Bedrock AgentCore Gateway enables agents to easily discover and securely connect with tools through a unified endpoint. It transforms APIs and Lambda functions into agent-compatible tools and connects to existing MCP servers, secured by native IAM enforcement and OAuth integration. Gateway eliminates weeks of custom integration work, accelerating innovative agent application development. It provides 1-click integration with popular tools like Salesforce, Slack, Jira, Asana, and Zendesk. By handling complex tool management and security at enterprise scale, Gateway eliminates weeks of custom integration work so developers can focus on building innovative agent applications. What are the key benefits of AgentCore Gateway? 1/Accelerate agent development through unified access:Combine multiple tool sources—from APIs to Lambda functions to MCP servers—into one unified endpoint. This single, secure endpoint with native IAM enforcement and OAuth integration enables your agents to discover and use tools effortlessly so developers can build and scale agent workflows faster without managing multiple tool connections or reimplementing integrations.2/Simplify tool development and integration:Transform existing enterprise resources into agent-ready tools in just a few lines of code and seamlessly connect to existing MCP servers and popular tools like Salesforce, Slack, Jira, Asana, and Zendesk. AgentCore Gateway handles the complex tasks of tool management and security at enterprise scale, freeing developers to focus on building differentiated agent capabilities.3/Scale with confidence through intelligent tool discovery:As your tool collection grows, help your agents find and use the right tools through contextual search. Built-in semantic search capabilities help agents effectively utilize available tools based on their task context, improving agent performance and reducing development complexity at scale. How does AgentCore Gateway help with tool selection and filtering? AgentCore Gateway includes built-in semantic search to help agents identify the most relevant tools for their tasks, and supports metadata-based filtering to manage tool access based on criteria like risk levels, improving agent efficiency and security. What types of tools can I use with AgentCore Gateway? AgentCore Gateway enables developers to bring a wide range of tools through a unified interface. These include AWS services (S3, DynamoDB, Aurora, Redshift, Lambda), and third-party services. Developers can also integrate custom tools using API specifications, function code, MCP servers, OpenAPI, Smithy, Lambda functions, or containerized solutions via ECR images. What security measures does AgentCore Gateway provide? AgentCore Gateway provides multiple authentication methods including IAM-based, OAuth 2.1, and API keys. It offers secure credential exchange mechanisms between different identity providers. Through integration with AgentCore Observability, customers gain detailed visibility into authentication events, tool invocations, and access patterns. AgentCore Gateway also supports web application firewall capabilities with configurable web ACLs to filter malicious requests. For more details seeAgentCore Gateway documentation. How does AgentCore Gateway work with other AgentCore and AWS Services? AgentCore Gateway works with AgentCore Runtime for secure tool execution, AgentCore Identity for authentication and authorization, and AgentCore Observability for comprehensive metrics and audit logs. AWS Partner tools procured throughAWS Marketplacecan be automatically imported into AgentCore Gateway. Through these integrations, developers can access a wide range of tools and services through a unified interface while maintaining enterprise-grade security and monitoring capabilities. Memory What is AgentCore Memory? AgentCore Memory makes it easy for developers to build context-aware agents using any open-source framework. Memory provides industry-leading accuracy along with support for both short-term memory for multi-turn conversations and long-term memory that persists across sessions, with the ability to share memory stores across agents. The capability offers unique flexibility, allowing developers to create custom extraction logic using their preferred large language models and prompts to capture exactly what matters for their use case. With managed infrastructure that handles vector embeddings and memory consolidation along with enterprise features like observability, developers can focus on building intelligent agents rather than managing memory systems. What are the key benefits of AgentCore Memory? 1/Eliminates infrastructure management: AgentCore Memory eliminates the need for developers to manage complex memory infrastructure. Developers can store and retrieve memories with just a few lines of code while AgentCore automatically handles vector embeddings, storage, and memory consolidation behind the scenes.2/Enterprise-grade: AgentCore memory provides encrypted, namespaces-based storage with VPC support to allow the developer to segment memory based on their preferred taxonomy such as by user, project, or business unit, keeping data isolated and easy to retrieve.3/Deep customization: AgentCore Memory provides developers with the option to use pre-defined strategies to extract user preferences and facts across sessions, or create custom extraction logic using their preferred large language models and prompts to capture exactly what matters for their use case. Code Interpreter What is AgentCore Code Interpreter? AgentCore Code Interpreter enables agents to write and execute code securely in sandbox environments, enhancing their accuracy and expanding their ability to solve complex end-to-end tasks. Code Interpreter comes with pre-built runtimes for multiple languages and advanced features, including large file support and internet access. Developers can customize environments with specific instance types and session properties to meet security requirements. Code Interpreter reduces manual intervention while enabling sophisticated AI development without compromising security or performance. What are the key benefits of AgentCore Code Interpreter? 1/Execute code securely:Develop agents that can perform complex workflows and data analysis in isolated sandbox environments with VPC support, while accessing internal data sources without exposing sensitive data or compromising security.2/Large-scale data processing: When working with large datasets, developers can easily reference files stored in Amazon S3, enabling efficient processing of gigabyte-scale data without API limitations.3/Ease of use: Provides a fully managed default mode with pre-built execution runtimes that support popular programming languages like JavaScript, TypeScript, and Python with common libraries pre-installed. Browser Tool What is AgentCore Browser tool? AgentCore Browser tool provides a fast, secure, cloud-based browser runtime to enable agents to interact with websites at scale. It includes enterprise-grade security features including VM-level isolation and federated identity integration. The tool offers observability via live viewing, CloudTrail logging, session replay to easily troubleshoot, maintain quality, and support compliance, and CAPTCHA handling via Bot Control Vendors, to ensure smooth automated interactions without request blocking. With automatic scaling, AgentCore Browser tool eliminates infrastructure overhead while helping to maintain rigorous security and compliance standards. What are the key benefits of AgentCore Browser tool? 1/Serverless Browser Infrastructure:Provides agents with a fast, fully-managed browser that automatically scales without infrastructure overhead.2/Enterprise-grade security:Browser tool provides extensive security through VM-level isolated sandboxes with VPC support, robust audit capabilities, complemented by advanced features like session-level isolation and automated CAPTCHA handling.3/Enterprise observability: Easily troubleshoot issues, maintain quality control, and ensure compliance, with real-time visibility and complete recorded history of all browser interactions, whether performed by agents or humans. Identity What is AgentCore Identity? Amazon Bedrock AgentCore Identity is a scalable agent identity and access management capability that helps you securely build enterprise-ready agents at speed. It accelerates agent development by providing standards-based authentication, compatibility with existing identity providers, and native support for OAuth-enabled services. With just-enough access, secure permission delegation, and identity-aware authorization, your agents can securely access and perform actions on AWS resources or third-party services, on behalf of users or by themselves with pre-authorized user consent. AgentCore Identity features a secure token vault, enabling frictionless user experiences for all your agent-powered interactions while maintaining precise access controls. What are the key benefits of AgentCore Identity? 1/ Secure, delegated access for agentsAgentCore Identity allows your agents to securely access AWS resources and third-party tools and services with scoped access controls and secure permissions delegation. Identity-aware authorization ensures agents get only the right access through dynamic decisions based on the user’s identity context, delivering enhanced security controls.2/ Accelerate agent developmentAgentCore Identity accelerates secure enterprise-ready agent development by lowering the development efforts you typically need for building custom identity infrastructure. With standards-based authentication, you can use your existing identity providers such asAmazon Cognito, Microsoft Entra ID, or Okta without migrating users or rebuilding authentication flows. With support across popular Infrastructure-as-code tools such as AWS CloudFormation, Terraform, and AWS CDK, you can easily configure and manage the lifecycle of AgentCore Identity resources in an automated manner for accelerated productivity.3/ Build streamlined agent experiencesAgentCore Identity streamlines end-user and agent builder experiences while providing precise access controls. With a secure token vault that stores users’ tokens, AgentCore Identity simplifies authentication flows and minimizes consent fatigue to deliver frictionless user experiences for all your agent-powered interactions.4/ Simplify authentication across enterprise servicesAgentCore Identity offers native support for OAuth-enabled services such as Slack, Salesforce, and GitHub, simplifying implementation of custom OAuth flows or token handling. Your agents can seamlessly and securely operate across enterprise applications and AWS resources, on behalf of users or by themselves with pre-authorized user consent. How does AgentCore Identity provide robust and secure access controls while streamlining end-user and agent builder experiences? AgentCore Identity implements a secure token vault that stores users' tokens and allows agents to retrieve them securely. For OAuth 2.0 compatible tools and services, when a user first grants consent for an agent to act on their behalf, AgentCore Identity collects and stores the user's tokens issued by the tool in its vault, along with securely storing the agent's OAuth client credentials. Agents, operating with their own distinct identity and when invoked by the user, can then access these tokens as needed, reducing the need for frequent user consent. When the user token expires, AgentCore Identity triggers a new authorization prompt to the user for the agent to obtain updated user tokens. For tools that use API keys, AgentCore Identity also stores these keys securely and provides agents with controlled access to retrieve them when needed. This secure storage streamlines the user experience while maintaining robust access controls, enabling agents to operate effectively across various tools and services. Observability What is AgentCore Observability? Amazon Bedrock AgentCore Observability is a managed service that helps developers trace, debug, and monitor agent performance in production environments with any framework or model. Available across all AgentCore services, it offers detailed visualizations of each step in the agent workflow, enabling developers to inspect execution paths, audit intermediate outputs, and debug performance bottlenecks. Powered by Amazon CloudWatch, AgentCore Observability provides real-time visibility into operational performance through Amazon CloudWatch dashboards and telemetry for key metrics such as traces, session count, latency, duration, token usage, and error rates. Rich metadata tagging and filtering simplify issue investigation, while OpenTelemetry (OTEL)-compatible telemetry enables integration with existing monitoring tools including Dynatrace, Datadog, Arize Phoenix, LangSmith, and Langfuse. You can easily add custom attributes and business metadata to your agent traces, making observability directly relevant to business outcomes and decision-making. What are the benefits of AgentCore Observability? 1/Maintain quality and trust:Get a comprehensive, end to end view of agent behavior, seeing detailed reasoning, inputs, outputs, and tool usage. Accelerate debugging and quality audits with comprehensive visibility into agent workflows, applications, and infrastructure. Enable swift issue detection and root cause identification to aid in agentic application debugging, helping teams maintain the quality and trustworthiness of their AI-powered systems.2/Accelerate time to market:Real-time dashboards powered by Amazon CloudWatch save developers time with a single-pane-of-glass view into agents' operational health, without the need to manually stitch together data from multiple sources. This helps teams quickly detect issues, assess performance trends, and take timely corrective actions. Minimal observability infrastructure setup enables faster time to market into production environments. Easily add custom attributes and business metadata to your agent traces, making observability directly relevant to business outcomes and decision-making.3/Integrate with the observability tool of your choice:AgentCore emits telemetry data in standardized OpenTelemetry (OTEL)-compatible format, enabling developers to easily integrate logs, metrics, and traces with their existing monitoring and observability tools such as CloudWatch, Datadog, Arize Phoenix, LangSmith, and Langfuse. Service-vended spans help support enhanced scrutiny and deep dive analysis, providing comprehensive observability data. Developer Experience What is AgentCore SDK? How can I access it? The AgentCore SDK is a developer toolkit that allows you to build, configure, and deploy agents using Amazon Bedrock AgentCore services with your preferred agent framework (e.g. Strands, LangGraph, CrewAI, or custom). It supports defining agent behavior, memory (both short-term per session and long-term shared memory), built-in tools like code interpreter and browser, tool server connections via the AgentCore Gateway, observability, and full identity/authentication controls. You access the SDK via your AWS account (installing the Python SDK or using the AgentCore starter toolkit), and use AWS IAM / AgentCore Identity for inbound auth, standard protocols / API access for outbound auth and tool integrations. What is the open-source AgentCore MCP Server? What does it do? The open-source AgentCore MCP Server enables natural language development workflows between Agentic IDEs like Kiro and AI coding assistants (Claude Code, GitHub Copilot, and Q Developer CLI) and AgentCore services. It converts your natural language instructions and existing code to work seamlessly with AgentCore services, enabling you to get started faster with AgentCore. You caninstall the MCP Serverwith a single command and use it through your preferred AI coding assistant to perform tasks from code transformation to agent deployment, while the MCP Server automatically manages AgentCore configurations and dependencies in the background. Billing and Compliance How am I charged for using AgentCore? AgentCore offers flexible, consumption-based pricing with no upfront commitments or minimum fees. Each service—Runtime, Gateway, Identity, Memory, Observability, Browser Tool, and Code Interpreter—can be used independently or together, and you pay only for what you use. This modular approach allows you to start small and scale as your agent applications grow. For more information visit theAgentCore pricing page. What is the Service Level Agreement (SLA) for AgentCore? The SLA for Amazon Bedrock applies to AgentCore. For more information, visit theAmazon Bedrock Service Level Agreement. I want tolearn more about AgentCore Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [], "links": ["https://aws.amazon.com/bedrock/agentcore/faqs/#aws-page-content-main", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/agentcore/pricing/", "https://aws.amazon.com/bedrock/agentcore/resources/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/agentcore/faqs/#general", "https://aws.amazon.com/bedrock/agentcore/faqs/#runtime", "https://aws.amazon.com/bedrock/agentcore/faqs/#gateway", "https://aws.amazon.com/bedrock/agentcore/faqs/#memory", "https://aws.amazon.com/bedrock/agentcore/faqs/#code-interpreter", "https://aws.amazon.com/bedrock/agentcore/faqs/#browser-tool", "https://aws.amazon.com/bedrock/agentcore/faqs/#identity", "https://aws.amazon.com/bedrock/agentcore/faqs/#observability", "https://aws.amazon.com/bedrock/agentcore/faqs/#developer-experience", "https://aws.amazon.com/bedrock/agentcore/faqs/#billing-and-compliance", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway.html", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/agentcore/faqs/", "https://aws.amazon.com/bedrock/agentcore/pricing/", "https://aws.amazon.com/bedrock/sla/?did=sla_card&trk=sla_card", "https://aws.amazon.com/bedrock/agentcore/faqs/"]}
{"url": "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agentcore-get-started-toolkit.html", "text": "Get started with Amazon Bedrock AgentCore This quickstart gets you from zero to running an agent in under five minutes using Amazon Bedrock AgentCore. Step 0: Install the AgentCore CLI Before you start, make sure you have installed the Amazon Bedrock AgentCore Command Line Interface (CLI) as part of the starter toolkit. For installation instructions, see below. If you face issues with pip install , review the instructions here. Amazon Bedrock AgentCore requires requires Python version 3.10 or newer. You can check your version: If you need to update Python, visitpython.org/downloads. Step 1: Create your agent Now, letâs walk you through to create your agent as below. The above command: bootstraps a simple agent in Strands Agents, LangGraph, OpenAI Agents Software Development Kit or Google Agent Development Kit (you can pick which framework) uses a foundation model from model providers including Amazon Bedrock, OpenAI, Google's Gemini, Anthropic's Claude, Amazon Nova, Meta Llama, and Mistral (you can pick which model provider) produces either a project folder in Python with a simple agent, or Infrastructure as Code (IaC) ready code in Terraform or Cloud Development Kit (CDK) (you can pick) automatically creates Gateway, Memory, and enables Observability automatically configures role, entrypoint, requirements and auth model [Optional]Once you have created your agent, start a local dev server to test it manually. On a separate terminal, run the below command to test your agent response. Step 2: Deploy your agent Now, host your simple agent in Amazon Bedrock AgentCore Runtime using the below command. If you donât already have permissions, refer toIAM Permissions for AgentCore. The above command: Consolidates all your Python code into a zip file and deploys it Deploys your agent to AgentCore Runtime Configures CloudWatch logging Step 3: Invoke your agent with prompts Now, test your deployed agent with a simple prompt If you see a joke in the response, your agent is now running in an AgentCore Runtime and can be invoked. Next Steps After building your first agent with Amazon Bedrock AgentCore, we recommend that you explore the following sections: Add memory to your Amazon Bedrock AgentCore agent Enable your agent to interact with web pages using Browser Securely connect tools and other resources to your Gateway Additional code examples Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock-agentcore/latest/devguide/bedrock-agentcore-dg.pdf#agentcore-get-started-toolkit", "https://docs.aws.amazon.com/bedrock-agentcore/index.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agentcore-get-started-toolkit.html#install-cli", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agentcore-get-started-toolkit.html#agentcore-get-started-create-agent", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agentcore-get-started-toolkit.html#agentcore-get-started-deploy-agentcore", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agentcore-get-started-toolkit.html#agentcore-invoke-agent", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agentcore-get-started-toolkit.html#next-steps", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/python.org/downloads", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-permissions.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/browser-tool.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway.html"]}
{"url": "https://us-west-2.console.aws.amazon.com/bedrock-agentcore/home?region=us-west-2", "text": "", "images": [], "links": []}
{"url": "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agents-tools-runtime.html", "text": "Host agent or tools with Amazon Bedrock AgentCore Runtime Amazon Bedrock AgentCore Runtime provides a secure, serverless and purpose-built hosting environment\n        for deploying and running AI agents or tools. It offers the following benefits: AgentCore Runtime lets you transform any local agent code to cloud-native\n                        deployments with a few lines of code no matter the underlying framework.\n                        Works seamlessly with popular frameworks like LangGraph, Strands, and\n                        CrewAI. You can also leverage it with custom agents that don't use a\n                        specific framework. AgentCore Runtime works with any Large Language Model, such as models offered by\n                        Amazon Bedrock, Anthropic Claude, Google Gemini, and OpenAI. AgentCore Runtime lets agents communicate with other agents and tools via Model\n                        Context Protocol (MCP). AgentCore Runtime supports both real-time interactions and long-running workloads up\n                        to 8 hours, enabling complex agent reasoning and asynchronous workloads that\n                        may involve multi-agent collaboration or extended problem-solving\n                        sessions. AgentCore Runtime can process 100MB payloads enabling seamless processing of\n                        multiple modalities (text, images, audio, video), with rich media content or\n                        large datasets. In AgentCore Runtime, each user session runs in a dedicated microVM with isolated\n                        CPU, memory, and filesystem resources. This helps create complete separation\n                        between user sessions, safeguarding stateful agent reasoning processes and\n                        helps prevent cross-session data contamination. After session completion,\n                        the entire microVM is terminated and memory is sanitized, delivering\n                        deterministic security even when working with non-deterministic AI\n                        processes. Runtime implements consumption-based pricing that charges only for\n                        resources actually consumed. Unlike allocation-based models that require\n                        pre-selecting resources, Runtime dynamically provisions what's needed\n                        without requiring right-sizing. The service aligns CPU billing with actual\n                        active processing - typically eliminating charges during I/O wait periods\n                        when agents are primarily waiting for LLM responses - while continuously\n                        maintaining your session state. AgentCore Runtime, powered by AgentCore Identity, assigns distinct\n                        identities to AI agents and seamlessly integrates with your corporate\n                        identity provider such as Okta, Microsoft Entra ID, or Amazon Cognito,\n                        enabling your end users to authenticate into only the agents they have\n                        access to. In addition, Runtime lets outbound authentication flows to\n                        securely access third-party services like Slack, Zoom, and GitHub - whether\n                        operating on behalf of users or autonomously (using either OAuth or API\n                        keys). AgentCore Runtime provides specialized built-in tracing that captures agent\n                        reasoning steps, tool invocations, and model interactions, providing clear\n                        visibility into agent decision-making processes, a critical capability for\n                        debugging and auditing AI agent behaviors. AgentCore Runtime is delivered through a single, comprehensive SDK that provides\n                        streamlined access to the complete AgentCore capabilities including\n                        Memory, Tools, and Gateway. This integrated approach eliminates the\n                        integration work typically required when building equivalent agent\n                        infrastructure from disparate components. How it works Understand the AgentCore Runtime service\n            contract IAM Permissions for AgentCore Runtime Get started with AgentCore Runtime Use any agent framework Use any foundation model Deploy MCP servers in AgentCore Runtime Deploy A2A servers in AgentCore Runtime Use isolated sessions for agents Handle asynchronous and long running agents with\n            Amazon Bedrock AgentCore Runtime Stream agent responses Pass custom headers to Amazon Bedrock AgentCore Runtime Authenticate and authorize with Inbound Auth and Outbound Auth AgentCore Runtime versioning and endpoints Invoke an AgentCore Runtime agent Observe agents in  Amazon Bedrock AgentCore Runtime Troubleshoot AgentCore Runtime Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock-agentcore/latest/devguide/bedrock-agentcore-dg.pdf#agents-tools-runtime", "https://docs.aws.amazon.com/bedrock-agentcore/index.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-how-it-works.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-service-contract.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-permissions.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-getting-started.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/using-any-agent-framework.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/using-any-model.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-mcp.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-a2a.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-sessions.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-long-run.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/response-streaming.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-header-allowlist.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-oauth.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agent-runtime-versioning.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-invoke-agent.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-observability.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-troubleshooting.html"]}
{"url": "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway.html", "text": "Amazon Bedrock AgentCore Gateway: Securely connect tools and other resources to your\n      Gateway Amazon Bedrock AgentCore Gateway provides an easy and secure way for developers to build, deploy, discover, and connect to tools at scale. AI agents need tools to perform real-world tasksâfrom querying databases to sending messages to analyzing documents. With Gateway, developers can convert APIs, Lambda functions, and existing services into Model Context Protocol (MCP)-compatible tools and make them available to agents through Gateway endpoints with just a few lines of code. Gateway supports OpenAPI, Smithy, and Lambda as input types, and is the only solution that provides both comprehensive ingress authentication and egress authentication in a fully-managed service. Gateway also provides 1-click integration with several popular tools such as Salesforce, Slack, Jira, Asana, and Zendesk. Gateway eliminates weeks of custom code development, infrastructure provisioning, and security implementation so developers can focus on building innovative agent applications. Key benefits Transform existing enterprise resources into agent-ready tools in just a few lines of code. Instead of spending months writing custom integration code and managing infrastructure, developers can focus on building differentiated agent capabilities while Gateway handles the undifferentiated heavy lifting of tool management and security at enterprise scale. Gateway also provides 1-click integration with several popular tools such as Salesforce, Slack, Jira, Asana, and Zendesk. Enable your agents to discover and use tools through a single, secure endpoint. By combining multiple tool sourcesâfrom APIs to Lambda functionsâinto one unified interface, developers can build and scale agent workflows faster without managing multiple tool connections or reimplementing integrations. As your tool collection grows, help your agents find and use the right tools through contextual search. Built-in semantic search capabilities help agents effectively utilize available tools based on their task context, improving agent performance and reducing development complexity at scale. Manage both inbound authentication (verifying agent identity) and outbound authentication (connecting to tools) in a single service. Handle OAuth flows, token refresh, and secure credential storage for third-party services. Work with popular open-source frameworks including CrewAI, LangGraph, LlamaIndex, and Strands Agents. Integrate with any model while maintaining enterprise-grade security and reliability. Eliminate infrastructure management with a fully managed service that automatically scales based on demand. Built-in observability and auditing capabilities simplify monitoring and troubleshooting. Key capabilities Gateway provides the following key capabilities: Security Guard- Manages OAuth authorization to ensure only valid users and agents can access tools and resources. Translation- Converts agent requests using protocols like Model Context Protocol (MCP) into API requests and Lambda invocations, eliminating the need to manage protocol integration or version support. Composition- Combines multiple APIs, functions, and tools into a single MCP endpoint for streamlined agent access. Secure Credential Exchange- Handles credential injection for each tool, enabling agents to use tools with different authentication requirements seamlessly. Semantic Tool Selection- Enables agents to search across available tools to find the most appropriate ones for specific contexts, allowing agents to leverage thousands of tools while minimizing prompt size and reducing latency. Infrastructure Manager- Provides a serverless solution with built-in observability and auditing, eliminating infrastructure management overhead. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock-agentcore/latest/devguide/bedrock-agentcore-dg.pdf#gateway", "https://docs.aws.amazon.com/bedrock-agentcore/index.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway.html#gateway-benefits-using", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway.html#gateway-core-concepts-connectivity-layer"]}
{"url": "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory.html", "text": "Add memory to your Amazon Bedrock AgentCore agent AgentCore Memory is a fully managed service that gives your AI agents the ability to\n            remember past interactions, enabling them to provide more intelligent, context-aware,\n            and personalized conversations. It provides a simple and powerful way to handle both\n            short-term context and long-term knowledge retention without the need to build or manage\n            complex infrastructure. AgentCore Memory addresses a fundamental challenge in agentic AI: statelessness. Without\n            memory capabilities, AI agents treat each interaction as a new instance with no\n            knowledge of previous conversations. AgentCore Memory provides this critical capability,\n            allowing your agent to build a coherent understanding of users over time. AgentCore Memory supports a variety of SDKs and agent frameworks. For examples,\n            seeAmazon Bedrock AgentCore Memory examples. Memory types AgentCore Memory offerstwo typesof memory that\n            work together to create intelligent, context-aware AI agents: Short-term memory captures turn-by-turn interactions within a single\n                        session. This lets agents maintain immediate context without requiring users\n                        to repeat information. Example:When a user asks, \"What's the\n                        weather like in Seattle?\" and follows up with \"What about tomorrow?\", the\n                        agent relies on recent conversation history to understand that \"tomorrow\"\n                        refers to the weather in Seattle. Long-term memory automatically extracts and stores key insights from\n                        conversations across multiple sessions, including user preferences,\n                        important facts, and session summaries â for persistent knowledge retention\n                        across multiple sessions. Example:If a customer mentions they\n                        prefer window seats during flight booking, the agent stores this preference\n                        in long-term memory. In future interactions, the agent can proactively offer\n                        window seats, creating a personalized experience. Memory key benefits Create more natural conversations:By\n                    remembering previous turns in a conversation, agents can understand context,\n                    resolve ambiguous statements, and interact in a way that feels more\n                    human. Deliver personalized experiences:Retain user\n                    preferences, historical data, and key facts across sessions to tailor responses\n                    and actions to individual users. Reduce development complexity:Offload the\n                    undifferentiated heavy lifting of managing conversational state and memory,\n                    allowing you to focus on building your agent's core business logic. Common use cases of memory Conversational agents:A customer support\n                    chatbot remembers a user's previous issues and preferences, enabling it to\n                    provide more relevant assistance in future interactions. Task-oriented / workflow agents:An AI agent\n                    orchestrating a multi-step business process, such as invoice approval, uses\n                    memory to track the status of each step and maintain workflow progress. Multi-agent systems:A team of AI agents\n                    managing a supply chain shares memory to synchronize inventory levels,\n                    anticipate demand, and optimize logistics. Autonomous or planning agents:An autonomous\n                    vehicle uses memory to plan routes, adjust to traffic conditions, and learn from\n                    past experiences to improve future driving decisions. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://docs.aws.amazon.com/images/bedrock-agentcore/latest/devguide/images/memory-overview.png", "local": "images\\29298f7b8c3a1b18b33f6a09639d02521d7d7ec1.png", "alt": "Memory AgentCore Memory"}, {"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock-agentcore/latest/devguide/bedrock-agentcore-dg.pdf#memory", "https://docs.aws.amazon.com/bedrock-agentcore/index.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory.html#memory-types-overview", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory.html#memory-key-benefits", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory.html#common-use-cases", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory-examples.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory-types.html"]}
{"url": "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity.html", "text": "Provide identity and credential management for agent applications with Amazon Bedrock AgentCore Identity Amazon Bedrock AgentCore Identity is an identity and credential management service designed specifically for\n        AI agents and automated workloads. It provides secure authentication, authorization, and\n        credential management capabilities that enable agents and tools to access AWS resources\n        and third-party services on behalf of users while helping to maintain strict security\n        controls and audit trails. Agent identities are implemented as workload identities with\n        specialized attributes that enable agent-specific capabilities while helping to maintain\n        compatibility with industry-standard workload identity patterns. The service integrates\n        natively with Amazon Bedrock AgentCore to provide identity and credential management for agent\n        applications, includingHost agent or tools with Amazon Bedrock AgentCore RuntimeandAmazon Bedrock AgentCore Gateway: Securely connect tools and other resources to your\n      Gateway. Overview of Amazon Bedrock AgentCore Identity Get started with AgentCore Identity Using the AgentCore Identity console Manage workload identities with\n            AgentCore Identity Manage credential providers with\n            AgentCore Identity Provider setup and configuration Data protection in Amazon Bedrock AgentCore Identity Tagging AgentCore Identity resources Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock-agentcore/latest/devguide/bedrock-agentcore-dg.pdf#identity", "https://docs.aws.amazon.com/bedrock-agentcore/index.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agents-tools-runtime.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity-overview.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity-getting-started.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity-how-to.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity-manage-agent-ids.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity-outbound-credential-provider.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity-idps.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity-data-protection.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity-tagging.html"]}
{"url": "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability.html", "text": "Observe your agent applications on Amazon Bedrock AgentCore Observability With AgentCore, you can trace, debug, and monitor AI agents' performance in production environments. AgentCore Observability helps you trace, debug, and monitor agent performance in production environments. It offers detailed visualizations of each step \n        in the agent workflow, enabling you to inspect an agent's execution path, audit intermediate outputs, and debug performance bottlenecks and failures. AgentCore Observability gives you real-time visibility into agent operational performance through access to dashboards powered by Amazon CloudWatch and telemetry \n            for key metrics such as session count, latency, duration, token usage, and error rates. Rich metadata tagging and filtering simplify issue investigation \n            and quality maintenance at scale. AgentCore emits telemetry data in standardized OpenTelemetry (OTEL)-compatible format, enabling you to easily integrate \n            it with your existing monitoring and observability stack. By default, AgentCore outputs a set of key built-in metrics for agents, gateway resources, and memory resources. For memory resources, AgentCore \n          also outputs spans and log data if you enable it. You can also instrument your agent \n          code to provide additional span and trace data and custom  metrics and logs. SeeAdd observability to your Amazon Bedrock AgentCore\n            resourcesto learn more. All of the metrics, spans, and logs output by AgentCore are stored in Amazon CloudWatch, and can be viewed in the CloudWatch console \n          or downloaded from CloudWatch using the AWS CLI or one of the AWS SDKs. In addition to the raw data stored in CloudWatch Logs, for agent runtime data only, the CloudWatch console provides an observability dashboard containing trace visualizations, graphs for \n        custom span metrics, error breakdowns, and more. To learn more about viewing your agents' observability data, seeView observability data for your Amazon Bedrock AgentCore agents Get started with AgentCore Observability Add observability to your Amazon Bedrock AgentCore\n            resources Understand observability for agentic resources in AgentCore Amazon Bedrock AgentCore generated observability data View observability data for your Amazon Bedrock AgentCore agents Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock-agentcore/latest/devguide/bedrock-agentcore-dg.pdf#observability", "https://docs.aws.amazon.com/bedrock-agentcore/index.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability-configure.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability-view.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability-get-started.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability-configure.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability-telemetry.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability-service-provided.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability-view.html"]}
{"url": "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/code-interpreter-tool.html", "text": "Execute code and analyze data using\n      Amazon Bedrock AgentCore Code Interpreter The Amazon Bedrock AgentCore Code Interpreter enables AI agents to write and execute code securely in sandbox\n    environments, enhancing their accuracy and expanding their ability to solve complex end-to-end\n    tasks. This is critical in Agentic AI applications where the agents may execute arbitrary code\n    that can lead to data compromise or security risks. The AgentCore Code Interpreter tool provides secure\n    code execution, which helps you avoid running into these issues. The Code Interpreter comes with pre-built runtimes for multiple languages and advanced\n    features, including large file support and internet access, and CloudTrail logging capabilities. For\n    inline upload, the file size can be up to 100 MB. And for uploading to Amazon S3 through terminal\n    commands, the file size can be as large as 5 GB. Developers can customize environments with session properties and network modes to meet\n    their enterprise and security requirements. The AgentCore Code Interpreter reduces manual intervention\n    while enabling sophisticated AI development without compromising security or performance. Overview The AgentCore Code Interpreter is a capability that allows AI agents to write, execute, and debug\n      code securely in sandbox environments. It provides a bridge between natural language\n      understanding and computational execution, enabling agents to manipulate data and perform\n      calculations programmatically. The AgentCore Code Interpreter runs in a containerized environment within Amazon Bedrock AgentCore,\n      ensuring that code execution remains isolated and secure. Why use Code Interpreter in agent\n        development The AgentCore Code Interpreter enhances agent development in the following ways: Execute code securely: Develop agents that can perform complex workflows and data\n          analysis in isolated sandbox environments, while accessing internal data sources without\n          exposing sensitive data or compromising security. Multiple programming languages: The Code Interpreter supports various programming\n          languages including Python, JavaScript, and TypeScript, making it versatile for different\n          use cases. Monitoring and large-scale data processing: Track and troubleshoot code execution.\n          When working with large datasets, you can easily reference files stored in Amazon S3, enabling\n          efficient processing of gigabyte-scale data without API limitations. Ease of use: Use a fully managed default mode with pre-built execution runtimes that\n          support popular programming languages with common libraries pre-installed. Extends problem-solving capabilities: Allows agents to solve computational problems\n          that are difficult to address through reasoning alone and enables precise mathematical\n          calculations and data processing at scale. Long execution duration support: The Code Interpreter tool provides support for a\n          default execution time of 15 minutes, which can be extended for up to eight hours. Handles structured data: Processes CSV, Excel, JSON, and other data formats, and\n          performs data cleaning, and analysis. Enables complex workflows: Allows multi-step problem solving that combines reasoning\n          with computation and facilitates iterative development and debugging. The AgentCore Code Interpreter makes agents more powerful by complementing their reasoning\n      abilities with computational execution, allowing them to tackle a much wider range of tasks\n      effectively. Best practices To get the most out of the AgentCore Code Interpreter: Keep code snippets concise and focused on specific tasks Use comments to document your code Optimize code for performance when working with large datasets Save intermediate results when performing complex operations Use thecode_sessioncontext manager to ensure proper cleanup Include try/except blocks in code to handle errors gracefully Code execution results are returned and processed as streams Clean up temporary files when no longer needed Close sessions when you're done to release resources Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://docs.aws.amazon.com/images/bedrock-agentcore/latest/devguide/images/code-interpreter.png", "local": "images\\939b8cbdb8829592fda019c15775b91653474cd6.png", "alt": "Architecture showing the built-in tools offering and Browser and CI tools."}, {"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock-agentcore/latest/devguide/bedrock-agentcore-dg.pdf#code-interpreter-tool", "https://docs.aws.amazon.com/bedrock-agentcore/index.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/code-interpreter-tool.html#code-interpreter-overview", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/code-interpreter-tool.html#code-interpreter-why-important", "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/code-interpreter-tool.html#code-interpreter-best-practices"]}
{"url": "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/browser-sandbox-tool.html", "text": "", "images": [], "links": []}
{"url": "https://aws.amazon.com/bedrock/agents/", "text": " Search Amazon Bedrock Amazon Bedrock Agents Enable generative AI applications to automate multistep tasks by seamlessly connecting with company systems, APIs, and data sources What is Amazon Bedrock Agents? Amazon Bedrock Agents uses the reasoning of foundation models (FMs), APIs, and data to break down user requests, gathers relevant information, and efficiently completes tasks—freeing teams to focus on high-value work. Building an agent is straightforward and fast, with setup in just a few steps. Agents now includes memory retention for seamless task continuity and Amazon Bedrock Guardrails for built-in security and reliability. For more advanced needs, Amazon Bedrock supports multi-agent collaboration, allowing multiple specialized agents to work together on complex business challenges. Features Multi-agent collaboration Amazon Bedrock multi-agent collaboration allows developers to build, deploy, and manage multiple specialized agents seamlessly working together to address increasingly complex business workflows. Each agent focuses on specific tasks under the coordination of a supervisor agent, which breaks down intricate processes into manageable steps to ensure precision and reliability. By automating these complex operational processes, businesses can free their teams from operational burdens, allowing them to focus on innovation and deliver real business value. Retrieval augmented generation Agents securely connects to your company’s data sources and augments the user requests with the right information to generate an accurate response. For example, if the user asks about claims eligibility, the RAG agent will look up information from the knowledge base and reconcile between the submitted claims and the eligibility policy response: “You need to turn in your driver’s license, pictures of the damaged car, and an accident report.” Orchestrate and execute Customers can create an agent in Amazon Bedrock in just a few quick steps, accelerating the time it takes to build generative AI applications. Customers first select a model and write a few instructions in natural language. For example, “you are an inventory management agent that determines product availability in the inventory system.” Agents orchestrates and analyzes the task and breaks it down into the correct logical sequence using the FM’s reasoning abilities. Agents automatically calls the necessary APIs to transact with the company systems and processes to fulfill the request, determining along the way if they can proceed or if they need to gather more information. Memory retention Agents has the ability to retain memory across interactions, offering more personalized and seamless user experiences. This feature allows an agent to remember historical interactions and improves the accuracy of multistep tasks. Users benefit from improved recommendations and recall of prior context when required, ensuring a more cohesive and efficient interaction with the agent. Code interpretation Agents supports the ability to dynamically generate and execute code in a secure environment. This feature automates complex analytical queries that were previously hard to answer solely through model reasoning. Use this capability to address a wide range of sophisticated use cases, such as data analysis, data visualization, and mathematical problem solving. Introducing Amazon Bedrock AgentCore Using any open-source framework and model, Amazon Bedrock AgentCore helps you deploy and operate AI agents securely and at scale. How to get started Best practices for building robust generative AI applications with Amazon Bedrock Agents—Part 1 Amazon Bedrock Agents user guide Best practices for building robust generative AI applications with Amazon Bedrock Agents—Part 2 Learn more about Amazon Bedrock AgentCore Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/537038fd-646f-4640-830d-167bad8442d8.2fe57df29c9a0e53bf293221c0bc37e4f2c3779c.png", "local": "images\\3bc49bec02778b899c13889c44ea4f427e016841.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/retrieval-augmented-1.d08f939c911c4adb59ce7475d1b5815aff5b0b3d.jpg", "local": "images\\12063ccb69266056b5c6446f9fcc773728d239c2.jpg", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/orchestrate-and-execute-1.a568120669c878e02f721ce3fe9b4e9ded2ad005.png", "local": "images\\464870d3cf77554c36fbc00a4edb45278be7c409.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/memory-retention-1.014526aac78aa510a76a2ad3ac4e5ca9308a7717.png", "local": "images\\e17a95396cbed093ccfb68817aa197b95d451089.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/code-interpretation-1.1112d23789ae086c6881dd65c81c5dfd8e62af0f.png", "local": "images\\6a70539cc07aa2487e12df9f7d3906bdea8fcd25.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/agents/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/agentcore/", "https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/agents/"]}
{"url": "https://aws.amazon.com/bedrock/pricing/#aws-page-content-main", "text": " Search Amazon Bedrock Amazon Bedrock pricing Pricing overview   Amazon Bedrock is a comprehensive platform for building generative AI applications and agents with access to leading foundation models, services to deploy and operate agents, and tools for customizing, safeguarding, and optimizing applications. Amazon Bedrock offers flexible pricing options for model inference to optimize cost while balancing speed, scale, and model access needs. This includes on-demand pricing for pay-as-you-go usage with no upfront commitments, and batch mode for cost-efficient processing of large volumes of input. AgentCore offers flexible, consumption-based pricing with no upfront commitments or minimum fee, which you can learn about by accessing theAgentCore pricing page.   Service tiers and model import Standard Tier The Standard tier provides consistent performance at regular rates for everyday AI tasks. With the Standard tier you only pay for what you use, with no time-based term commitments. For text-generation models, you are charged for every input token processed and every output token generated. For embeddings models, you are charged for every input token processed. A token comprises a few characters and refers to the basic unit of text that a model learns to understand the user input and prompt. For image-generation models, you are charged for every image generated. Learn more Priority Tier The Priority tier is a premium service tier that provides preferential compute allocation for mission-critical applications. This service tier is priced at a premium over the Standard tier.  For most models that support Priority Tier, customers can realize up to 25% better output tokens per second (OTPS) latency compared to standard tier Learn more Flex Tier Flex tier offers discounted standard pricing for workloads that can trade immediate processing for cost efficiency.Perfect for non-urgent AI workloads. Batch With Batch mode, you can provide a set of prompts as a single input file and receive responses as a single output file, allowing you to get simultaneous large-scale predictions. The responses are processed and stored in your Amazon S3 bucket so you can access them at a later time. Amazon Bedrock offers select foundation models (FMs) from leading AI providers like Anthropic, Meta, Mistral AI, and Amazon for batch inference at a 50% lower price compared to on-demand inference pricing. Please refer to model listhere. Custom Model Import Custom Model Import allows you to leverage your prior model customization investments within Amazon Bedrock and consume them in the same fully-managed manner as Bedrock’s existing hosted foundation models. You can import custom weights for supported model architectures and serve the custom model using On-Demand mode. There is no charge to import a custom model to Bedrock. Once you import a model, you will be able to access it on-demand without requiring to perform any control plane action. You are only charged for model inference, based on the number of copies of your custom model required to service your inference volume and the duration each model copy is active, billed in 5-minute windows. A model copy is a single instance of an imported model ready to serve inference requests. The price per model copy per minute depends on factors such as architecture, context length, AWS Region, compute unit version (hardware generation), and is tiered by model copy size. Customization and optimization Model Customization Model Distillation With Amazon Bedrock Model Distillation, customers can transfer knowledge from a larger capable model (known as “the teacher model”) to a smaller, faster, and cost-efficient model (known as “the student model”), such that the student model can become as performant as the teacher for specific use-cases. With Amazon Bedrock Model Distillation you pay for what you use. Distillation is a two-step process where first synthetic data is generated from the teacher model and then the student model is trained. Synthetic data generation is charged at on-demand pricing of the selected teacher model. Fine-tuning of the student model is charged at model customization rates. You can set up inference on a custom model by either creating a custom model on-demand deployment or purchasing Provisioned Throughput depending on your specific workload requirements and cost objectives. The on-demand inference option includes a token-based pricing model that charges based on the number of tokens processed during inference. Fine tuning / Continued pre-trainingWith Amazon Bedrock, you can customize FMs with your data to deliver tailored responses for specific tasks and your business context. You can fine-tune models with labeled data or using continued pretraining with unlabeled data. For customization of a text-generation model, you are charged for the model training based on the total number of tokens processed by the model (number of tokens in the training data corpus x the number of epochs) and for model storage charged per month per model. An epoch refers to one full pass through your training dataset during fine-tuning or continued pretraining. You can set up inference on a custom model by either creating a custom model on-demand deployment or purchasing Provisioned Throughput depending on your specific workload requirements and cost objectives. The on-demand inference option includes a token-based pricing model that charges based on the number of tokens processed during inference. If you purchase Provisioned Throughput, one model unit is made available with no commitment term for inference on a customized model. You will be charged for the number of hours you use in the first model unit for custom model inference. If you want to increase your throughput beyond one model unit, then you must purchase a 1-month or 6-month commitment term. Prompt Caching With prompt caching on Amazon Bedrock, you can cache repeated context across API calls to reduce your costs and response latencies. Prompts often contain common context or prefixes such as long, multi-turn conversations, many-shot examples and detailed instructions that refine model behavior. Using existing Amazon Bedrock APIs, you can specify the prompt prefixes that you want to cache for five minutes in an AWS account-specific cache. During that time, any requests with matching prefixes receive a discount of up to 90% on cached tokens and a latency improvement of up to 85%. Prices and performance improvements vary by model and prompt length, but your caches are always isolated to your AWS account. Prompt Management and Optimization Amazon Bedrock Prompt Managementaccelerates the creation, testing, and running of prompt through an intuitive UI and a set of APIs. You can easily test and version your prompts, compare different variants, and run them in a secure serverless infrastructure. Prompt Optimization in Amazon Bedrock automatically rewrites prompts for better performance and more concise responses for foundation models. It integrates with Prompt Management for side-by-side comparison of original and optimized versions, and prompt life cycle management. You can also use Prompt Optimization in Bedrock Playground, or directly via API. Tools Guardrails Amazon Bedrock Guardrailsis the only responsible AI capability from a major cloud provider that helps you build and customize safety, privacy, and truthfulness safeguards for your generative AI applications. It evaluates user inputs and model responses based on use case-specific policies, providing an additional layer of safeguards beyond what's natively available. Safeguards from Guardrails can be applied to models hosted on Amazon Bedrock or with any third-party models (such as OpenAI and Google Gemini) via the ApplyGuardrail API. You can also use Guardrails with an agent framework such as Strands Agents, including agents deployed using Amazon Bedrock AgentCore. Guardrails helps filter hallucinations and improve factual accuracy through contextual grounding checks against RAG content and Automated Reasoning checks to deliver provably truthful responses. Knowledge Bases Amazon Bedrock Knowledge Bases is a fully managed Retrieval-Augmented Generation (RAG) workflow that enables customers to create highly accurate, low-latency, secure, and custom generative AI applications by incorporating contextual information from their own data sources. It supports various data sources, including S3, and Confluence, Salesforce, and SharePoint, in preview. It also offers document ingestion for streaming data. Bedrock Knowledge Bases converts unstructured data into embeddings, stores them in vector databases, and enables retrieval from diverse data stores. It also integrates with Kendra for managed retrieval and supports structured data retrieval using natural language to SQL. Amazon Bedrock Data Automation Amazon Bedrock Data Automation transforms unstructured, multimodal content into structured data formats for use cases like intelligent document processing, video analysis, and RAG. Bedrock Data Automation can generate Standard Output content using predefined defaults which are modality specific, like scene-by-scene descriptions of videos, audio transcripts or automated document analysis. Customers can additionally create Custom Outputs by specifying their output requirements in Blueprints based on their own data schema that they can then easily load into an existing database or data warehouse. Through an integration with Knowledge Bases, Bedrock Data Automation can also be used to parse content for RAG applications, improving the accuracy and relevancy of results by including information embedded in both images and text. Flows Amazon Bedrock Flowsis a workflow authoring and execution feature of Bedrock for generative AI applications. It accelerates the creation, testing, and deployment of user-defined generative AI workflows through an intuitive visual builder and a set of APIs. It allows you to seamlessly link the latest foundation models, Prompts, Agents, Knowledge Base, Guardrails, and AWS services (such as Amazon Lex, AWS Lambda, Amazon S3) along with business logic to build generative AI workflows. You can easily test and version your workflows, and run it in a secure serverless environment through a visual interface or API without having to stand up your own infrastructure. Evaluations Model Evaluation:With Amazon Bedrock model evaluation, you pay for what you use, with no minimum volume commitments on the number of prompts or responses. For automatic (programmatic) evaluation, you only pay for the inference from your choice of model in the evaluation. The automatically-generated algorithmic scores are provided at no extra charge. For automatic (Model/LLM-as-a-judge) evaluation, you only pay for the inference from your choice of generator model and evaluator model. In an LLM-as-a-judge model evaluation job, the built-in metrics use system judge prompt templates unique to each metric and available judge model that will be charged as part of your token usage, and the judge prompts are available in the publicAWS documentationfor transparency. For human-based evaluation where you bring your own workteam, you are charged for the model inference in the evaluation, and a charge of $0.21 per completed human task. A human task is defined as an occurrence of a human worker submitting an evaluation of a single prompt and its associated inference responses in the human evaluation user interface. The price per task is the same whether you have one or two models in your evaluation job and also the same regardless of how many evaluation metrics and rating methods you include. The charges for the human tasks will appear under the Amazon SageMaker section in your AWS bill and are the same for all AWS Regions. There is no separate charge for the workforce, as the workforce is supplied by you. If you are using the “bring your own inference responses” feature instead of calling a Bedrock model during the evaluation job, you are only charged for the evaluator model inference (LLM-as-a-judge jobs) or the $0.21 per completed human task (human-based evaluation jobs). For an evaluation managed by AWS, pricing is customized for your evaluation needs in a private engagement while working with the AWS expert evaluations team. RAG Evaluation:With Amazon Bedrock RAG evaluation, you pay for what you use, with no minimum volume commitments on the number of prompts or responses. If you are evaluating an Amazon Bedrock Knowledge Base, you only pay for the inference from your choice of generator model and evaluator model (the evaluation job uses an LLM-as-a-judge), as well as any charges incurred from using the Knowledge Base in the evaluation job according to Amazon Bedrock Knowledge Bases pricing. If you are using the “bring your own inference responses” feature, you are only charged for the evaluator model inference. In a RAG evaluation job, the built-in metrics use system judge prompt templates unique to each metric and available judge model that will be charged as part of your token usage, and the judge prompts are available in the publicAWS documentationfor transparency. Some metrics involve doing judge model inference on retrieved context from your Knowledge Base/RAG system or your ground truth answers in addition to the input prompt, which affects the costs associated with each metric - more information on each metric can be found in the public AWS documentation for evaluations. Model pricing details Pricing is dependent on the modality, provider, and model. Please select the model provider to see detailed pricing. Amazon Bedrock offers select foundation models (FMs) from leading AI providers like Anthropic, Meta, Mistral AI, and Amazon for batch inference at a 50% lower price compared to on-demand inference pricing. Please refer to model listhere. AI21 Labs On-Demand pricing Amazon Nova Pricing for Understanding Models Built-In-Tools Pricing for Creative Content Generation models Pricing for Speech Understanding and Generation Models On-Demand pricing for speech to speech foundation models Note:*The text tokens input and output pricing applies to specific use cases such as speech-to-text transcription, tool calls for task completion or knowledge grounding, adding conversation history to the session etc. On-demand inference for custom Nova models is priced the same as base Nova inference. Pricing for Embedding models Amazon Titan Anthropic On-Demand and Batch pricing Models with extended access Latency Optimized Inference Provisioned Throughput Pricing Region: US East (N. Virginia) and US West (Oregon) Claude Instant $39.60 $22.00 Claude 2.0/2.1 $63.00 $35.00 Region: Asia Pacific (Tokyo) Claude Instant $53.00 $29.00 Claude 2.0/2.1 $86.00 $48.00 Region: Europe (Frankfurt) Claude Instant $49.00 $27.00 Claude 2.0/2.1 $79.00 $44.00 Please reach out to your AWS account team for more details on model units.  Cohere On-Demand pricing Pricing for customization (fine-tuning) *Total tokens trained = number of tokens in training data corpus x number of epochs Provisioned Throughput pricing Price per hour per model unit for 6-month commitment Cohere Command $39.60 $23.77 $6.85 $6.76 $6.76 Please reach out to your AWS account or sales team for more details on model units. DeepSeek On-Demand pricing On-Demand pricing Meta Llama 4 On-Demand and Batch pricing Llama 3.3 On-Demand and Batch pricing Llama 3.2 On-Demand and Batch pricing Pricing for model customization (fine-tuning) Provisioned Throughput pricing Llama 3.1 On-Demand and Batch pricing Pricing for model customization (fine-tuning) Provisioned Throughput pricing Llama 3 On-Demand pricing Llama 2 On-Demand pricing Region: US East (N. Virginia) and US West (Oregon) Llama 2 Chat (13B) $0.00075 $0.001 $0.00195 Pricing for model customization (fine-tuning) Llama 2 Pretrained (13B) $0.00149 $1.95 $23.50 $0.00799 *Custom model storage = $1.95 Provisioned Throughput pricing Llama 2 Pretrained and Chat (13B) $21.18 $13.08 $21.18 *Llama 2 Pre-trained models are available only in provisioned throughput after customization. Please reach out to your AWS account or sales team for more details on model units. Mistral AI OpenAI TwelveLabs On-Demand pricing Stability AI On-Demand pricing Previous generation of image models offered by Stability AI are priced per image, depending on step count and image resolution.  Region: Oregon, N. Virginia, Ohio Writer On-demand pricing Amazon Bedrock Qwen Custom Model Import Regions: US East (N. Virginia) and US West (Oregon) Region: Europe (Frankfurt) Note:The Custom Model Units needed to host a model depend on a variety of factors - notably the model architecture, model parameter count, and context length. The exact number of Custom Model Units needed will be determined at the time of import. For reference, Llama 3.1 8B 128K model requires 2 Custom Model Units, a Llama 3.1 70B 128k model requires 8 Custom Model Units. *Billed in 5 minute windows Regions: US East (N. Virginia) and US West (Oregon) Region: Europe (Frankfurt) Note:The Custom Model Units needed to host a model depend on a variety of factors - notably the model architecture, model parameter count, and context length. The exact number of Custom Model Units needed will be determined at the time of import. For reference, Llama 3.1 8B 128K model requires 2 Custom Model Units, a Llama 3.1 70B 128k model requires 8 Custom Model Units. *Billed in 5 minute windows Regions: US East (N. Virginia) and US West (Oregon) Region: Europe (Frankfurt) Note:The Custom Model Units needed to host a model depend on a variety of factors - notably the model architecture, model parameter count, and context length. The exact number of Custom Model Units needed will be determined at the time of import. For reference, Llama 3.1 8B 128K model requires 2 Custom Model Units, a Llama 3.1 70B 128k model requires 8 Custom Model Units. *Billed in 5 minute windows Regions: US East (N. Virginia) and US West (Oregon) Region: Europe (Frankfurt) Note:The Custom Model Units needed to host a model depend on a variety of factors - notably the model architecture, model parameter count, and context length. The exact number of Custom Model Units needed will be determined at the time of import. For reference, Llama 3.1 8B 128K model requires 2 Custom Model Units, a Llama 3.1 70B 128k model requires 8 Custom Model Units. *Billed in 5 minute windows Regions: US East (N. Virginia) and US West (Oregon) Region: Europe (Frankfurt) Note:The Custom Model Units needed to host a model depend on a variety of factors - notably the model architecture, model parameter count, and context length. The exact number of Custom Model Units needed will be determined at the time of import. For reference, Llama 3.1 8B 128K model requires 2 Custom Model Units, a Llama 3.1 70B 128k model requires 8 Custom Model Units. *Billed in 5 minute windows On-Demand Inference Pricing:You are billed in 5-minute windows for the duration your model copy is active starting from the first successful invocation. The maximum throughput and concurrency limit per model copy depends on factors such as input/output token mix, hardware type, model size, architecture, inference optimizations, and is determined during the model import workflow. Bedrock automatically scales the number of model copies depending on your usage patterns. If there are no invocations for a 5-minute period, Bedrock will scale down to zero and scale back up when you invoke your model. While scaling back up, you may experience a cold-start duration (in tens of seconds) depending on model size. Bedrock also scales up the number of model copies if your inference volume consistently exceeds the concurrency limits of a single model copy. Note: There is a default maximum of 3 model copies per account per imported model that can be increased through Service Quotas. Regions: US East (N. Virginia) and US West (Oregon) Region: Europe (Frankfurt) Note:The Custom Model Units needed to host a model depend on a variety of factors - notably the model architecture, model parameter count, and context length. The exact number of Custom Model Units needed will be determined at the time of import. For reference, Llama 3.1 8B 128K model requires 2 Custom Model Units, a Llama 3.1 70B 128k model requires 8 Custom Model Units. *Billed in 5 minute windows On-Demand Inference Pricing:You are billed in 5-minute windows for the duration your model copy is active starting from the first successful invocation. The maximum throughput and concurrency limit per model copy depends on factors such as input/output token mix, hardware type, model size, architecture, inference optimizations, and is determined during the model import workflow. Bedrock automatically scales the number of model copies depending on your usage patterns. If there are no invocations for a 5-minute period, Bedrock will scale down to zero and scale back up when you invoke your model. While scaling back up, you may experience a cold-start duration (in tens of seconds) depending on model size. Bedrock also scales up the number of model copies if your inference volume consistently exceeds the concurrency limits of a single model copy. Note: There is a default maximum of 3 model copies per account per imported model that can be increased through Service Quotas. Regions: US East (N. Virginia) and US West (Oregon) Note:The Custom Model Units needed to host a model depend on a variety of factors - notably the model architecture, model parameter count, and context length. The exact number of Custom Model Units needed will be determined at the time of import. For reference, Llama 3.1 8B 128K model requires 2 Custom Model Units, a Llama 3.1 70B 128k model requires 8 Custom Model Units. *Billed in 5 minute windows On-Demand Inference Pricing:You are billed in 5-minute windows for the duration your model copy is active starting from the first successful invocation. The maximum throughput and concurrency limit per model copy depends on factors such as input/output token mix, hardware type, model size, architecture, inference optimizations, and is determined during the model import workflow. Bedrock automatically scales the number of model copies depending on your usage patterns. If there are no invocations for a 5-minute period, Bedrock will scale down to zero and scale back up when you invoke your model. While scaling back up, you may experience a cold-start duration (in tens of seconds) depending on model size. Bedrock also scales up the number of model copies if your inference volume consistently exceeds the concurrency limits of a single model copy. Note: There is a default maximum of 3 model copies per account per imported model that can be increased through Service Quotas. Tools and optimization pricing details Amazon Bedrock Flows You are charged based on the number of node transitions required to execute your application. Bedrock Flows counts a node transition each time a node in your workflow is executed. You are charged for the total number of node transitions across all your flows. All charges are metered daily and billed monthly starting February 1st, 2025. Additional Charges You may incur additional charges if the execution of your application workflow utilizes other AWS services or transfers data. For example, if your workflow invokes an Amazon Bedrock Guardrail policy, you will be billed for the number of text units processed by the policy. Structured Data Retrieval (SQL Generation) Structured Data Retrieval is charged for each request to generate a SQL query. The SQL query generated is used to retrieve the data from structured data stores. Rerank models Rerank models are designed to improve the relevance and accuracy of responses in Retrieval Augmented Generation (RAG) applications. They are charged per query. **You are charged for number of queries where a query can contain up to 100 document chunks. If the query contains more than 100 document chunks, it is counted as multiple queries. For example, if a request contains 350 documents, it will be treated as 4 queries. Please note that each document can only contain upto 512 tokens (inclusive of the query and document’s total tokens), and if the token length is higher than 512 tokens, it is broken down into multiple documents. A query is equivalent to a search unit. Amazon Bedrock Guardrails The pricing for Amazon Bedrock Guardrails is based on the charges incurred by the filter used in the guardrail.The pricing is the same for both standard tier and classic tier. Guardrails filter* Price $0.15 per 1,000 text units Content filters (image content) $0.00075 per image processed Denied topics for both standard tier and classic tier $0.15 per 1,000 text units Sensitive information filters $0.10 per 1,000 text units Sensitive information filters (regular expression) Free Word filters Free Contextual grounding checks $0.10 per 1,000 text units Automated Reasoning checks $0.17 per 1,000 text units per Automated Reasoning policy On-Demand pricing * Each guardrail filter is optional and can be enabled based on your application requirements. Charges will be incurred based on the filter type used in the guardrail. For example, if a guardrail is configured with content filters and denied topics, charges will be incurred for these two filters, while there will be no charges associated with sensitive information filters.Note: A text unit can contain up to 1000 characters. If a text input is more than 1000 characters, it is processed as multiple text units, each containing 1000 characters or less. For example, if a text input contains 5600 characters, it will be charged for 6 text units. Contextual grounding check uses a reference source and a query to determine if the model response is grounded based on the source and relevant to the query. The total number of text units charged is calculated by combining all the characters in the source, query, and model response. Model Evaluation Model evaluation is charged for the inference from your choice of model. Automatically-generated algorithmic scores are provided at no extra charge. For human-based evaluation where you bring your own workstream, you are charged for the model inference in the evaluation, and a charge of $0.21 per completed human task. Model Price per 1,000 input tokens Price per 1,000 output tokens Price per human task Model selected for evaluation Based on model selected Based on model selected  $0.21 Data Automation Amazon Bedrock Knowledge Bases offers a Bedrock Data Automation integration to provide more relevant and accurate responses for multimodal data. When setting up a Knowledge Base, you can select Bedrock Data Automation as your parsing method to analyze and extract meaningful insights from images or documents, which can include figures, charts, and diagrams. During processing, Bedrock Data Automation extracts meaningful information from ingested documents and images, which is then used in subsequent Knowledge Base steps for chunking, embedding, and storage. When integrated with Knowledge Bases, Bedrock Data Automation delivers and charges for standardized output. Intelligent Prompt Routing Intelligent Prompt Routing allows you to use a combination of foundation models (FMs) from the same model family to help optimize for quality and cost. For example, with theAnthropic’s Claudemodel family, Amazon Bedrock can intelligently route requests between Claude 3.5 Sonnet and Claude 3 Haiku depending on the complexity of the prompt. Similarly, Amazon Bedrock can route requests betweenMeta Llama3.3 70B and 3.18B, and Nova Pro and Nova Lite. The prompt router predicts which model will provide the best performance for each request while helping optimize the quality of response and cost. This is particularly useful for applications such as customer service assistants, where uncomplicated queries can be handled by smaller, faster, and more cost-effective models, and complex queries are routed to more capable models. Intelligent Prompt Routing can reduce costs by up to 30 percent without compromising on accuracy. Prompt Optimization for Amazon Bedrock You are charged based on the number of tokens in the input prompts and in the optimized prompts. All charges are billed monthly starting April 23, 2025. Pricing examples AI21 labs An application developer makes the following API calls to Amazon Bedrock: a request to AI21’s Jurassic-2 Mid model to summarize an input of 10K tokens of input text to an output of 2K tokens. Total cost incurred = 10K tokens/1000 * $0.0125 + 2K tokens/1000 * $0.0125 = $0.15 Amazon An application developer makes the following API calls to Amazon Bedrock on an hourly basis: a request to Amazon Titan Text Lite model to summarize an input of 2K tokens of input text to an output of 1K tokens. Total hourly cost incurred is = 2K tokens/1000 * $0.0003 + 1K tokens/1000 * $0.0004 = $0.001. An application developer makes the following API calls to Amazon Bedrock: a request to the Amazon Titan Image Generator base model to generate 1000 images of 1024 x 1024 in size of standard quality. Total cost incurred = 1000 images * $0.01 per image = $10 An application developer customizes an Amazon Titan Image Generator model using 1000 image-text pairs. After training, the developer uses custom model provisioned throughput for 1 hour to evaluate the performance of the model. The fine-tuned model is stored for 1 month. After evaluation, the developer uses provisioned throughput (1-month commitment term) to host the customized model. Monthly cost incurred for fine-tuning = fine-tuning training ($.005 * 500 * 64), where $0.005 is the price per image seen, 500 is the number of steps, and 64 is the batch size, + custom model storage per month ($1.95) + 1 hour of custom model inference ($21) = $160 + $1.95 + 21 = $182.95 An application developer buys two model units of Amazon Titan Text Express with a 1-month commitment for their text summarization use case. Total monthly cost incurred = 2 model units * $18.40/hour * 24 hours * 31 days = $27,379.20 An application developer buys one model unit of the base Amazon Titan Image Generator model with a 1-month commitment. Total cost incurred = 1 model unit * $16.20 * 24 hours * 31 days = $12,052.80 Amazon Bedrock Guardrails Example 1: Customer support chatbotAn application developer creates a customer support chatbot and uses content filters to block harmful content and denied topics to filter undesirable queries and responses.The chatbot serves 1000 user queries per hour. Each user query has an average input length of 200 characters and receives a FM response of 1500 characters.Each user query of 200 characters correspond to 1 text unit.Each FM response of 1,500 characters correspond to 2 text units.Text units processed each hour = (1 + 2) * 1000 queries = 3000 text unitsTotal cost incurred per hour for content filters and denied topics = 3000 * ($0.15 + $0.15) / 1000 = $0.90  Example 2: Call center transcript summarizationAn application developer creates an application to summarize chat transcripts between users and support agents. It uses sensitive information filter to redact personally identifiable information (PII) in the generated summaries for 10,000 conversations.Each generated summary has an average of 3,500 characters that corresponds to 4 text units.Total cost incurred to summarize 10,000 conversations = 10000 * 4 * ($0.1/1000) = $4  Example 3: Medical Protocol Verification EngineA healthcare technology company implements Automated Reasoning checks in their clinical decision support system to validate treatment suggestions against medical guidelines. The system processes 5,000 patient cases per month. Each case involves: Text units processed per month = (1 + 2 + 5) 5,000 cases = 40,000 text unitsTotal cost incurred per month for Automated Reasoning checks = 40,000 ($0.17) / 1000 = $6.80  Amazon Bedrock Knowledge Bases Pricing Example 1 (Reranking using Amazon Rerank 1.0 model) In a given month, you make 2 million requests to Rerank API using Amazon Rerank 1.0 model – 1 million requests contain fewer than 100 documents each and hence will be charged for one request each. The remaining 1 million requests contain 120-150 documents, and hence each request will be charged for 2 requests. Price for one request = $0.001Total charge = 1,000,000 * $0.001 + 1,000,000*2*$0.001= $3000 Pricing Example 2: (Structured data retrieval) An application developer creates a support chatbot that queries structured data stored in Amazon Redshift. The developer creates a Bedrock Knowledge Base and connects to Amazon Redshift. The chatbot serves 10000 user queries per hour. Each user query will cost $0.002 per GenerateQuery API to generate SQL from user query. Total cost incurred for generating SQL per hour = $0.002*10000 = $20.Total cost incurred in month = $20*24*30 = $1440 Anthropic An application developer makes the following API calls to Amazon Bedrock in the US West (Oregon) Region: a request to Anthropic’s Claude model to summarize an input of 11K tokens of input text to an output of 4K tokens. Total cost incurred = 11K tokens/1000 * $0.008 + 4K tokens/1000 * $0.024 = $0.088 + $0.096 = $0.184 An application developer buys one model unit of Anthropic Claude Instant in the US West (Oregon) Region: Total monthly cost incurred = 1 model unit * $39.60 * 24 hours * 31 days = $29,462.40 Cohere An application developer makes the following API calls to Amazon Bedrock: a request to Cohere’s Command model to summarize an input of 6K tokens of input text to an output of 2K tokens. Total cost incurred = 6K tokens/1,000 * $0.0015 + 2K tokens/1,000 * $0.0020 = $0.013 An application developer makes the following API calls to Amazon Bedrock: A request to Cohere’s Command - Light model to summarize an input of 6K tokens of input text to an output of 2K tokens. Total cost incurred = 6K tokens/1000 * $0.0003 + 2K tokens/1000 * $0.0006 = $0.003 An application developer makes the following API calls to Amazon Bedrock: A request to either Cohere’s Embed English or Embed Multilingual model to generate embeddings for 10K tokens of input. Total cost incurred = 10K tokens/1000 * $0.0001 = $.001 An application developer customizes a Cohere Command model using 1000 tokens of data. After training, uses custom model provisioned throughput for 1 hour to evaluate the performance of the model. The fine-tuned model is stored for 1 month. After evaluation, the developer uses provisioned throughput (1mo commit) to host the customized model. Monthly cost incurred for fine-tuning = Fine-tuning training ($0.004 * 1000) + custom model storage per month ($1.95) + 1 hour of custom model inference ($49.50) = $55.45 Monthly cost incurred for provisioned throughput (1-month commitment) of custom model = $39.60 Provisioned Throughput pricing An application developer, buys one model unit of Cohere Command with a 1-month commitment for their text summarization use case. Total monthly cost incurred = 1 model unit * $39.60 * 24 hours * 31 days = $29,462.40 Custom Model Import Pricing Example: An application developer imports a customized Llama 3.1 type model that is 8B parameter in size with a 128K sequence length in us-east-1 region and deletes the model after 1 month. This requires 2 Custom Model Units. So, the price per minute will be $0.1570 because 2 Custom Model Units are required. The model storage costs for 2 Custom Model Units would be $3.90 for the month. There is no charge to import the model. The first successful invocation is at 8:03 AM, at which time the metering starts. The 5-minute metering windows are from 8:03 AM - 8:07 AM; 8:07 AM - 8:11 AM, and so on. If there is at least one invocation during any 5-minute period, the window will be considered active for billing. If there is an invocation at 8:03 AM and no further invocations after 8:07 AM, the metering will stop at 8:07 AM. In this case, the bill would be calculated as follows: $0.1570 * 5 minutes * 1 five minute windows =$0.785. Data Automation Pricing example 1:Let’s say you process a 1,000 page document using BDA Custom Output. All 1,000 pages are processed using blueprint 1 which has 15 fields. The per page price for any blueprint with 30 fields or less is $0.040. The total cost would be $40.Total pages processed = 1,000Price per page for blueprints with less than 30 fields = $0.040Total charge = 1,000 * $0.040 = $40  Pricing example 2:Let’s say you process 2 documents using BDA Custom Output. Document 1 has 40 pages and is processed using blueprint 1 which has 20 fields. Document 2 has 10 pages and is processed using blueprint 2, which has 40 fields. The per page price of blueprint 1 is $0.040 since it contains 30 fields or less. The per page price of blueprint 2 is $0.045. The processing cost for Document 1 using blueprint 1 is $1.60. The processing cost for Document 2 using blueprint 2 is $0.45. The total cost of processing both documents would be $2.05.Total pages processed = 50Price per page for Blueprint 1 with less than 30 fields = $0.040Price per page for Blueprint 2 with 40 fields = $0.040 + (# of additional fields above 30 *$0.0005 per field)Number of additional fields above 30 = 40 - 30 = 10Price per page for Blueprint 2 with 40 fields = $0.040 + (10 *$0.0005 per field) = $0.045Charge for Document 1 using Blueprint 1 = 40 pages x $0.040 per page = $1.6Charge for Document 2 using Blueprint 2 = 10 pages x $0.045 per page = $0.45Total charge = Charge for Document 1 + Charge for Document 2 = $1.6 + $0.45 = $2.05  Pricing Example 3:Let’s say you setup Bedrock Knowledge Bases to use Bedrock Data Automation as a parser and then ingest a 1,000 page document. Note, cost structures differ between the Knowledge Bases parsing options. BDA uses per-page pricing, while Foundational Model parsers charge based on input and output tokens. For context, processing 1,000 pages, where 30% contain tables and 30% contain figures, typically requires 2,900 input tokens and 750 output tokens. Token consumption varies by content type, so customers are encouraged to test using their own data to get more accurate estimates. Bedrock Knowledge Bases and Bedrock Data Automation integration uses standard output, where the per page price is $0.010. The total cost would be $10. Total pages processed = 1,000Price per page for standard output = $0.010Total charge = 1,000 * $0.010 = $10  Pricing example 4:Let’s say you process a 60 minute video using BDA Standard Output. The per minute price for video standard output is $0.050. The total cost would be $3.00.Total minutes processed = 60Price per minute for video standard output = $0.050Total charge = 60 * $0.050 = $3.00  Pricing example 5:Let’s say you process 2,000 images using BDA Custom Output. The first 1,000 images are processed using blueprint 1, which has 10 fields. The last 1,000 pages are processed using blueprint 2, which has 40 fields. The per image price for blueprint 1 is $0.005, since it contains 30 fields or less. The per image price of blueprint 2 is $0.01. The processing cost for the first 1,000 images using blueprint 1 is $5.00. The processing cost for the second 1,000 images using blueprint 2 is $10.00. The total cost of processing all 2,000 images would be $15.00Cost for first 1000 images = 1,000 images * $0.005 per image = $5.00Cost for second 1,000 images = 1,000 images * ($0.005 + (# of additional fields above 30 *$0.0005 per field))= 1,000 * ($0.005 + ((40-30)*$0.0005))= 1,000 * ($0.005 + (10*$0.0005)) = $10.00Total cost = $5.00 + $10.00 = $15.00  Pricing example 6:Let’s assume that you want to use Bedrock Data Automation Standard Output to process 15,000 minutes of meeting audio recordings in your organization. The total cost of processing all 15,000 audio minutes would be $90.Total minutes processed = 15,000 minutesTotal charge = 15,000 min × $0.006 = $90  DeepSeek On-Demand pricing An application developer makes the following API calls to Amazon Bedrock on an hourly basis: a request to the DeepSeek-R1 model to summarize an input of 2K tokens of input text to an output of 1K tokens (including reasoning tokens): Total hourly cost incurred = 2K tokens/1000 * $0.00135 + 1K tokens/1000 * $0.0054 = $0.0081 Flows Example: News summarizationAn application developer creates a flow to automate news summarization for traders. The flow includes an Input node that takes in an S3 location, and a S3 retrieval node that retrieves 10 files that include articles from 10 major news agency in S3 (2 node transitions). It then uses an iterator node to invoke a model with a prompt node to summarize each file (+ 10 files x 2 node transitions). It then collects all the results using a collector node, write the results to S3 using S3 storage node, and complete in an Output node (+ 3 node transition). They run this flow every half hour of every week day. The number of node transition per flow execution is: 2+1+10*2 + 3 = 25 node transitions/flow execution The number of flow execution per month is: 24 hours *2* 5 days * 4 weeks = 960 flow executions/month. Total monthly bill is: 25 * 960 * $0.035/1000 = $0.84 Additional chargesThe bill will also include additional charges for AWS services used in the workflow execution, including Amazon S3 usages in the retrieval and storage nodes, and Amazon Bedrock foundation model usage in the prompt node. Meta An application developer makes the following API calls to Amazon Bedrock: a request to Meta’s Llama 2 Chat (13B) model to summarize an input of 2K tokens of input text to an output of 500 tokens. Total cost incurred = 2K tokens/1000 * $0.00075 + 500 tokens/1000 * $0.001 = $0.002 An application developer customizes the Llama 2 Pretrained (70B) model using 1000 tokens of data. After training, uses custom model provisioned throughput for 1 hour to evaluate the performance of the model. The fine-tuned model is stored for 1 month. After evaluation, the developer uses provisioned throughput (1mo commit) to host the customized model. Monthly cost incurred for fine-tuning = Fine tuning training ($0.00799 * 1000) + custom model storage per month ($1.95) + 1 hour of custom model inference ($23.50) = $33.44 Monthly cost incurred for provisioned throughput (a 1-month commit) of custom model = $21.18 An application developer buys one model unit of Meta Llama 2 with a 1-month commitment for their text summarization use case. Total monthly cost incurred = 1 model unit * $21.18 * 24 hours * 31 days = $15,757.92 Mistral AI An application developer makes the following API calls to Amazon Bedrock on an hourly basis: a request to Mistral 7B model to summarize an input of 2K tokens of input text to an output of 1K tokens. Total hourly cost incurred = 2K tokens/1000 * $0.00015 + 1K tokens/1000 * $0.0002 = $0.0005 An application developer makes the following API calls to Amazon Bedrock on an hourly basis: a request to Mixtral 8x7B model to summarize an input of 2K tokens of input text to an output of 1K tokens. Total hourly cost incurred = 2K tokens/1000 * $0.00045 + 1K tokens/1000 * $0.0007 = $0.0016 An application developer makes the following API calls to Amazon Bedrock on an hourly basis: a request to Mistral Large model to summarize an input of 2K tokens of input text to an output of 1K tokens. Total hourly cost incurred = 2K tokens/1000 * $0.008 + 1K tokens/1000 * $0.024 = $0.04 Model evaluation Model evaluation example 1: On-demand pricingAn application developer submits a dataset for human-based model evaluation using Anthropic Claude 2.1 and Anthropic Claude Instant in the US East (N. Virginia) AWS Region. The dataset contains 50 prompts, and the developer requires one worker to rate each prompt-response set (configurable in the evaluation job creation as “workers per prompt” parameter).There will be 50 tasks in this evaluation job (one task for each prompt-response set per each worker). The 50 prompts combine to 5000 input tokens, and the associated responses combine to 15,000 tokens for Anthropic Claude Instant and 20,000 tokens for Anthropic Claude 2.1.The following charges are incurred for this model evaluation job: Model evaluation example 2: On-demand pricingAn application developer submits a dataset for human-based model evaluation using Anthropic Claude 2.1 and Anthropic Claude Instant in the US East (N. Virginia) AWS Region.The dataset contains 50 prompts, and the developer requires two workers to rate each prompt-response set (configurable in the evaluation job creation as “workers per prompt” parameter). There will be 100 tasks in this evaluation job (1 task for each prompt-response set per each worker: 2 workers x 50 prompt-response sets = 100 human tasks).The 50 prompts combine to 5000 input tokens, and the associated responses combine to 15000 tokens for Anthropic Claude Instant and 20000 tokens for Anthropic Claude 2.1.The following charges are incurred for this model evaluation job: Prompt Optimization Example: News summarizationAn application developer creates a prompt to summarize news for traders using Claude 3.5. The original prompt includes 429 tokens. The optimized prompt has 511 tokens, and includes more specific instructions and examples to generate more concise answer from the FMs. He uses the optimized prompt with 511 tokens as the input for prompt optimizer, and creates 2 new variants for Claude 3.7 and Nova Pro with 582 and 579 tokens. The total number of input and output tokens for prompt optimization: 429 + 511 + 511 + 582 + 511 + 579 = 3,123 Total monthly bill is: 3,123 / 1000 * $0.03 = $0.09 Stability AI An application developer makes the following API calls to Amazon Bedrock: a request to the SDXL model to generate a 512 x 512 image with a step size of 70 (premium quality). Total cost incurred = 1 image * $0.036 per image = $0.036 An application developer makes the following API calls to Amazon Bedrock: A request to the SDXL 1.0 model to generate a 1024 x 1024 image with a step size of 70 (premium quality). Total cost incurred = 1 image * $0.08 per image = $0.08 An application developer buys one model unit of SDXL 1.0 with a 1-month commitment. Total cost incurred = 1 * $49.86 * 24 hours * 31 days = $37,095.84 TwelveLabs On-Demand pricing An application developer makes the following API calls to Amazon Bedrock: a request to thePegasus 1.2model to describe what a 10-second-long video entails, which provides an output of 2,000 tokens. Total cost incurred = 10 seconds * $0.00049 + 2K tokens / 1000 * $0.0075 = $0.0199 An application developer makes the following API calls to Amazon Bedrock: a request to theMarengo Embed [3.0 or 2.7]model to embed 10 videos, with a combined duration of 100 minutes. Total cost incurred = 100 minutes (i.e. 6000 secs) * $0.00070 = $4.2 An application developer makes the following API calls to Amazon Bedrock: a request to theMarengo Embed 3.0model to by providing a text and image together, to generate an embedding that they could use to find the clip that has the bag shown in the given image, across the embedding repository that they would have crated using above example. Total cost incurred = 1 text request * $0.00007 +1 image request * $0.0001 = $0.00017 An application developer makes the following API calls to Amazon Bedrock: a request to theMarengo Embed [3.0 or 2.7]model to by providing a text to generate an embedding that they could use to find matching clips from an the embedding repository that they would have crated using above example. Total cost incurred = 1 text request * $0.00007 = $0.00007 Writer An application developer makes the following API calls to Amazon Bedrock: a request to Writer’s Palmyra X5 model to summarize an input of 10K tokens of input text to an output of 2K tokens. Total cost incurred = 10K tokens/1000 * $0.003 + 2K tokens/1000 * $0.015 = $0.06 Next steps Explore common Amazon Bedrock use cases with a guided workshop View demos of Amazon Bedrock capabilities Learn Resources Developers Help", "images": [], "links": ["https://aws.amazon.com/bedrock/pricing/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/agentcore/pricing/", "https://aws.amazon.com/bedrock/service-tiers/", "https://aws.amazon.com/bedrock/service-tiers/", "https://aws.amazon.com/bedrock/service-tiers/", "https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-supported.html", "https://aws.amazon.com/bedrock/flows/", "https://aws.amazon.com/bedrock/guardrails/", "https://aws.amazon.com/bedrock/flows/", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-supported.html", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/claude/", "https://aws.amazon.com/bedrock/llama/", "https://aws.amazon.com/bedrock/pricing/"]}
{"url": "https://aws.amazon.com/bedrock/service-tiers/", "text": " Search Amazon Bedrock Amazon Bedrock Service Tiers Match the right service tier to your workload needs Service options to match your workload needs Priority Tier Priority requests receive preferential treatment in the processing queue, moving ahead of standard requests for faster responses even during high-demand periods. Priority Tier is ideal for mission critical workflows and customer-facing applications highly sensitive to latency.  For most models that support Priority Tier, customers can realize up to 25% better output tokens per second (OTPS) latency compared to standard tier. Pricing: Standard rate + premium Use Cases: Real-time customer service, live applications, urgent decision-making systems Supported Models: Check thedocumentation. Standard Tier The Standard tier provides consistent performance at regular rates for everyday AI tasks. Pricing: Standard rate per token Use Cases: General AI applications, content generation, routine analysis Supported Models:Available across all foundation models in Amazon Bedrock.Learn more Flex Tier Flex tier offers discounted standard pricing for workloads that can trade immediate processing for cost efficiency. Perfect for non-urgent AI workloads. Pricing:  Discounted standard model rate. Use Cases:Model evaluations, summarizations, multi-step agentic workflows Supported Models:Check thedocumentation  Page topics Overview What are the new service tiers for Amazon Bedrock? Amazon Bedrock now offers two new service tiers in addition to the existing Standard tier: How do these new tiers differ from the existing on-demand Standard tier? The Standard tier remains available as the default service option with reliable performance for everyday AI applications. The new service tiers provide additional inference options depending on performance and cost requirements. Priority What is the Priority tier? The Priority tier is a premium service tier that provides faster response times and preferential processing for time-sensitive workloads. For most models that support Priority tier, customers can realize up to 25% better output tokens per second (OTPS) latency compared to standard tier. When should I use Priority? Priority is a good fit for: How does preferential processing work? Priority tier requests receive access to more compute resources and processing priority over other tiers, providing faster performance even when the system is under heavy load. Flex What is the Flex tier? The Flex tier is a cost-effective service tier designed for non-time-critical workloads. It can be used in cases where your applications and agentic workflows can tolerate some increase in latency. When should I use Flex? The Flex tier is ideal for: What kind of delays can I expect with Flex? Flex may experience longer latencies than standard tier. During high traffic, Flex tier requests are processed after Standard tier requests. It is designed for non-interactive workloads that can tolerate these longer latencies. What's the difference between Flex and Batch inference? While both options are cost-effective for non-time-critical workloads, they serve different use cases: Model Availability Which models are available with the new service tiers? We have several models available from leading providers like OpenAI, DeepSeek, Qwen and Amazon.For a complete list check thedocumentation. Choosing the Right Tier How do I decide which service tier to use? Consider these factors: Can I switch between service tiers? Yes, you can choose different service tiers based on your specific use case and requirements for each API call or application. Will pricing be different between the service tiers? Yes, each service tier has a different pricing structure. Flex tier offers cost savings over standard pricing for flexible workloads, while Priority tier commands a premium for faster performance and prioritized compute. Please see theAmazon pricing pagefor specific prices by model. Getting Started How do I start using the new service tiers? The new service tiers are available through the Amazon Bedrock console and API. You can use the same inference API as the default standard tier by just adding an extra parameter to select service tier at invoke time. Please reviewdocumentationfor more details. Do I need to make any changes to my existing applications? Existing applications will continue to work with the Standard tier. To take advantage of the new service tiers, you'll need to update your API calls to specify the desired service tier for supported models. Where can I find more information about pricing details? For detailed pricing information and the latest updates on model availability, please refer to theAmazon Bedrock pricing pageor contact your AWS account team. Estimate your monthly Amazon Bedrock costs Get started with Amazon Bedrock Get started with Amazon Bedrock Explore common Amazon Bedrock use cases with a guided workshop Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/Priority-tier.fa21e683cc0c50327c35674be715ea09df0407ae.png", "local": "images\\e22953991378a6730e993976cd2ec87f2bf7e6be.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/Standard-tier.f529366d2b40efcc371da134646207f675b57c3e.png", "local": "images\\0129d92751548744a82520c369c126001be88487.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/Flex-tier.36f5047404a3935d52de6a308f9a88bbfeafb333.png", "local": "images\\64270178f2f78af299e63a0c25c34b6cf1ceacda.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/service-tiers/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/pricing/", "https://docs.aws.amazon.com/bedrock/latest/userguide/service-tiers-inference.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/inference-supported.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/service-tiers-inference.html", "https://aws.amazon.com/bedrock/service-tiers/#overview", "https://aws.amazon.com/bedrock/service-tiers/#priority", "https://aws.amazon.com/bedrock/service-tiers/#flex", "https://aws.amazon.com/bedrock/service-tiers/#model-availability", "https://aws.amazon.com/bedrock/service-tiers/#choosing-the-right-tier", "https://aws.amazon.com/bedrock/service-tiers/#getting-started", "https://aws.amazon.com/bedrock/service-tiers/", "https://aws.amazon.com/bedrock/service-tiers/", "https://aws.amazon.com/bedrock/service-tiers/", "https://aws.amazon.com/bedrock/service-tiers/", "https://docs.aws.amazon.com/bedrock/latest/userguide/service-tiers-inference.html", "https://aws.amazon.com/bedrock/service-tiers/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/service-tiers/", "https://docs.aws.amazon.com/bedrock/latest/userguide/service-tiers-inference.html", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/service-tiers/"]}
{"url": "https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-supported.html", "text": "Supported Regions and models for batch inference The following list provides links to general information about Regional and model support in Amazon Bedrock: For a list of Region codes and endpoints supported in Amazon Bedrock, seeAmazon Bedrock endpoints and quotas. For a list of Amazon Bedrock model IDs to use when calling Amazon Bedrock API operations, seeSupported foundation models in Amazon Bedrock. For a list of Amazon Bedrock inference profile IDs to use when calling Amazon Bedrock API operations, seeSupported cross-Region inference profiles. Batch inference can used with different types of models. The following list describes support for different types of Amazon Bedrock models: Single-region model supportâ Lists regions that support sending inference requests to a foundation model in one AWS Region. For a full table of models available across Amazon Bedrock, seeSupported foundation models in Amazon Bedrock. Cross-region inference profile supportâ Lists regions that support using a cross-region inference profile, which support sending inference requests to a foundation model in multiple AWS regions within a geographical area. An inference profile has a prefix preceding the model ID that indicates its geographical area (for example,us.,apac). For more information for available inference profiles across Amazon Bedrock, seeSupported Regions and models for inference profiles. Custom model supportâ Lists regions that support sending inference requests to a customized model. For more information about model customization, seeCustomize your model to improve its performance for your use case. The following table summarizes support for batch inference: us-east-1 us-east-1 us-gov-west-1 ap-northeast-1 ap-northeast-2 ap-south-1 ap-southeast-1 ap-southeast-2 eu-central-1 eu-central-1 eu-north-1 eu-south-1 eu-south-2 eu-west-1 eu-west-3 us-east-1 us-east-2 us-west-2 us-east-1 us-gov-west-1 ap-northeast-1 ap-northeast-2 ap-south-1 ap-southeast-1 ap-southeast-2 eu-central-1 eu-central-1 eu-north-1 eu-south-1 eu-south-2 eu-west-1 eu-west-3 us-east-1 us-east-2 us-west-2 us-east-1 us-east-2 us-west-2 us-east-1 us-gov-west-1 ap-northeast-1 ap-northeast-2 ap-south-1 ap-southeast-1 ap-southeast-2 eu-central-1 eu-central-1 eu-north-1 eu-south-1 eu-south-2 eu-west-1 eu-west-3 us-east-1 us-east-2 us-west-2 ap-northeast-2 ap-south-1 ap-southeast-2 ca-central-1 eu-central-1 eu-west-1 eu-west-2 eu-west-3 sa-east-1 us-east-1 us-west-2 us-east-1 us-west-2 ap-northeast-1 ap-northeast-2 ca-central-1 eu-central-1 eu-north-1 eu-south-1 eu-south-2 eu-west-2 sa-east-1 us-east-1 us-west-2 ap-northeast-1 ap-northeast-2 ap-south-1 ap-southeast-1 ap-southeast-2 ca-central-1 eu-central-1 eu-west-1 eu-west-2 eu-west-3 sa-east-1 us-east-1 us-west-2 us-west-2 us-east-1 us-west-2 ap-northeast-2 ap-south-1 ap-southeast-2 ca-central-1 eu-central-1 eu-west-1 eu-west-2 eu-west-3 sa-east-1 us-east-1 us-west-2 ap-northeast-1 ap-northeast-2 ap-south-1 ap-southeast-1 ap-southeast-2 eu-central-1 eu-west-1 eu-west-3 us-east-1 us-west-2 us-west-2 us-east-1 us-east-2 us-west-2 ap-northeast-1 ap-northeast-2 ap-southeast-1 eu-central-1 us-east-1 us-east-2 us-west-2 ap-northeast-1 ap-northeast-2 ap-south-1 ap-southeast-1 ap-southeast-2 eu-central-1 eu-west-1 eu-west-3 us-east-1 us-west-2 us-west-2 ap-northeast-1 ap-northeast-2 ap-northeast-3 ap-south-1 ap-south-2 ap-southeast-1 ap-southeast-2 us-east-1 us-east-2 us-west-2 ap-northeast-1 ap-northeast-2 ap-south-1 ap-south-2 ap-southeast-1 ap-southeast-2 eu-central-1 eu-north-1 eu-west-1 eu-west-3 us-east-1 us-east-2 us-west-2 ap-northeast-1 ap-northeast-2 ap-northeast-3 ap-south-1 ap-south-2 ap-southeast-1 ap-southeast-2 ap-southeast-3 ap-southeast-4 ca-central-1 eu-central-1 eu-central-2 eu-north-1 eu-south-1 eu-south-2 eu-west-1 eu-west-2 eu-west-3 sa-east-1 us-east-1 us-east-2 us-gov-west-1 us-west-1 us-west-2 ap-east-2 ap-northeast-1 ap-northeast-2 ap-northeast-3 ap-south-1 ap-south-2 ap-southeast-1 ap-southeast-2 ap-southeast-3 ap-southeast-4 ap-southeast-5 ap-southeast-7 eu-central-1 eu-north-1 eu-south-1 eu-south-2 eu-west-1 eu-west-3 il-central-1 me-central-1 us-east-1 us-east-2 us-west-1 us-west-2 ap-northeast-1 ap-northeast-2 ap-northeast-3 ap-south-1 ap-south-2 ap-southeast-1 ap-southeast-2 ap-southeast-3 ap-southeast-4 ca-central-1 eu-central-1 eu-central-2 eu-north-1 eu-south-1 eu-south-2 eu-west-1 eu-west-2 eu-west-3 sa-east-1 us-east-1 us-east-2 us-west-1 us-west-2 ap-northeast-1 ap-south-1 ap-southeast-3 eu-north-1 eu-west-2 us-east-2 us-west-2 us-west-2 us-west-2 us-east-1 us-west-2 us-west-2 us-east-1 us-west-2 us-west-2 us-east-1 us-west-2 eu-west-3 us-west-2 eu-central-1 eu-west-1 eu-west-3 us-east-1 us-west-2 eu-west-3 us-west-2 eu-central-1 eu-west-1 eu-west-3 us-east-1 us-west-2 us-west-2 us-east-1 us-west-2 us-east-1 us-west-2 us-east-1 us-east-2 us-west-2 us-east-1 us-west-2 us-east-1 us-east-2 us-west-2 us-east-1 us-west-2 us-east-1 us-east-2 us-west-2 us-west-2 us-east-1 ap-northeast-1 ap-south-1 ap-southeast-3 eu-central-1 eu-north-1 eu-south-1 eu-west-1 eu-west-2 sa-east-1 us-east-1 us-east-2 us-west-2 ap-northeast-1 ap-south-1 ap-southeast-3 eu-central-1 eu-north-1 eu-south-1 eu-west-1 eu-west-2 sa-east-1 us-east-1 us-east-2 us-west-2 ap-northeast-1 ap-south-1 ap-southeast-3 eu-central-1 eu-north-1 eu-south-1 eu-west-2 us-east-2 us-west-2 ap-northeast-1 ap-south-1 ap-southeast-3 eu-central-1 eu-north-1 eu-south-1 eu-west-1 eu-west-2 sa-east-1 us-east-1 us-east-2 us-west-2 ap-northeast-1 ap-south-1 ap-southeast-3 eu-north-1 eu-west-2 us-east-2 us-west-2 ap-northeast-1 ap-south-1 ap-southeast-3 eu-central-1 eu-north-1 eu-south-1 eu-west-1 eu-west-2 sa-east-1 us-east-1 us-east-2 us-west-2 Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Thanks for letting us know we're doing a good job! If you've got a moment, please tell us what we did right so we can do more of it.  Thanks for letting us know this page needs work. We're sorry we let you down. If you've got a moment, please tell us how we can make the documentation better. ", "images": [{"url": "https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png", "local": "images\\43cee1e4928af0f2dc9e20c760192ed234d32a41.png", "alt": "Warning"}], "links": ["https://docs.aws.amazon.com/pdfs/bedrock/latest/userguide/bedrock-ug.pdf#batch-inference-supported", "https://docs.aws.amazon.com/bedrock/index.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html", "https://docs.aws.amazon.com/general/latest/gr/bedrock.html#bedrock_region", "https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-support.html#inference-profiles-support-system", "https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-support.html", "https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html"]}
{"url": "https://aws.amazon.com/bedrock/flows/", "text": " Search Amazon Bedrock Amazon Bedrock Flows Accelerate development and scaling of generative AI workflows Demo Learn how to leverage the flexibility and power of Bedrock Flows to accelerate your AI application development and deployment without writing code. Quickly build generative AI workflows visually Bedrock Flows accelerates the creation, testing, and deployment of user-defined workflows for generative AI applications through an intuitive visual builder. You can seamlessly drag, drop and link Prompts, Agents, Knowledge bases, Guardrails, Lex, Lambda, other AWS services, with business logic to create a workflow. This removes the need to write code and offers easy customization of the business logic. You can also create and updates flows using APIs or AWS Cloud Development Kit. Test and deploy faster with serverless infrastructure You can test your flows directly in the console for faster iteration with built-in traceability of inputs and outputs. Once a flow is ready, you can version the flow and integrate it into your generative AI application. The versioning capability on flows enables an easy rollback mechanism, and A/B testing. You can invoke the flow via an API call to integrate with your application without the need to deploy and manage your own infrastructure. Collaborate on workflow creation Prompt Flows is available in Amazon Bedrock Studio, an SSO-enabled web interface that provides the easiest way for developers across an organization to experiment and collaborate with access to FMs. You can collaborate directly with your teammates to create, evaluate and deploy the right prompt flows for your use case. Customer Quotes Thomson Reuters Thomson Reuterstransforms the way professionals work by delivering innovative tech and GenAI powered by trusted expertise and industry-leading insights. \"The mandate of the Thomson Reuters Enterprise AI Platform is to enable our subject-matter experts, engineers, and AI researchers to co-create Gen-AI capabilities that bring cutting-edge, trusted technology in the hands of our customers and shape the way professionals work. Bedrock Flows will enable us to create complex, flexible, multi-prompt workflows which we can easily evaluate, compare and version. We can also quickly integrate flows with our applications using the SDK APIs for serverless flow execution — without wasting time in deployment and infrastructure management. We are excited about the potential productivity gain and acceleration for generative-AI application development with Bedrock Flows.\"  Dentsu Creative Dentsu Creative is a global creative agency network designed to create meaningful connection between brands and consumers. \"We have successfully leveraged Amazon Bedrock Flows to transform customer experiences. Using Bedrock Flows, we accelerated the process of reshaping books into an easy-to-read format for readers with learning disabilities. Bedrock Flows also enabled us to easily connect customer service solutions with foundation models like Claude Haiku to address common inquiries, saving hours and allowing customer support team to focus on more complex requests. By empowering non-technical users to understand how AI and business logic are applied with the intuitive visual interface, Bedrock Flows has driven transparency and visibility for generative AI solutions in our organization. Whether reaching new audiences or scaling customer requests, Dentsu continues to innovate with cutting-edge generative AI technology powered by Amazon Bedrock Flows.\" How to get started Amazon Bedrock Flows is now generally available with enhanced safety and traceability Amazon Bedrock Flows user guide Simplifying the Prompts Lifecycle with Prompt Management and Flows for Amazon Bedrock No-code generative AI application development with Amazon Bedrock Flows Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/2890a0e7edf3937094db2fe9673a5be7.ace0eaac12bae0b91e800ca1d2ccd61a3cc27531.png", "local": "images\\79eef1aab2f37d0c5da91dd0e49e93c32674b8ff.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/2d7c0eec058fa4ac20dd4d5c4f723f56.e64b373137432b5650014a5dd7891e55d25ef62d.png", "local": "images\\797584fe41651a3f23972c0456e87f787ef7986f.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/bd06dc2e364f87352b8c5aff7a6c9f6e.f8e898d9a2b243729cab9640493658664b5cb3d3.png", "local": "images\\d53f5a1b5fc2033fa7ab2f446e6ea3cdc7403e64.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/8df269d958a820e550b3264cea229aa8.35e34aace84c90be25523aef45c9bd2e2ccce5ba.png", "local": "images\\02bc6f40f84110baa4133850752d650489466c62.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/5da119793dd38a16d255bd1f197aa83b.1cc1067697721f438e6767c0ef6d3a6696b45ed8.png", "local": "images\\2afb4689758794c41fd730cf71f06ec970e37aa2.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/flows/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock/latest/userguide/flows.html", "https://aws.amazon.com/bedrock/flows/"]}
{"url": "https://aws.amazon.com/bedrock/claude/", "text": " Search Amazon Bedrock Claude by Anthropic in Amazon Bedrock Build generative AI solutions with Anthropic’s state-of-the-art model, Claude Claude 4 by Anthropic Claude Haiku 4.5, Sonnet 4.5, and Opus 4.5 are hybrid reasoning models oﬀering two modes: near-instant responses and extended thinking for deeper reasoning. These foundation models were developed to build advanced AI agents that can analyze thousands of data sources, execute long-running tasks, write high-quality content, and perform complex actions. Claude Opus 4.5 Claude Opus 4.5 is Anthropic's most intelligent model and sets a new standard for production code, sophisticated agents, and complex office tasks. Opus 4.5 delivers state-of-the-art coding capabilities and frontier performance at one-third the cost of previous Opus-level intelligence. It excels at complex, longer-running tasks that require pulling information across multiple data sources, handling ambiguity and reasoning about tradeoffs without hand-holding. With advanced capabilities including tool search and tool use, Claude can scale to use hundreds of tools. Plus an effort parameter, in beta, that lets you balance performance with latency, and cost, Opus 4.5 serves as an ideal virtual collaborator for sustained reasoning and production-grade work. From transforming multi-day coding projects into hours-long tasks to managing comprehensive workflows across finance, cybersecurity, manufacturing, healthcare, and sales, it handles the most demanding enterprise applications with unprecedented reliability. Claude Haiku 4.5 Claude Haiku 4.5 delivers near-frontier performance matching Claude Sonnet 4's capabilities in coding, computer use, and agent tasks at substantially lower cost and faster speeds, making state-of-the-art AI accessible for scaled deployments and budget-conscious applications. It excels at applications like customer service agents and chatbots where response time is critical, while delivering significant performance improvements for computer use tasks that enable faster and more responsive applications. This model unlocks new use cases where customers previously had to choose between performance and cost, enabling cost-effective agent experiences and supporting multi-agent systems for complex coding projects. This allows Haiku 4.5 to power large-scale financial analysis and research applications with unprecedented efficiency, maintaining Claude's unique character while delivering the performance and cost-effectiveness needed for production deployments across enterprise environments. Claude Sonnet 4.5 Claude Sonnet 4.5 is Anthropic's most intelligent model and its best performing for building complex agents, delivering leading coding capabilities and executing across development tasks while maintaining the optimal balance of speed and cost for production environments. It excels at autonomously planning and executing complex, multi-step workflows and is particularly effective in areas like finance, research, and cybersecurity. With enhanced tool handling and memory capabilities, Claude Sonnet 4.5 serves as an ideal virtual collaborator for sustained reasoning and long-horizon tasks. This allows Sonnet 4.5 to handle sophisticated agent applications with unprecedented reliability, from managing comprehensive research projects to executing end-to-end business process automation across multiple enterprise systems. Trustworthy AI Systems Benefits Up to 1M token context window (preview) By default, Anthropic's Claude models have a 200,000 token context window enabling you to relay a large volume of information to Claude. This translates to roughly 150,000 words, or over 500 pages of material. Sonnet 4 and 4.5 now offers an expanded context length of 1 million tokens (preview). This allows for large-scale code analysis, lengthy document synthesis, and the creation of more sophisticated context-aware agents that can maintain coherence across hundreds of tool calls and multi-step workflows, including complete API documentation and interaction histories. Intelligence Vision Speed Frontier AI safety features Building Agentic AI with Amazon Bedrock AgentCore and Claude Amazon Bedrock AgentCore and Claude are revolutionizing enterprise agentic AI by combining frontier model capabilities with production-grade infrastructure. Learn how AgentCore's fully-managed services—including Runtime, Memory, Identity, Gateway, Code Interpreter, Browser Tool, and Observability—enable developers to focus on agent logic and business value rather than building infrastructure from scratch. Learn how Claude's high performance and contextual understanding, paired with AgentCore's enterprise-grade security, reliability, and built-in tools, deliver autonomous agents that can run for up to eight hours with automatic scaling. Watch thevideoor read theblog. Amazon and Anthropic strategic collaboration In this fireside chat, Dario Amodei, CEO and co-founder of Anthropic, discusses Claude and how Anthropic and AWS are working together to accelerate the responsible deployment of generative AI. Use cases Customer service Claude can act as an always-on virtual sales representative, ensure speedy and friendly resolution to service requests, and increase customer satisfaction. Operations Claude can extract relevant information from business emails and documents, categorize and summarize survey responses, and wrangle reams of text with high speed and accuracy. Legal Claude is able to parse legal documents and answer questions about them, so lawyers can reduce costs and focus on higher-level work. Insurance Claude can help insurance agents make faster, better claims decisions by providing insights across customer conversations, claims documents, and policies. Coding Claude can help boost developer productivity by assisting with in-line code generation, debugging, and having natural-language conversation to help developers understand existing code. Model versions Customers Case studies  Doordash Perplexity FeatherSnap Showpad Alida Health eCareers Elastic PGA Tour How to get started Get Started with Claude Opus 4.5 in Amazon Bedrock Anthropic’s Claude Sonnet 4.5 is now in Amazon Bedrock Anthropic’s Claude 4 is now available in Amazon Bedrock Claude 3.7 now available in Amazon Bedrock Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/doordash logo-1.e4f6c35817ca6fa2d97018ab945f17effab130a4.png", "local": null, "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/perplexity-logo-1.4c1912c3bbe87eba5d68513cfd48941ee4e2b55b.png", "local": "images\\5e0e2a42f980f7379c2ea8fe4aa5ef24969d54de.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/feathersnap-logo-1.2789bd7186404e4f36079920043a3b6213c89dc4.png", "local": "images\\e81825085902ad4cdc83c36ebc50beeee90ddc89.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/showpad-logo-1.ef1b3e405d706e0b91c88f3704f664bf8a1c6843.png", "local": "images\\9aad1cad673587acb3a778836661fa23aca9ee24.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/alida-logo-1.2eeba405b4528ffb413f8c5d43274e966ed3a35f.png", "local": "images\\647af64c21c43787b8a99a78216ffc264114286e.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/health-e-careers-logo-1.4b08ecca18da2f527cac7ebd0514eea0bdfa13c8.png", "local": "images\\734093610a8d0df03fdb0065bf5c25a5e30aa40d.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/elastic-logo-1.93dd2e50c7e282f3efcf90d40633e0fe9f83d308.png", "local": null, "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/artificial-intelligence/approved/images/pga-tour-logo-1.dbe74d2d9eb4da1189923f56324cec695cff3e71.png", "local": "images\\e6daedb5d7400ef0982bfc599bf4aecd0f44f6cb.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/claude/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://console.aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/claude/"]}
{"url": "https://aws.amazon.com/bedrock/llama/", "text": " Search Amazon Bedrock Meta's Llama in Amazon Bedrock Build the future of AI with Llama Introducing Llama 4 The Llama 4 models mark the beginning of a new era for the Llama ecosystem, delivering the most scalable generation of Llama. With native multimodality, mixture-of-experts architecture, expanded context windows, significant performance improvements, and optimized computational efficiency, Llama 4 is engineered to address diverse application requirements. The Llama 4 models come in easy-to-deploy sizes, making them adaptable for various use cases. Llama 4 Maverick 17B Llama 4 Maverick is a natively multimodal model for image and text understanding with advanced intelligence and fast responses at a low cost. Llama 4 Scout 17B Llama 4 Scout is a natively multimodal model that integrates advanced text and visual intelligence with efficient processing capabilities. The model enables comprehensive multi-document analysis, robust codebase reasoning, and sophisticated data processing through its extensive context handling. Benefits Personalized and more performant Llama 3.2 offers a more personalized AI experience, with on-device processing. The Llama 3.2 models are designed to be more efficient, with reduced latency and improved performance, making them suitable for a wide range of applications. 128K token context window 128K context length allows Llama to capture even more nuanced relationships in data. Pretrained on over 15 trillion tokens Llama models are trained on over 15 trillion tokens from online public data sources to better comprehend language intricacies. Support in multiple languages Llama 3.2 is multilingual and supports eight languages including English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai. Zero infrastructure management The Amazon Bedrock managed API makes using Llama models easier than ever. Organizations of all sizes can access the power of Llama without worrying about the underlying infrastructure. Since Amazon Bedrock is serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy the generative AI capabilities of Llama into your applications using the AWS services you are already familiar with. This means you can focus on what you do best—building your AI applications. Meet Llama For over the past decade, Meta has been focused on putting tools into the hands of developers and fostering collaboration and advancements among developers, researchers, and organizations. Llama models are available in a range of parameter sizes, enabling developers to select the model that best fits their needs and inference budget. Llama models in Amazon Bedrock open up a world of possibilities because developers don't need to worry about scalability or managing infrastructure. Amazon Bedrock is a turnkey way for developers to get started using Llama. Use cases Llama models excel at image understanding and visual reasoning, language nuances, contextual understanding, and complex tasks, such as visual data analysis, image captioning, dialogue generation, and translation, and can handle multistep tasks seamlessly. Additional use cases Llama models are a great fit for include sophisticated visual reasoning and understanding, image-text-retrieval, visual grounding, document visual question answering, text summarization and accuracy, text classification, sentiment analysis and nuance reasoning, language modeling, dialog systems, code generation, and following instructions. Model versions Nomura uses Llama models from Meta in Amazon Bedrock to democratize generative AI Aniruddh Singh, Nomura's Executive Director and Enterprise Architect, outlines the financial institution’s journey to democratize generative AI firm-wide using Amazon Bedrock and Llama models from Meta. Amazon Bedrock provides critical access to leading foundation models like Llama, enabling seamless integration. Llama offers key benefits to Nomura, including faster innovation, transparency, bias guardrails, and robust performance across text summarization, code generation, log analysis, and document processing. TaskUs revolutionizes customer experiences using Llama models from Meta in Amazon Bedrock TaskUs, a leading provider of outsourced digital services and next-generation customer experience to the world’s most innovative companies, helps its clients represent, protect, and grow their brands. Its innovative TaskGPT platform, powered by Amazon Bedrock and Llama models from Meta, empowers teammates to deliver exceptional service. TaskUs builds tools on TaskGPT that leverage Amazon Bedrock and Llama for cost-effective paraphrasing, content generation, comprehension, and complex task handling. Additional resources Llama in Amazon Bedrock documentation Meta’s Llama documentation Meta on AWS Code Repository Customizing models for enhanced results: Fine-tuning in Amazon Bedrock How to get started Meta's Llama 4 now available in Amazon Bedrock Meta’s Llama 3.3 now available in Amazon Bedrock Meta’s Llama 3.2 now available in Amazon Bedrock Learn Resources Developers Help", "images": [], "links": ["https://aws.amazon.com/bedrock/llama/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://console.aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html", "https://aws.amazon.com/bedrock/llama/"]}
{"url": "https://aws.amazon.com/bedrock/model-choice#aws-page-content-main", "text": " Search Amazon Bedrock Model choice Turn model flexibility into a strategic business advantage—choose from industry-leading foundation models, deploy custom models, or explore specialized models Match your AI workloads to the perfect model every time With access to hundreds of top foundation models (FMs) to power your applications and the ability to swap them in and out without rewriting code, Amazon Bedrock gives you the flexibility to build and innovate as your needs evolve. The Amazon Bedrock built-in evaluation tools help you compare and balance performance, cost, and accuracy across models, turning model selection into a strategic advantage that drives real business outcomes. Broad selection of fully managed models from leading AI companies Unlock your freedom to innovate with flexible AI Access leading models on demand with no infrastructure hassle Take advantage of the latest generative AI innovations with easy access to a choice of high-performing models from leading AI companies like AI21 Labs, Anthropic, Cohere, DeepSeek, Luma AI, Meta, Mistral AI, OpenAI, Qwen, Stability AI, TwelveLabs, Writer, and Amazon. Start building today without provisioning servers or managing infrastructure. Discover emerging and specialized models for domain-specific tasks Amazon Bedrock Marketplace lets you discover, test, and use over 100 popular, emerging, and specialized FMs alongside other industry-leading models in Amazon Bedrock. Maximize existing AI investments by importing custom models Protect your competitive advantage by bringing proprietary models to Amazon Bedrock. Amazon Bedrock Custom Model Import lets you import and use your customized models alongside existing, out-of-the-box FMs through a single, serverless, unified API. You can access your imported custom models on demand and without the need to manage underlying infrastructure. Evaluate and optimize for what matters Build with confidence by comparing and validating responses from different models and Retrieval Augmented Generation (RAG) workflows for your real-world scenarios with Amazon Bedrock Evaluations. Evaluate model outputs for brand voice, friendliness, and relevance and assess workflows for context relevance and correctness. For deeper insights, use LLM-as-a-judge to evaluate outputs for harmfulness, completeness, and accuracy using your own prompts and criteria. Featured model providers Amazon Family of FMs delivering fast and cost effective multimodal intelligence-including text, image, document, and video understanding, image and video generation, interactive speech, and code generation. Anthropic Excels at complex reasoning, code generation, & instruction following. DeepSeek Advanced reasoning models that solve complex problems step-by-step. Mistral AI Specialized expert models for agentic reasoning and multimodal tasks. Meta Advanced image & language reasoning. OpenAI Open weight models that excel at coding, scientific analysis, and mathematical reasoning. Why model choice matters to customers Veolia Discover how Veolia uses almost all FMs available in Amazon Bedrock to automate translation services, image generation, and knowledge querying to solve critical environmental challenges. Vercel Learn how Vercel leverages Amazon Bedrock's breadth of model choice to enable experimentation and innovation while its reliability and scalability powers breakthrough developer toolkits for next-generation experiences. Get started Sign in to the Amazon Bedrock console Learn more about Amazon Bedrock Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Large_Language_Model_2_1200.e58db46b51106302fbbb6bb21819e35b66f6fc0f.png", "local": "images\\f7fe3ae0a0634cdfad2fc0a114687983bb8edb0d.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Large_Language_Model_1_1200_1.9b916d5df94e6e6380e6f6f4ff0fba09b2e1d1bc.png", "local": "images\\b7a33de2014722604c95f9a5363c18a18ea3b6c3.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Prompt_Engineering_1_1200_1.10c1252ea6c474527b08b89a5fc37d4b08648a45.png", "local": "images\\46084ae0eb0fcd90d1e65dff447cf4ed0323c264.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/AWS_Illustration_Fine_Tuning_1_1200.5142165611cec32fbea50154e8aa55602134b6c6.png", "local": "images\\eaaedbff7793c5ad2343061cce7dfd7d0c222331.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/model-choice#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://console.aws.amazon.com/bedrock/", "https://console.aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/marketplace/", "https://aws.amazon.com/bedrock/custom-model-import/", "https://aws.amazon.com/bedrock/evaluations/", "https://aws.amazon.com/bedrock/anthropic/", "https://aws.amazon.com/bedrock/deepseek", "https://aws.amazon.com/bedrock/mistral/", "https://aws.amazon.com/bedrock/meta/", "https://aws.amazon.com/bedrock/openai/", "https://console.aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock/latest/userguide/foundation-models-reference.html", "https://aws.amazon.com/bedrock/model-choice"]}
{"url": "https://aws.amazon.com/bedrock/marketplace/", "text": " Search Amazon Bedrock Amazon Bedrock Marketplace Access over 100 models in Amazon Bedrock Overview Key Benefits Expanded model selection Unified and secure experience Configurable scalability Customers  Zendesk With a diverse and multicultural customer base of 100,000 brands around the world, Zendesk has an opportunity to access specialized models, likeWidn.ai, from Amazon Bedrock Marketplace’s vast catalogue and incorporate them into their generative AI features for translation use cases, further personalizing and localizing customer service requests—across email, chat, phone, and social media—and providing agents with data such as sentiment or intent in the customer’s native language. This partnership will allow companies to expand the reach of AI to engage in a way that is even more personalized and catered to customers. Jon Aniano, Senior Vice President, Product, Zendesk IBM We’re excited to continue bringing IBM Granite models to AWS Bedrock, giving clients greater choice and flexibility in accessing efficient and trustworthy AI products. Our growing partnership allows us to meet clients where they are, building a strong AI ecosystem and enabling seamless integration core capabilities into their existing environments. Maryam Ashoori, Director of Product Management, IBM Model Providers Get started User Guide: Amazon Bedrock Marketplace AWS Machine Learning launch announcement blog for Bedrock Marketplace Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/marketplace/en_US/approved/bedrock/images/725beab22419083faee478be1ddb6b1e-aws-bedrock-marketplace-model-catalog-ui-screenshot-1387x888-png.fad3e8ba4e8cf2e85f0aa3cd354d1e9f7c681c8f.png", "local": "images\\75d830adc09c1b2338529d4c0d90b5b04e0c354b.png", "alt": "Screenshot of the AWS Bedrock Marketplace model catalog user interface, showing filters and a grid of available AI models for deployment, including their categories and providers."}, {"url": "https://d1.awsstatic.com/case-studies/600x400_zendesk_logo.736fea3375f99cf2169f7785171786d319fe9cdd.png", "local": "images\\89aa4ffdaebc74ec0880abcc11aa51710a6356ec.png", "alt": "Missing alt text value"}, {"url": "https://d1.awsstatic.com/partner-network/AWS%20Partner%20Webinar%20Series/Webinar%20Page%20Refresh-Phase%201-Q217/IBM_Logo_200x100%20.dfb27617e9922e344c49f9c38630a48c17579612.png", "local": "images\\e6b83ddeeda0407663b054c19e23f491824a8ff4.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/marketplace/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/faqs/", "https://aws.amazon.com/bedrock/", "https://docs.aws.amazon.com/bedrock/latest/userguide/amazon-bedrock-marketplace.html", "https://aws.amazon.com/bedrock/marketplace/"]}
{"url": "https://aws.amazon.com/bedrock/custom-model-import/", "text": " Search Amazon Bedrock Amazon Bedrock Custom Model Import Leverage your customized models in Amazon Bedrock Overview Amazon Bedrock Custom Model Import enables the import and use of your customized models alongside existing foundation models (FMs) through a single serverless, unified API. You can access your imported custom models on-demand and without the need to manage underlying infrastructure. Accelerate your generative AI application development by integrating your supported custom models with native Bedrock tools and features like Knowledge Bases, Guardrails, and Agents.  Key benefits Remove infrastructure management Amazon Bedrock Custom Model Import simplifies and streamlines the process of deploying models that have been customized outside of Bedrock. Previously, utilizing models customized externally required the use of self-managed infrastructure, creating a fragmented experience for developers. With Custom Model Import, you can access your imported custom models in a serverless, on-demand fashion, eliminating the need to manage instances and the overhead associated with model lifecycle and infrastructure management. Unified developer experience Amazon Bedrock Custom Model Import unifies the developer experience by allowing seamless integration of externally customized models. This feature enables developers to access both base and imported custom models through a single API, streamlining gen AI application development. Additionally, you can leverage native Bedrock tooling, such as Knowledge Bases, Guardrails, Agents, and more with your imported custom models just as you do with other Bedrock on-demand foundation models. Flexibility to use your existing investments Custom Model Import provides you the flexibility to use your prior investments in model customization. Previously, those who customized models outside of Amazon Bedrock had to recreate those customizations within Bedrock. With Custom Model Import, you can now import your existing customized models and register them as imported models on Amazon Bedrock. You can flexibly use these FMs alongside base models to build applications. This flexibility gives you more value out of your previous model customization efforts and integrates the FMs into your applications built in Bedrock.  Deploy DeepSeek-R1 with Custom Model Import Amazon Bedrock Custom Model Import supports distilled Llama versions of DeepSeek-R1. Current supported variants include DeepSeek-R1-Distill-Llama-8B and DeepSeek-R1-Distill-Llama-70B, offering an optimal balance between performance and resource efficiency. These models can be imported from Amazon Simple Storage Service (Amazon S3) or an Amazon SageMaker AI model repo, and deployed in a fully managed and serverless environment through Custom Model Import. Read the blog Get started Deploy DeepSeek-R1 distilled Llama models with Amazon Bedrock Custom Model Import Amazon Bedrock Custom Model Import now generally available To begin using Custom Model Import, navigate to the Amazon Bedrock console, select \"Amazon Bedrock Custom Model Import,\" and follow the guided workflow to import your custom model weights Amazon Bedrock Custom Model Import now available Did you find what you were looking for today? Let us know so we can improve the quality of the content on our pages Learn Resources Developers Help", "images": [{"url": "https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/product-categories/ai-ml/machine-learning/approved/images/325417b3-3cd5-483f-90bf-6c9a18ca54bd.4b37196598896f0940e5ee7c81a048abc0fa3e9b.png", "local": "images\\d36c1bd62db5d856c7cd4b4d441ea2ba3a22e704.png", "alt": "Missing alt text value"}], "links": ["https://aws.amazon.com/bedrock/custom-model-import/#aws-page-content-main", "https://aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/getting-started/", "https://aws.amazon.com/bedrock/agentcore/", "https://aws.amazon.com/bedrock/pricing/", "https://aws.amazon.com/bedrock/", "https://console.aws.amazon.com/bedrock/", "https://aws.amazon.com/bedrock/custom-model-import/"]}
